[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods 1 (DClin) R Sessions",
    "section": "",
    "text": "Welcome to the module\nThis site accompanies PSY4179-N and is specifically focused on the quantitative research design/stats/R sessions delivered by Christopher Wilson.\nInformation about the learning outcomes, assessments and other teaching sessions on the module can be found on the module Blackboard site."
  },
  {
    "objectID": "index.html#data-analysis-with-r",
    "href": "index.html#data-analysis-with-r",
    "title": "Research Methods 1 (DClin) R Sessions",
    "section": "Data analysis with R",
    "text": "Data analysis with R\nYou may be familiar with SPSS from your undergraduate statistics teaching. Please note that we do not use SPSS for teaching and instead use R Statistics. The reason for this is that R is a free statistical package, meaning that it can be accessed in NHS settings that do not have funding for SPSS. This will enable you to run statistical analyses whilst on placement where required, and also enables you to conduct statistical analyses as a qualified Psychologist without incurring any software costs. R is also more flexible than SPSS and has greater functionality. During the teaching you will be shown how to set up and install R, and how to run statistical analyses in this software.\nDownloading R and R Studio software\n\n\nYou can obtain R and R Studio from the following links:\nhttps://cran.r-project.org/\nhttps://rstudio.com/"
  },
  {
    "objectID": "index.html#textbooks-that-can-be-accessed-online",
    "href": "index.html#textbooks-that-can-be-accessed-online",
    "title": "Research Methods 1 (DClin) R Sessions",
    "section": "Textbooks that can be accessed online",
    "text": "Textbooks that can be accessed online\ne-books can be accessed from the library website: https://www.tees.ac.uk/depts/lis/\nResearch Methods and Statistics\nCoolican, 2019. Research Methods and Statistics in Psychology. Taylor & Francis Group\nBarker, C., Pistrang, N., & Elliott, R. (2015). Research methods in clinical psychology: An introduction for students and practitioners (3rd ed.). Chichester, West Sussex: Wiley Blackwell.\nWeiner, I. B., Schinka, J. A., & Velicer, W. F. (2012). Handbook of psychology, research methods in psychology (2. Aufl. ed.). Somerset: Wiley.\nWorking with R and RStudio to do analysis\nNavarro, D. (2017) Learning statistics with R.\nPhillips N. D. (2018) YaRrr! The Pirate’s Guide to R\nHorton, Pruim and Kaplan (2015) A Student’s Guide to R\nMather, M. (2019) R for Academics\nWickham and Grolemund (2019). R For Data Science\nAllaire and Grolemund (2019). R Markdown: The Definitive Guide\nBasics of RStudio\nData Import\nData Transformation\nData Visualisation with GGPlot"
  },
  {
    "objectID": "introductiontoR.html#by-the-end-of-this-section-you-should-be-able-to",
    "href": "introductiontoR.html#by-the-end-of-this-section-you-should-be-able-to",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.1 By the end of this section, you should be able to:",
    "text": "1.1 By the end of this section, you should be able to:\n\nDownload R and R studio\nIdentify the R script, R console, Data environment and file browser in R studio\nWrite and run R code from a script\nInstall and load R packages"
  },
  {
    "objectID": "introductiontoR.html#why-learn-use-r",
    "href": "introductiontoR.html#why-learn-use-r",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.2 Why learn / use R?",
    "text": "1.2 Why learn / use R?\n\n1.2.1 Some information about R\n\nR is developed and used by scientists and researchers around the world\nOpen source = no cost\nConstant development\nConnects to other data science/research tools\nWorldwide community: training widely available\nEncourages transparency and reproducibility\nPublication-ready outputs\n\n1.2.2 Moving from other software to R\n\nWorkflow is different\n\nOrganise files and data differently\nWorkspace can contain data and outputs\nCan manage multiple datasets within a workspace\n\n\nLearning curve can be steep initially\n\ne.g. Variables and coding, scripts\n\n\nNeed to know what you want\n\ne.g. building your regression model / ANOVA error terms"
  },
  {
    "objectID": "introductiontoR.html#r-has-many-advantages",
    "href": "introductiontoR.html#r-has-many-advantages",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.3 R has many advantages",
    "text": "1.3 R has many advantages\n\nUsing scripts means analysis is easy to follow and reproduce\nR scripts are small, online collaboration, no SPSS “older version” problems\nData can be organised and reorganised however you need it (tidyr)\nPackages are available for “cutting edge” analysis: e.g. Big Data & Machine Learning\nA robust language for precise plots and graphics (ggplot)\nR analysis code can be embdeded into documents and presentations (R Markdown)"
  },
  {
    "objectID": "introductiontoR.html#download-r-and-r-studio",
    "href": "introductiontoR.html#download-r-and-r-studio",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.4 Download R and R Studio",
    "text": "1.4 Download R and R Studio\n\n\nClick on these links to download:\n\nR project\nRStudio"
  },
  {
    "objectID": "introductiontoR.html#the-r-studio-environment",
    "href": "introductiontoR.html#the-r-studio-environment",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.5 The R Studio environment",
    "text": "1.5 The R Studio environment\n\nThe interface for R Studio looks daunting at first. However, there are 4 main sections, 2 on the left and 2 on the right.\n\nMAIN TOP: R Script files or R Document Files\n\nWhere we usually type our code as a script before we run it. Script files are usually saved so we can work on them and rerun the code again later (.R files).\n\n\nMAIN BOTTOM: Console\n\nShows the output of our R code. We can type R code directly into the console and the answer will ouput immediately. However, it is more convenient to use script files.\n\n\nRIGHT TOP: Environment\n\nContains all of the objects (e.g. data, analysis, equations, plots) that are currently stored in memory. We can save all of this to a file and load it later (.RData files).\n\n\nRIGHT BOTTOM: File Browser\n\nThe folder that R is working from is called ‘the working directory’ and it will automatically look for files there if we try to import something (e.g. a data file). Using the more button on the file browser allows you to set your desired working directory."
  },
  {
    "objectID": "introductiontoR.html#working-with-a-script",
    "href": "introductiontoR.html#working-with-a-script",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.6 Working with a script",
    "text": "1.6 Working with a script\nScripts can be opened from the File > New File > R Script menu.\n\n\nCreating a new script\n\n\nThe purpose of scripts is to allow you to type your analysis code and save it for use later. Scripts include, for example:\n\nCode for importing data into R\nYour analysis code (e.g. t-test or descriptive statistics)\nCode for graphs and tables\nComments and notes (preceded by the ‘#’ symbol)\n\n\n\nExample of an R script\n\n\nTo run a script, you click the Run button. You can choose to:\n\nRun the whole script\nRun the selected line of code\n\n\n\nThe run button\n\n\nWhen you run the script, you will normally see output in the console.\n\n\nOutput appears in the console\n\n\nIf your script contains code for a plot (graph), it will appear in the Plots window in the bottom right.\n\n\nPlots appear in the plot window"
  },
  {
    "objectID": "introductiontoR.html#installing-and-loading-packages",
    "href": "introductiontoR.html#installing-and-loading-packages",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.7 Installing and loading packages",
    "text": "1.7 Installing and loading packages\n\n\n\ninstall Packages from RStudio, Inc. on Vimeo.\n\nPackages add functionality to R and allow us to do new types of analysis.\n\nThey can be installed via the menu Tools -> Install Packages\n\nThe can also be installed using code:\n\ninstall.packages()\nFor example, tidyr is a package that contains functions for sorting and organising data. To install the package:\n\n\nInstalling a package in RStudio\n\n\nor use the code:\ninstall.packages(“tidyr”)\nOnce a package is has been installed, you need tp load it using the library() command. For example:\n\n   library(tidyr)"
  },
  {
    "objectID": "workingWithData.html#by-the-end-of-this-section-you-will-be-able-to",
    "href": "workingWithData.html#by-the-end-of-this-section-you-will-be-able-to",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.1 By the end of this section, you will be able to:",
    "text": "2.1 By the end of this section, you will be able to:\n\nImport data into R from excel, SPSS and csv files\nSave data to objectA word that identifies and stores the value of some data for later use.\n\nIdentify different data structures and data types\n\nConvert data types from one type to another\nOrder, filter and group data\nSummarise data\nCreate new variables or objectsA word that identifies and stores the value of some data for later use. from data"
  },
  {
    "objectID": "workingWithData.html#in-this-section-we-will-use-the-tidyverse-set-of-packages",
    "href": "workingWithData.html#in-this-section-we-will-use-the-tidyverse-set-of-packages",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.2 In this section, we will use the Tidyverse set of packages",
    "text": "2.2 In this section, we will use the Tidyverse set of packages\n\nA ‘toolkit’ of packages that are very useful for organsing and manipulating data\nWe will use the haven package to import SPSS files\nWe will use the dplyr to organise data\nAlso includes the ggplot2 and tidyR packages which we will use later\n\nTo install:\ninstall.packages(“tidyverse”)\n(See the previous section on installing packages)"
  },
  {
    "objectID": "workingWithData.html#import-data-into-r-from-excel-spss-and-csv-files",
    "href": "workingWithData.html#import-data-into-r-from-excel-spss-and-csv-files",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.3 Import data into R from excel, SPSS and csv files",
    "text": "2.3 Import data into R from excel, SPSS and csv files\nWe can import data from a range of sources using the Import Dataset button in the Environment tab:\n\n\nImporting data\n\nIt is also possible to import data using code, for example:\n` # importing a .csv file\n\n   library(readr)\n    studentData &lt;- read_csv(\"Datasets/studentData.csv\")\n\n    #importing an SPSS file\n    \n    library(haven)\n    mySPSSData &lt;- read_sav(\"datasets/salesData.sav\")\n\nOnce the data are imported, it will be visible in the environment:\n\n\nImported data in the environment"
  },
  {
    "objectID": "workingWithData.html#restructuring-and-reorganising-data-in-r-long-versus-wide-data",
    "href": "workingWithData.html#restructuring-and-reorganising-data-in-r-long-versus-wide-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.4 Restructuring and reorganising data in R (long versus wide data)",
    "text": "2.4 Restructuring and reorganising data in R (long versus wide data)"
  },
  {
    "objectID": "workingWithData.html#understanding-objects-in-r",
    "href": "workingWithData.html#understanding-objects-in-r",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.5 Understanding objects in R",
    "text": "2.5 Understanding objects in R\nIn R, an objectA word that identifies and stores the value of some data for later use. is anything that is saved to memory. For example, we might do some analysis:\nmean(happiness)\nHowever, in the example above, the result would appear in the console but not be saved anywhere. To store the result for reuse later, we save it to an object:\n\nhappinessMean &lt;- mean(happiness)\n\nIn the above code (reading left to right):\n\nWe name the object “happinessMean”. This name can be anything we want.\nThe arrow means that the result of the code on the right will be saved to the object on the left.\nThe code on the right of the arrow calculates the mean of happiness data\n\nWhen this code is run, happinessMean will be stored in the environment window:\n\n\nStoring the result of a calculation in the environment\n\nTo recall an object from the environment, we can simply type its name. For example:\n\n happinessMean\n\n[1] 10.29769\n\n\n\nIts important to note that anything can be stored as an object in R and recalled later. This includes, dataframes, the results of statistical calculations, plots etc."
  },
  {
    "objectID": "workingWithData.html#identify-different-data-structures-and-variable-types",
    "href": "workingWithData.html#identify-different-data-structures-and-variable-types",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.6 Identify different data structures and variable types",
    "text": "2.6 Identify different data structures and variable types\n\n2.6.1 Data structures (sometimes referred to as “data containers”)\nThere are many different types of data structures that R can work with. The most common type of data for most people tends to be a data frame. A data frame is what you might consider a “normal” 2-dimensional dataset, with rows of data and columns of variables:\n\n\nA dataframe example\n\n\nR can also use other data types.\nA vector is a one-dimensional set of values:\n\n# a vector example\n\nscores <- c(1,4,6,8,3,4,6,7)\n\nA matrix is a multi-dimensional set of values. The below example is a 3-dimensional matrix, there are 2 groups of 2 rows and 3 columns:\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n\n\nWe will primarily work with dataframes (and sometimes vectors), as this is how the data in psychology research is usually structured.\n\n\n2.6.2 Data types\nWith numerical data, there are 4 key data types:\n\n\nfactor (data type) or nominal (a category, group or factor)\n\nordinalDiscrete variables that have an inherent order, such as level of education or dislike/like. (a ranking)\n\ninterval (data type) (scale data that can include negative values)\n\nratio (data type) (scale data that cannot include negative values)\n\n R can use all of these variable types:\n\n\nNominal variables are called factors\n\n\nOrdinal variables are called ordered factors\n\n\nInterval and ratio variables are called numeric data and can sometimes be called integers (if they are only whole numbers) or doubles (if they all have decimal points)\n\nR can also use other data types such as text (characterA data type representing strings of text.) data.\n\n2.6.3 Convert variables from one data type to another\nWhen we first import data into R, it might not recognise the data types correctly. For example, in the below data, we can see the intervention variable :\n\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n8\n1\n6.670519\n\n\n14\n1\n7.288869\n\n\n4\n2\n7.473557\n\n\n20\n1\n7.871574\n\n\n16\n2\n8.676627\n\n\n7\n1\n8.786994\n\n\n6\n1\n8.985780\n\n\n3\n1\n9.184181\n\n\n10\n1\n9.309547\n\n\n17\n1\n9.488595\n\n\n\n\n\n\nIn the intervention variable, the numbers 1 and 2 refer to different intervention groups. Therefore, the variable is a factor (data type) variable. To ensure that R understands this, we can resave the intervention variable as a factor using the as.factor() function:\n\nhappinessSample$intervention <- as.factor(happinessSample$intervention)"
  },
  {
    "objectID": "workingWithData.html#working-with-dataframes",
    "href": "workingWithData.html#working-with-dataframes",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.7 Working with dataframes",
    "text": "2.7 Working with dataframes\nDataframesa 2-dimensional dataset, usually with rows of data and columns of variables are the more standard data format that were are used to (think of how a dataset looks in SPSS or Excel).\nIn a dataframea 2-dimensional dataset, usually with rows of data and columns of variables, variables are columns and each row usually reperesents one measurement or one participant.\n\n2.7.1 View dataframe\nTo view a dataframe, we can click on it in the environment window and it will display:\n\n\nClicking on datasets in hte environment will open them up for viewing\n\n\n\nViewing a dataframe\n\n\n2.7.2 Refer to variables (columns) in a dataframe\nColumns in a dataframe are accessed using the “$” sign. For example, to access the happiness column in the happinessSample dataframe, we would type:\n\nhappinessSample$happiness\n\n [1] 11.580517 11.947034  9.909773  6.245260  9.381039 11.515421  8.745944\n [8]  9.301780  8.906846 11.011479 10.726459 11.337853  9.199057 11.120169\n[15] 11.563120  9.446345 10.075152 10.017880 11.284192 12.638480\n\n\nAs we can see above, the result is then displayed."
  },
  {
    "objectID": "workingWithData.html#order-filter-and-group-data",
    "href": "workingWithData.html#order-filter-and-group-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.8 Order, filter and group data",
    "text": "2.8 Order, filter and group data\nIf you have the tidyverse package loaded, it is easy to organise and filter data.\n\narrange(happinessSample, happiness)\narrange(happinessSample, desc(happiness)) # Arrange in descending order\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n4\n2\n6.245260\n\n\n7\n2\n8.745944\n\n\n9\n2\n8.906846\n\n\n13\n2\n9.199057\n\n\n8\n2\n9.301780\n\n\n5\n2\n9.381039\n\n\n16\n1\n9.446345\n\n\n3\n1\n9.909773\n\n\n18\n2\n10.017880\n\n\n17\n2\n10.075152\n\n\n11\n2\n10.726459\n\n\n10\n2\n11.011479\n\n\n14\n1\n11.120169\n\n\n19\n2\n11.284192\n\n\n12\n1\n11.337853\n\n\n6\n2\n11.515421\n\n\n15\n2\n11.563120\n\n\n1\n1\n11.580517\n\n\n2\n1\n11.947034\n\n\n20\n1\n12.638480\n\n\n\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n20\n1\n12.638480\n\n\n2\n1\n11.947034\n\n\n1\n1\n11.580517\n\n\n15\n2\n11.563120\n\n\n6\n2\n11.515421\n\n\n12\n1\n11.337853\n\n\n19\n2\n11.284192\n\n\n14\n1\n11.120169\n\n\n10\n2\n11.011479\n\n\n11\n2\n10.726459\n\n\n17\n2\n10.075152\n\n\n18\n2\n10.017880\n\n\n3\n1\n9.909773\n\n\n16\n1\n9.446345\n\n\n5\n2\n9.381039\n\n\n8\n2\n9.301780\n\n\n13\n2\n9.199057\n\n\n9\n2\n8.906846\n\n\n7\n2\n8.745944\n\n\n4\n2\n6.245260\n\n\n\n\n\n\n\nShow clients with a happiness score of less than 4\n\n\nfilter(happinessSample, happiness &lt; 4)\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n\n\n\n\nShow Intervention group 2 with happiness scores above 7\n\n\nfilter(happinessSample, happiness &gt; 7 & intervention == 2)\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n5\n2\n9.381039\n\n\n6\n2\n11.515421\n\n\n7\n2\n8.745944\n\n\n8\n2\n9.301780\n\n\n9\n2\n8.906846\n\n\n10\n2\n11.011479\n\n\n11\n2\n10.726459\n\n\n13\n2\n9.199057\n\n\n15\n2\n11.563120\n\n\n17\n2\n10.075152\n\n\n18\n2\n10.017880\n\n\n19\n2\n11.284192\n\n\n\n\n\n\n\nGroup by intervention and show the mean happiness score\n\n\nhappinessSample %&gt;% group_by(intervention) %&gt;% summarise(mean = mean(happiness))\n\n\n\n\nintervention\nmean\n\n\n\n1\n11.140025\n\n\n2\n9.844125"
  },
  {
    "objectID": "workingWithData.html#create-new-variables-objects-from-data",
    "href": "workingWithData.html#create-new-variables-objects-from-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.9 Create new variables / objects from data",
    "text": "2.9 Create new variables / objects from data\nTo create new variables from data, we can use the mutate() function.\nFor example, let’s say we wanted to calculate the difference between each person’s happiness score and the mean happiness score.\nWe could do the following:\n\nhappinessSample %&gt;% mutate(difference = happiness - mean(happiness))\n\n\n\n\nparticipant\nintervention\nhappiness\ndifference\n\n\n\n1\n1\n11.580517\n1.2828274\n\n\n2\n1\n11.947034\n1.6493438\n\n\n3\n1\n9.909773\n-0.3879166\n\n\n4\n2\n6.245260\n-4.0524304\n\n\n5\n2\n9.381039\n-0.9166514\n\n\n6\n2\n11.515421\n1.2177310\n\n\n7\n2\n8.745944\n-1.5517460\n\n\n8\n2\n9.301780\n-0.9959098\n\n\n9\n2\n8.906846\n-1.3908438\n\n\n10\n2\n11.011479\n0.7137887\n\n\n11\n2\n10.726459\n0.4287693\n\n\n12\n1\n11.337853\n1.0401634\n\n\n13\n2\n9.199057\n-1.0986329\n\n\n14\n1\n11.120169\n0.8224791\n\n\n15\n2\n11.563120\n1.2654296\n\n\n16\n1\n9.446345\n-0.8513449\n\n\n17\n2\n10.075152\n-0.2225381\n\n\n18\n2\n10.017880\n-0.2798103\n\n\n19\n2\n11.284192\n0.9865019\n\n\n20\n1\n12.638480\n2.3407900"
  },
  {
    "objectID": "descriptives.html#working-example---record-sales-data",
    "href": "descriptives.html#working-example---record-sales-data",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.1 Working example - record sales data",
    "text": "3.1 Working example - record sales data\nLet’s import the data\n\nAlbum_Sales &lt;- read.csv(\"datasets/album_sales.csv\")\n\nLet’s look at the data\n\nhead(Album_Sales)\n\n\n\n\nAdverts\nSales\nAirplay\nAttract\nGenre\n\n\n\n10.256\n330\n43\n10\nCountry\n\n\n985.685\n120\n28\n7\nPop\n\n\n1445.563\n360\n35\n7\nHipHop\n\n\n1188.193\n270\n33\n7\nHipHop\n\n\n574.513\n220\n44\n5\nMetal\n\n\n568.954\n170\n19\n5\nCountry"
  },
  {
    "objectID": "descriptives.html#lets-make-sure-our-data-types-are-correct-1",
    "href": "descriptives.html#lets-make-sure-our-data-types-are-correct-1",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.2 Let’s make sure our data types are correct #1",
    "text": "3.2 Let’s make sure our data types are correct #1\n\nThis variable is currently stored as charcters, not as a factor / category variable\n\n\nstr(Album_Sales$Genre) \n\n chr [1:200] \"Country\" \"Pop\" \"HipHop\" \"HipHop\" \"Metal\" \"Country\" \"Pop\" ...\n\n\n\nWe can save it as a factor\n\n\nAlbum_Sales$Genre &lt;- as.factor(Album_Sales$Genre)\n\nstr(Album_Sales$Genre) \n\n Factor w/ 4 levels \"Country\",\"HipHop\",..: 1 4 2 2 3 1 4 4 3 2 ..."
  },
  {
    "objectID": "descriptives.html#measures-of-central-tendency",
    "href": "descriptives.html#measures-of-central-tendency",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.3 Measures of central tendency",
    "text": "3.3 Measures of central tendency\nThe main measures of central tendency are: - Mean - Median - Mode\n\n3.3.1 Mean\n“What is the mean of album sales?”\n\nmean(Album_Sales$Sales)\n\n[1] 193.2\n\n\n\n3.3.2 Trimmed mean\n\nThe trimmed mean is used to reduce the influence of outliers on the summary\n\n\nmean(Album_Sales$Sales, trim = 0.05)\n\n[1] 192.6667\n\n\n\n3.3.3 Median\n“What is the median amount of Airplay?”\n\nmedian(Album_Sales$Airplay)\n\n[1] 28\n\n\n\n3.3.4 Mode\n“What is the most common attractiveness rating of bands?”\n\nThe easiest way to get the mode in R is to generate a frequency table\n\n\ntable(Album_Sales$Attract)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n 3  1  1  4 17 44 73 44 12  1 \n\n\n\nWe can then look for the most frequently occuring response"
  },
  {
    "objectID": "descriptives.html#measures-of-dispresion-or-variance",
    "href": "descriptives.html#measures-of-dispresion-or-variance",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.4 Measures of dispresion or variance",
    "text": "3.4 Measures of dispresion or variance\n\n3.4.1 Range\nThe range is the difference between the lowest and highest values\n\nYou can calculate it using these values\n\n\nmax(Album_Sales$Airplay) - min(Album_Sales$Airplay)\n\n[1] 63\n\n\n\nOr you can use the range command to get the min and max values in one go\n\n\nrange(Album_Sales$Airplay)\n\n[1]  0 63\n\n\n\n3.4.2 Interquartile range\n\nWe know that the median is the “middle” of the data = 50th percentile\nThe interquatile range is the difference between the values at the 25th and 75th percentiles\n\n\nquantile( x = Album_Sales$Airplay, probs = c(.25,.75) )\n\n  25%   75% \n19.75 36.00 \n\n\n\nInterquartile range = 36 - 19.75 = 16.25\n\nSum of squares\n\nThe difference between each value and the mean value, squared, and then summed together\n\n\nsum( (Album_Sales$Adverts - mean(Album_Sales$Adverts))^2 )\n\n[1] 46936335\n\n\n\n3.4.3 Variance\n\nVariance: Sum of sqaures divided by n-1\n\n\n# variance calculation\nvarianceAdverts &lt;- sum( (Album_Sales$Adverts - mean(Album_Sales$Adverts))^2 ) / 199\n\n\n3.4.4 Standard deviation\n\nStandard deviation is square root of the variance\n\n\n# sd calculation\n\n\nsqrt(varianceAdverts)\n\n[1] 485.6552\n\n\n\nCan be calculated using the sd() command\n\n\nsd(Album_Sales$Adverts)\n\n[1] 485.6552"
  },
  {
    "objectID": "descriptives.html#skewness-and-kurtosis",
    "href": "descriptives.html#skewness-and-kurtosis",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.5 Skewness and Kurtosis",
    "text": "3.5 Skewness and Kurtosis\n\n3.5.1 Assessing skewness of distribution #1\n\nIt is possible to use graphs to view the distribution\nWe will focus on graphic presentation of data next week\n\n\nhist(Album_Sales$Sales)\n\n\n\n\n\n\n\n\n3.5.2 Assessing skewness of distribution #2\n\nWe can check raw skewness value using the skew() command in the psych package\n\n\nlibrary(psych)\n\nWarning: package 'psych' was built under R version 4.2.3\n\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nskew(Album_Sales$Sales)\n\n[1] 0.0432729\n\n\n\n3.5.3 Kurtosis\n\n\ninformal term\ntechnical name\nkurtosis value\n\n\n\n“too flat”\nplatykurtic\nnegative\n\n\n“just pointy enough”\nmesokurtic\nzero\n\n\n“too pointy”\nleptokurtic\npositive\n\n\n\n\nkurtosi(Album_Sales$Sales)\n\n[1] -0.7157339\n\n\n\n3.5.4 Assessing normality of distribution\n\nWe can use the shapiro-wilk test of normality\nThis is part of “base” r (no package needed)\n\n\nshapiro.test(Album_Sales$Sales)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Album_Sales$Sales\nW = 0.98479, p-value = 0.02965"
  },
  {
    "objectID": "descriptives.html#getting-and-overall-summary",
    "href": "descriptives.html#getting-and-overall-summary",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.6 Getting and overall summary",
    "text": "3.6 Getting and overall summary\n\n3.6.1 summary() - in “base R”\n\nsummary(Album_Sales)\n\n    Adverts             Sales          Airplay         Attract     \n Min.   :   9.104   Min.   : 10.0   Min.   : 0.00   Min.   : 1.00  \n 1st Qu.: 215.918   1st Qu.:137.5   1st Qu.:19.75   1st Qu.: 6.00  \n Median : 531.916   Median :200.0   Median :28.00   Median : 7.00  \n Mean   : 614.412   Mean   :193.2   Mean   :27.50   Mean   : 6.77  \n 3rd Qu.: 911.226   3rd Qu.:250.0   3rd Qu.:36.00   3rd Qu.: 8.00  \n Max.   :2271.860   Max.   :360.0   Max.   :63.00   Max.   :10.00  \n     Genre   \n Country:46  \n HipHop :53  \n Metal  :48  \n Pop    :53  \n             \n             \n\n\n\n3.6.2 describe() - in the “psych” package #1\n\ndescribe(Album_Sales)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n\nAdverts\n1\n200\n614.4123\n485.655208\n531.916\n560.80516\n489.0882\n9.104\n2271.86\n2262.756\n0.8399360\n0.1687385\n34.3410091\n\n\nSales\n2\n200\n193.2000\n80.698957\n200.000\n192.68750\n88.9560\n10.000\n360.00\n350.000\n0.0432729\n-0.7157339\n5.7062779\n\n\nAirplay\n3\n200\n27.5000\n12.269585\n28.000\n27.46250\n11.8608\n0.000\n63.00\n63.000\n0.0588275\n-0.0924910\n0.8675907\n\n\nAttract\n4\n200\n6.7700\n1.395290\n7.000\n6.88125\n1.4826\n1.000\n10.00\n9.000\n-1.2668371\n3.5555040\n0.0986619\n\n\nGenre*\n5\n200\n2.5400\n1.115627\n3.000\n2.55000\n1.4826\n1.000\n4.00\n3.000\n-0.0242500\n-1.3661364\n0.0788867\n\n\n\n\n\n\n\n3.6.3 describe() - in the “psych” package #2\n\nWe can describe by factor variables\n\n\ndescribeBy(Album_Sales, group = Album_Sales$Genre)\n\n\n Descriptive statistics by group \ngroup: Country\n        vars  n   mean     sd median trimmed    mad  min     max   range  skew\nAdverts    1 46 656.22 507.96 574.14  620.40 581.96  9.1 1985.12 1976.01  0.51\nSales      2 46 201.74  73.64 210.00  200.79  66.72 60.0  360.00  300.00  0.03\nAirplay    3 46  29.07  10.53  28.00   28.50  11.12  9.0   54.00   45.00  0.44\nAttract    4 46   6.52   1.63   7.00    6.71   1.48  1.0   10.00    9.00 -1.49\nGenre*     5 46   1.00   0.00   1.00    1.00   0.00  1.0    1.00    0.00   NaN\n        kurtosis    se\nAdverts    -0.65 74.89\nSales      -0.52 10.86\nAirplay    -0.10  1.55\nAttract     3.54  0.24\nGenre*       NaN  0.00\n------------------------------------------------------------ \ngroup: HipHop\n        vars  n   mean     sd median trimmed    mad   min  max   range  skew\nAdverts    1 53 606.32 452.84 601.43  568.33 501.36 10.65 2000 1989.35  0.70\nSales      2 53 199.62  92.71 200.00  200.70 103.78 10.00  360  350.00 -0.10\nAirplay    3 53  28.09  13.86  30.00   28.33  14.83  0.00   55   55.00 -0.14\nAttract    4 53   6.96   1.13   7.00    7.00   1.48  3.00    9    6.00 -0.80\nGenre*     5 53   2.00   0.00   2.00    2.00   0.00  2.00    2    0.00   NaN\n        kurtosis    se\nAdverts     0.05 62.20\nSales      -0.91 12.74\nAirplay    -0.83  1.90\nAttract     2.03  0.15\nGenre*       NaN  0.00\n------------------------------------------------------------ \ngroup: Metal\n        vars  n   mean     sd median trimmed    mad  min     max   range  skew\nAdverts    1 48 693.45 534.06  593.0  640.19 521.34 45.3 2271.86 2226.56  0.92\nSales      2 48 197.71  75.18  200.0  198.25  88.96 40.0  340.00  300.00 -0.07\nAirplay    3 48  27.96  11.37   27.5   28.00  11.12  2.0   57.00   55.00  0.02\nAttract    4 48   6.85   1.34    7.0    6.90   1.48  2.0    9.00    7.00 -0.84\nGenre*     5 48   3.00   0.00    3.0    3.00   0.00  3.0    3.00    0.00   NaN\n        kurtosis    se\nAdverts     0.21 77.08\nSales      -0.94 10.85\nAirplay    -0.26  1.64\nAttract     1.74  0.19\nGenre*       NaN  0.00\n------------------------------------------------------------ \ngroup: Pop\n        vars  n   mean     sd median trimmed    mad   min     max   range  skew\nAdverts    1 53 514.63 446.04  429.5  453.85 438.01 15.31 1789.66 1774.35  1.01\nSales      2 53 175.28  77.92  160.0  171.86  88.96 40.00  360.00  320.00  0.34\nAirplay    3 53  25.13  12.75   26.0   25.02  11.86  1.00   63.00   62.00  0.25\nAttract    4 53   6.72   1.47    7.0    6.81   1.48  1.00    9.00    8.00 -1.11\nGenre*     5 53   4.00   0.00    4.0    4.00   0.00  4.00    4.00    0.00   NaN\n        kurtosis    se\nAdverts     0.27 61.27\nSales      -0.67 10.70\nAirplay     0.46  1.75\nAttract     2.51  0.20\nGenre*       NaN  0.00"
  },
  {
    "objectID": "descriptives.html#basic-statistical-tests-more-detail-in-later-sections",
    "href": "descriptives.html#basic-statistical-tests-more-detail-in-later-sections",
    "title": "\n3  Exploratory and descriptive analysis with R\n",
    "section": "\n3.7 Basic statistical tests (more detail in later sections)",
    "text": "3.7 Basic statistical tests (more detail in later sections)\n\n3.7.1 Corrleation\n“Is there a relationship between advert spend and sales?”\n\nWe would use an correlational analysis to answer this question\n\n\nplot(Album_Sales$Sales,Album_Sales$Adverts)\n\n\n\n\n\n\n\n“Is there a relationship between advert spend and sales?”\n\nWe would use an correlational analysis to answer this question\n\n\ncor.test(Album_Sales$Sales,Album_Sales$Adverts)\n\n\n    Pearson's product-moment correlation\n\ndata:  Album_Sales$Sales and Album_Sales$Adverts\nt = 9.9793, df = 198, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4781207 0.6639409\nsample estimates:\n      cor \n0.5784877 \n\n\n\n3.7.2 Tests of difference - t-test\n“Is there a significant difference in sales between the Country and Hip-hop musical genres?”\n\nWe would use a t-test to answer this question\n\n\nmyTtestData &lt;- Album_Sales %&gt;% filter(Genre == c(\"Country\", \"HipHop\"))\n\nt.test(myTtestData$Sales ~ myTtestData$Genre)\n\n\n    Welch Two Sample t-test\n\ndata:  myTtestData$Sales by myTtestData$Genre\nt = 0.80489, df = 40.62, p-value = 0.4256\nalternative hypothesis: true difference in means between group Country and group HipHop is not equal to 0\n95 percent confidence interval:\n -27.80146  64.62904\nsample estimates:\nmean in group Country  mean in group HipHop \n             216.0000              197.5862 \n\n\n\n3.7.3 Tests of difference - ANOVA\n“Is there a significant difference in sales between all musical genres?”\n\nWe would use an ANOVA to answer this question\n\n\nmyAnova &lt;- aov(Album_Sales$Sales ~ Album_Sales$Genre)\nsummary(myAnova)\n\n                   Df  Sum Sq Mean Sq F value Pr(&gt;F)\nAlbum_Sales$Genre   3   23530    7843   1.208  0.308\nResiduals         196 1272422    6492"
  },
  {
    "objectID": "graphing.html#presenting-data-visually",
    "href": "graphing.html#presenting-data-visually",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.1 Presenting data visually",
    "text": "4.1 Presenting data visually"
  },
  {
    "objectID": "graphing.html#using-ggplot-to-make-graphs",
    "href": "graphing.html#using-ggplot-to-make-graphs",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.2 Using GGplot to make graphs",
    "text": "4.2 Using GGplot to make graphs"
  },
  {
    "objectID": "graphing.html#by-the-end-of-this-section-you-will-be-able-to",
    "href": "graphing.html#by-the-end-of-this-section-you-will-be-able-to",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.3 By the end of this section, you will be able to:",
    "text": "4.3 By the end of this section, you will be able to:\n\nDescribe the ggplot “grammar of visualisation”: coordinates and geoms\nWrite a graph function to display multiple variables on a plot\nAmend the titles and legends of a plot\nSave plots in PDF or image formats"
  },
  {
    "objectID": "graphing.html#the-grammar-of-visualisation",
    "href": "graphing.html#the-grammar-of-visualisation",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.4 The “grammar of visualisation”",
    "text": "4.4 The “grammar of visualisation”\n\nGraphs are made up of 3 components:\n\nA dataset\nA coordinate system\nVisual marks to represent data (geoms)\n\n\n\n\nThe “grammar of visualisation” #2\n::: {.cell layout-align=“center”} ::: {.cell-output-display}  ::: :::\n\nIn the above example, the dataset is the studentData that we used previously.\nThe grades variable is mapped to the X axis\nThe hoursOfStudy variable is mapped to the Y axis"
  },
  {
    "objectID": "graphing.html#how-to-code-a-graph",
    "href": "graphing.html#how-to-code-a-graph",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.5 How to code a graph",
    "text": "4.5 How to code a graph\n\nThe graph is created using the following code:\n\n\n\n\n\n\n\n\n\n\nIn this code, we specify the dataset, the variables for the X and Y axes and the geom that will represent the data points visually (in this case, each datum is a point)"
  },
  {
    "objectID": "graphing.html#the-graph-output",
    "href": "graphing.html#the-graph-output",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.6 The graph output",
    "text": "4.6 The graph output\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_point()"
  },
  {
    "objectID": "graphing.html#changing-the-geoms-leads-to-different-visualisations",
    "href": "graphing.html#changing-the-geoms-leads-to-different-visualisations",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.7 Changing the geoms leads to different visualisations",
    "text": "4.7 Changing the geoms leads to different visualisations\n\nIf we change from points to lines, for example we get a different plot:\n\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_line()"
  },
  {
    "objectID": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot",
    "href": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.8 It is possible to represent more variables on the plot",
    "text": "4.8 It is possible to represent more variables on the plot\n\nBy specifying that colours of our points should be attached to the route variable, the data is now colour-coded\n\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_point(aes(color = route))"
  },
  {
    "objectID": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot-2",
    "href": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot-2",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.9 It is possible to represent more variables on the plot #2",
    "text": "4.9 It is possible to represent more variables on the plot #2\n\nBy specifying that size of our points should be attached to the satisfactionLevel variable, the size of the points adjusts\n\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_point(aes(color = route, size=satisfactionLevel))\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot-3",
    "href": "graphing.html#it-is-possible-to-represent-more-variables-on-the-plot-3",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.10 It is possible to represent more variables on the plot #3",
    "text": "4.10 It is possible to represent more variables on the plot #3\n\nBy specifying that shape of our points should be attached to the hasDependents variable, the shape of the points changes accordingly\n\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_point(aes(color = route, size=satisfactionLevel, shape=hasDepdendants))\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "graphing.html#plotting-summaries-of-data",
    "href": "graphing.html#plotting-summaries-of-data",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.11 Plotting summaries of data",
    "text": "4.11 Plotting summaries of data\n\nWe can summarise the data (e.g. get the mean or sd) using the stat_summary() function\nBelow we are making a bar chart with the mean grade for each route\n\n\nggplot(data=studentData, aes(x=route, y= grades, fill=route)) + stat_summary(fun.y = \"mean\", geom = \"bar\")\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead."
  },
  {
    "objectID": "graphing.html#changing-the-axis-labels-and-title-on-a-plot",
    "href": "graphing.html#changing-the-axis-labels-and-title-on-a-plot",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.12 Changing the axis labels and title on a plot",
    "text": "4.12 Changing the axis labels and title on a plot\nWe can change the axis labels and title using the labs() command:\nlabs(x=\"Student Grade\", y=\"Hours of Study\", title = \"Scattterplot of student data\")\n\nlibrary(ggplot2)\n\nggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + geom_point(aes(color = route, size=satisfactionLevel, shape=hasDepdendants)) + labs(x=\"Student Grade\", y=\"Hours of Study\", title = \"Scattterplot of studentdata\")\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "graphing.html#changing-the-legend-on-a-plot",
    "href": "graphing.html#changing-the-legend-on-a-plot",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.13 Changing the legend on a plot",
    "text": "4.13 Changing the legend on a plot\nTo change the legend, we use the labs() command too, and reference the relevant property (e.g. size, shape, colour)\nlabs(x=\"Student Grade\", y=\"Hours of Study\", title = \"Scattterplot of student data\", color=\"Route of study\", size=\"Satisfaction level\", shape=\"Has dependents?\")\n\nlibrary(ggplot2)\n\n  ggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) + \n    geom_point(aes(color = route, size=satisfactionLevel, shape=hasDepdendants)) +\n  labs(x=\"Student Grade\", y=\"Hours of Study\", title = \"Scattterplot of studentdata\", color=\"Route of study\", size=\"Satisfaction level\", shape=\"Has dependents?\")\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "graphing.html#storing-plots-to-be-recalled-later",
    "href": "graphing.html#storing-plots-to-be-recalled-later",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.14 Storing plots to be recalled later",
    "text": "4.14 Storing plots to be recalled later\n\nPlots can be assigned to objects in R and recalled later, just like any other piece of data\n\n\nlibrary(ggplot2)\n\n## Create plot and store it as \"myPlot\" object\n\nmyPlot <- ggplot(data=studentData, aes(x=grades,y=hoursOfStudy)) +\n  geom_point(aes(color = route, size=satisfactionLevel, shape=hasDepdendants)) +\n  labs(x=\"Student Grade\", y=\"Hours of Study\", title = \"Scattterplot of studentdata\", color=\"Route of study\", size=\"Satisfaction level\", shape=\"Has dependents?\")"
  },
  {
    "objectID": "graphing.html#recalling-a-stored-plot",
    "href": "graphing.html#recalling-a-stored-plot",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.15 Recalling a stored plot",
    "text": "4.15 Recalling a stored plot\n\n #Recall myPlot\nmyPlot\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "graphing.html#saving-plots-1",
    "href": "graphing.html#saving-plots-1",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.16 Saving plots # 1",
    "text": "4.16 Saving plots # 1\n\nPlots can be save using the export button in the plots tab"
  },
  {
    "objectID": "graphing.html#plots-can-also-be-saved-using-code",
    "href": "graphing.html#plots-can-also-be-saved-using-code",
    "title": "\n4  Graphing and data visualisation with R\n",
    "section": "\n4.17 Plots can also be saved using code",
    "text": "4.17 Plots can also be saved using code\n\nYou might want to include code to save your plot in a script, for example\nThis can allow greater control over the output file and plot dimensions:\n\n\nggsave(plot= myPlot, file=\"myPlot.pdf\", width = 4, height = 4)\n\nWarning: Using size for a discrete variable is not advised.\n\nggsave(plot= myPlot, file=\"myPlot.png\", width = 4, height = 4, units=\"cm\", dpi=320)\n\nWarning: Using size for a discrete variable is not advised."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "\n5  Correlation\n",
    "section": "",
    "text": "6 Simple Regression"
  },
  {
    "objectID": "correlation.html#what-is-correlation",
    "href": "correlation.html#what-is-correlation",
    "title": "\n5  Correlation\n",
    "section": "\n5.1 What is Correlation?",
    "text": "5.1 What is Correlation?\n\nThe relationship between 2 variables\nQuestion: Is treatment duration related to aggression levels?"
  },
  {
    "objectID": "correlation.html#how-is-correlation-calculated",
    "href": "correlation.html#how-is-correlation-calculated",
    "title": "\n5  Correlation\n",
    "section": "\n5.2 How is correlation calculated?",
    "text": "5.2 How is correlation calculated?\n\nThink of this as covariance divided by individual variance\nIf the changes are consistent with both variables, the final value will be higher"
  },
  {
    "objectID": "correlation.html#running-correlation-in-r",
    "href": "correlation.html#running-correlation-in-r",
    "title": "\n5  Correlation\n",
    "section": "\n5.3 Running correlation in R",
    "text": "5.3 Running correlation in R\n\nStep 1: Check assumptions\n\nData,distribution,linearity\n\n\nStep 2: Run correlation\nStep 3: Check R value\nStep 4: Check significance\n\n\n5.3.1 Check assumptions: data\n\nParametric tests require interval or ratio data\nIf the data are ordinal then a non-parametric correlation is used\n\n\nWhat type of data are treatment duration and aggression level?\n\n\n5.3.2 Check assumptions: distribution\n\nParametric tests require normally distributed data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Check assumptions: distribution #2\n\nParametric tests require normally distributed data\n\n\nshapiro.test(regression_data$treatment_duration)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$treatment_duration\nW = 0.94971, p-value = 0.0007939\n\n\n\nshapiro.test(regression_data$aggression_level)\n\n\n    Shapiro-Wilk normality test\n\ndata:  regression_data$aggression_level\nW = 0.9928, p-value = 0.8756\n\n\n\nThe normality assumption is less of an issue when sample size is > 30\n\n5.3.4 Checking assumptions: linearity\n\n\n\n\n\n\n\n\n\nregression_data %>% ggplot(aes(x=treatment_duration,y=aggression_level)) +\n  geom_point()\n\n\n\n\n\n\n\n\nHere we are looking to see if the relationship is linear\n\n5.3.5 Run correlation\n\nR can run correlations using the cor.test() command\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n5.3.6 Check r Value (correlation value)\n\nThe r value tells us the strength and direction of the relationship\nIn the output it is labelled as “cor” (short for correlation)\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996 \n\n\n\n5.3.7 Check the significance of the correlation\n\nWe can see that the significance by looking at the p value\n\nThe significance is 1.146^-15\nThis means: 0.0000000000000001146\n\n\nTherefore p value < 0.05\n\n\ncor.test(regression_data$treatment_duration,regression_data$aggression_level)\n\n\n    Pearson's product-moment correlation\n\ndata:  regression_data$treatment_duration and regression_data$aggression_level\nt = -9.5503, df = 98, p-value = 1.146e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.7838251 -0.5765006\nsample estimates:\n       cor \n-0.6942996"
  },
  {
    "objectID": "correlation.html#what-is-regression",
    "href": "correlation.html#what-is-regression",
    "title": "\n5  Correlation\n",
    "section": "\n6.1 What is regression?",
    "text": "6.1 What is regression?\n\nTesting to see if we can make predictions based on data that are correlated\n\n\nWe found a strong correlation between treatment duration and agression levels. Can we use this data to predict aggression levels of other clients, based on their treatment duration?\n\n\nWhen we carry out regression, we get a information about:\n\nHow much variance in the outcome is explained by the predictor\n\nHow confident we can be about these results generalising (i.e. significance)\nHow much error we can expect from anu predictions that we make (i.e. standard error of the estimate)\nThe figures we need to calculate a predicted outcome value (i.e. coefficient values)"
  },
  {
    "objectID": "correlation.html#how-is-regression-calculated",
    "href": "correlation.html#how-is-regression-calculated",
    "title": "\n5  Correlation\n",
    "section": "\n6.2 How is regression calculated?",
    "text": "6.2 How is regression calculated?\n\n\n\n\n\n\n\n\n\nWhen we run a regression analysis, a calculation is done to select the “line of best fit”\nThis is a “prediction line” that minimises the overall amount of error\n\nError = difference between the data points and the line"
  },
  {
    "objectID": "correlation.html#the-regression-equation",
    "href": "correlation.html#the-regression-equation",
    "title": "\n5  Correlation\n",
    "section": "\n6.3 The regression equation",
    "text": "6.3 The regression equation\n\n\n\n\n\n\n\n\n\nOnce the line of best fit is calculated, predictions are based on this line\n\nTo make predictions we need the intercept and slope of the line\n\n\nIntercept or constant= where the line crosses the y axis\n\nSlope or beta = the angle of the line\n\n\nPredictions are made using the calculation for a line: Y = bX + c\nYou can think of the equation like this:\n\npredicted outcome value = beta coefficient * value of predictor + constant"
  },
  {
    "objectID": "correlation.html#running-regression-in-r",
    "href": "correlation.html#running-regression-in-r",
    "title": "\n5  Correlation\n",
    "section": "\n6.4 Running regression in R",
    "text": "6.4 Running regression in R\n\nStep 1: Run regression\nStep 2: Check assumptions\n\nData\nDistribution\nLinearity\nHomogeneity of variance\nUncorrelated predictors\nIndpendence of residuals\nNo influental cases / outliers\n\n\nStep 3: Check R^2 value\nStep 4: Check model significance\nStep 5: Check coefficient values"
  },
  {
    "objectID": "correlation.html#run-regression",
    "href": "correlation.html#run-regression",
    "title": "\n5  Correlation\n",
    "section": "\n6.5 Run regression",
    "text": "6.5 Run regression\n\nWe use the lm() command to run regression while saving the results\nWe then use the summary() function to check the results\n\n\nmodel1 <- lm(formula= aggression_level ~ treatment_duration ,data=regression_data)\nsummary(model1)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         12.3300     0.7509   16.42  < 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15"
  },
  {
    "objectID": "correlation.html#what-are-residuals",
    "href": "correlation.html#what-are-residuals",
    "title": "\n5  Correlation\n",
    "section": "\n6.6 What are residuals?",
    "text": "6.6 What are residuals?\n\nIn regression, the assumptions apply to the residuals, not the data themselves\nResidual just means the difference between the data point and the regression line"
  },
  {
    "objectID": "correlation.html#check-assumptions-distribution-1",
    "href": "correlation.html#check-assumptions-distribution-1",
    "title": "\n5  Correlation\n",
    "section": "\n6.7 Check assumptions: distribution",
    "text": "6.7 Check assumptions: distribution\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe second plot that it outputs shows the normality\n\n\nplot(model1, which=2)\n\n\n\n\n\n\n\n\nWe could also use a histogram to check the distribution\nNotice how we can use the $ sign to get the residuals from the model\n\n\nhist(model1$residuals)"
  },
  {
    "objectID": "correlation.html#check-assumptions-linearity",
    "href": "correlation.html#check-assumptions-linearity",
    "title": "\n5  Correlation\n",
    "section": "\n6.8 Check assumptions: linearity",
    "text": "6.8 Check assumptions: linearity\n\nUsing the plot() command on our regression model will give us some useful diagnostic plots\nThe first plot that it outputs shows the residuals vs the fitted values\nHere, we want to see them spread out, with the line being horizontal and straight\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\nThere is a slight amount of curvilinearity here but nothing to be worried about"
  },
  {
    "objectID": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "href": "correlation.html#check-assumptions-homogeneity-of-variance-1",
    "title": "\n5  Correlation\n",
    "section": "\n6.9 Check assumptions: Homogeneity of Variance #1",
    "text": "6.9 Check assumptions: Homogeneity of Variance #1\n\nWe can use the sample plot to check Homogeneity of Variance\nWe want the variance to be constant across the data set. We do not want the variance to change at different points in the data\n\n\nplot(model1, which=1)\n\n\n\n\n\n\n\n\nA violation of Homogeneity of Variance would usually look like a funnel, with the data narrowing"
  },
  {
    "objectID": "correlation.html#check-assumptions-influential-cases",
    "href": "correlation.html#check-assumptions-influential-cases",
    "title": "\n5  Correlation\n",
    "section": "\n6.10 Check assumptions: Influential cases",
    "text": "6.10 Check assumptions: Influential cases\n\nWe need to check that there are no extreme outliers - they could throw off our predictions\nWe are looking for participants that have high rediduals + high leverage\n\nSome guidance suggests anything higher than 1 is an influential case\nOthers suggest 4/n is the cut off point (4 divided by number of participants)\n\n\n\n\nplot(model1, which=4)\n\n\n\n\n\n\n\n\nWe are looking for participants that have high rediduals + high leverage\n\nNo cases over 1\nMany are over 0.04 (4/n = 0.04)\n\n\n\n\nplot(model1, which=5)"
  },
  {
    "objectID": "correlation.html#check-the-r-squared-value",
    "href": "correlation.html#check-the-r-squared-value",
    "title": "\n5  Correlation\n",
    "section": "\n6.11 Check the r squared value",
    "text": "6.11 Check the r squared value\n\nr^2 = the amount of variance in the outcome that is explained by the predictor(s)\n\nThe closer this value is to 1, the more useful our regression model is for predicting the outcome\n\n\nmodelSummary <- summary(model1)\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         12.3300     0.7509   16.42  < 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15\n\n\n\nThe r^2 of 0.482052 means that 48% of the variance in aggression level is explained by treatment duration"
  },
  {
    "objectID": "correlation.html#check-model-significance",
    "href": "correlation.html#check-model-significance",
    "title": "\n5  Correlation\n",
    "section": "\n6.12 Check model significance",
    "text": "6.12 Check model significance\n\nThe model significance is displayed at the very end of the output\n\np-value: 1.146e-15\nAs p < 0.05, the model is significant\n\n\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         12.3300     0.7509   16.42  < 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15"
  },
  {
    "objectID": "correlation.html#check-coefficient-values",
    "href": "correlation.html#check-coefficient-values",
    "title": "\n5  Correlation\n",
    "section": "\n6.13 Check coefficient values",
    "text": "6.13 Check coefficient values\n\nThe coefficient values are displayed in the coefficients table\nIf we have more than one predictor, they are all listed here\n\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15\n\n\n\nThe beta coefficient for treatment duration is in the Estimate column\nFor every unit increase in treatment duration, aggression level decreases by 0.69"
  },
  {
    "objectID": "correlation.html#the-regression-equation-1",
    "href": "correlation.html#the-regression-equation-1",
    "title": "\n5  Correlation\n",
    "section": "\n6.14 The regression equation",
    "text": "6.14 The regression equation\n\nThe regression equation is:\n\nOutcome = predictor value * beta coefficient + constant\n\nFor this model, that is:\n\nAggression level = treatment duration * -0.69 + 12.33\n\nmodelSummary$coefficients\n\n                     Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)        12.3300211 0.75087601 16.420848 6.840516e-30\ntreatment_duration -0.6933201 0.07259671 -9.550297 1.145898e-15"
  },
  {
    "objectID": "correlation.html#accounting-for-error-in-predictions",
    "href": "correlation.html#accounting-for-error-in-predictions",
    "title": "\n5  Correlation\n",
    "section": "\n6.15 Accounting for error in predictions",
    "text": "6.15 Accounting for error in predictions\n\nWe also know that the accuracy of predictions will be within a certain margin of error\nThis is known as standard error of the estimate or residual standard error\n\n\n\nmodelSummary\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration, data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4251 -1.1493 -0.0593  0.8814  3.4542 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         12.3300     0.7509   16.42  < 2e-16 ***\ntreatment_duration  -0.6933     0.0726   -9.55 1.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.551 on 98 degrees of freedom\nMultiple R-squared:  0.4821,    Adjusted R-squared:  0.4768 \nF-statistic: 91.21 on 1 and 98 DF,  p-value: 1.146e-15"
  },
  {
    "objectID": "multipleRegression.html#by-the-end-of-this-session-you-will-be-able-to",
    "href": "multipleRegression.html#by-the-end-of-this-session-you-will-be-able-to",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.1 By the end of this session, you will be able to:",
    "text": "6.1 By the end of this session, you will be able to:\n\nCompare multiple regression to simple regression\nDescribe the assumptions of multiple regression\nConsider sample size in regression\nUse categorical predictors in regression in R\nConduct different types of multiple regression\nInterpret the output of Multiple regression"
  },
  {
    "objectID": "multipleRegression.html#what-is-multiple-regression",
    "href": "multipleRegression.html#what-is-multiple-regression",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.2 What is multiple regression?",
    "text": "6.2 What is multiple regression?\n\nAn extension of simple regression\nSame format as simple regression but adding each predictor:\n\n\\[ Y = b_1X_1 + b_2X_2 + b_0 \\]\n(The constant can be referred to in the equation as c or b0 )"
  },
  {
    "objectID": "multipleRegression.html#what-are-the-assumptions-of-multiple-regression",
    "href": "multipleRegression.html#what-are-the-assumptions-of-multiple-regression",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.3 What are the assumptions of Multiple Regression?",
    "text": "6.3 What are the assumptions of Multiple Regression?\n\nThey are primarily the same as simple regression\nThe additional assumption of no multicollinearity (due to having multiple predictors)\n\ni.e. predictors should not be highly correlated"
  },
  {
    "objectID": "multipleRegression.html#what-is-multicollinearity",
    "href": "multipleRegression.html#what-is-multicollinearity",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.4 What is multicollinearity?",
    "text": "6.4 What is multicollinearity?\n\nMulticollinearity = predictors correlated highly with each other.\nThis is not good because:\n\nIt makes it difficult to determine the role of individual predictors\nIncreases the error of the model (higher standard errors)\nDifficult to identify significant predictors - wider confidence interval"
  },
  {
    "objectID": "multipleRegression.html#testing-multicollinearity",
    "href": "multipleRegression.html#testing-multicollinearity",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.5 Testing multicollinearity",
    "text": "6.5 Testing multicollinearity\n\n## use the mctest package\n# install.packages(‘mctest’)o\nlibrary(mctest)\n\nm1 <- lm(aggression_level ~ treatment_group + treatment_duration + trust_score, data=regression_data)\n\nmctest(m1) \n\n\nCall:\nomcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, \n    theil = theil, cn = cn)\n\n\nOverall Multicollinearity Diagnostics\n\n                       MC Results detection\nDeterminant |X'X|:         0.9229         0\nFarrar Chi-Square:         7.7960         0\nRed Indicator:             0.1547         0\nSum of Lambda Inverse:     3.1728         0\nTheil's Method:           -0.8800         0\nCondition Number:         13.6549         0\n\n1 --> COLLINEARITY is detected by the test \n0 --> COLLINEARITY is not detected by the test\n\n\n\n\nThe format of mctest() is:\nmctest(predictors, outcome)\n\nIn the above example we used the cbind() function to bind 3 columns of data together (the predictors)"
  },
  {
    "objectID": "multipleRegression.html#sample-size-for-multiple-regression",
    "href": "multipleRegression.html#sample-size-for-multiple-regression",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.6 Sample size for multiple regression",
    "text": "6.6 Sample size for multiple regression\n\nIs based on the number of predictors\nMore predictors = more participants needed\nDo a power analysis\nLoose “rule of thumb” = 10-15 participants per predictor"
  },
  {
    "objectID": "multipleRegression.html#approaches-to-multiple-regression-all-predictors-at-once",
    "href": "multipleRegression.html#approaches-to-multiple-regression-all-predictors-at-once",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.7 Approaches to multiple regression: All predictors at once",
    "text": "6.7 Approaches to multiple regression: All predictors at once\n\nResearch question: Do a client’s treatment duration and treatment group predict aggression level?\n\n\nmodel1 <- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group)\n\n\nHere we are including all of the predictors at the same time\nNote that we are using a plus sign + between each predictor\n\nThis means that no interactions will be tested\n\n\n\n\n6.7.1 Using categorical predictors in R\n\nTreatment group is a categorical (also called “nominal” or “factor”) variable\nNo special “dummy coding” is required in R to use categorical predictors in regression\nR will use the first group as the reference category and test whether being in another group shows a significant difference\nR chooses the reference group based on numerical value or alphabetical order\nIf you want you can change the reference category or “force” it using the relevel function:\n\n\nregression_data$treatment_group <- relevel(regression_data$treatment_group, ref = \"therapy1\")\n\nMore information in categorical predictors in section @ref(catreg) \n\n6.7.2 Reviewing the output\n\nsummary(model1)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration + treatment_group, \n    data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9468 -1.1104  0.0205  0.9621  3.4481 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             11.58713    0.77331  14.984  < 2e-16 ***\ntreatment_duration      -0.66024    0.07119  -9.274 4.96e-15 ***\ntreatment_grouptherapy2  0.85032    0.30449   2.793   0.0063 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.5 on 97 degrees of freedom\nMultiple R-squared:  0.5206,    Adjusted R-squared:  0.5107 \nF-statistic: 52.67 on 2 and 97 DF,  p-value: 3.267e-16\n\n\n\nMultiple R^2 = Total variance in outcome that is explained by the model\np-value = Statistical significance of the model\nCoefficients = Contribution of each predictor to the model\n\nPr = Significance of the individual predictor\nEstimate = Change in the outcome level that occurs when the predictor increases by 1 unit of measurement\n\n\n\n6.7.3 All predictors at once (testing interactions)\n\nResearch questions: - Do a client’s treatment duration and treatment group predict aggression level - Do the predictors interact?\n\n\nmodel2 <- lm(data = regression_data, aggression_level ~ treatment_duration * treatment_group)\n\n\nHere we are including all of the predictors at the same time\nNote that we are using an asterisk * between each predictor\n\nThis means that interactions will be tested\n\n\n\nReviewing the output\n\n\nsummary(model2) %>% coefficients\n\n                                             Estimate Std. Error    t value\n(Intercept)                                12.3529190  1.1006127 11.2236751\ntreatment_duration                         -0.7334435  0.1033086 -7.0995381\ntreatment_grouptherapy2                    -0.5615517  1.4753596 -0.3806202\ntreatment_duration:treatment_grouptherapy2  0.1394649  0.1425977  0.9780305\n                                               Pr(>|t|)\n(Intercept)                                3.599000e-19\ntreatment_duration                         2.166226e-10\ntreatment_grouptherapy2                    7.043260e-01\ntreatment_duration:treatment_grouptherapy2 3.305175e-01\n\n\n\n\nWe get additional information in the coefficients table about the interaction between variables\n\ne.g. does the interaction between level of trust and treatment duration predict the outcome (aggression level)?\n\n\nWe can see from the output that none of the interactions are significant\n\n6.7.4 Hierarchical multiple regression: Theory driven “blocks” of variables\n\nIt might be the case that we have previous research or theory to guide how we run the analysis\nFor example, we might know that treatment duration and therapy group are likely to predict the outcome\nWe might want to check whether client’s level of trust in the clinician has any additional impact on our ability to predict the outcome (aggression level)\n\n\n\nTo do this, we run three regression models\n\nModel 0: the constant (baseline)\nModel 1: treatment duration and therapy group\nModel 2: treatment duration and therapy group and trust score\n\n\n\n\n\nWe then compare the two regression models to see if:\n\nModel 1 is better than Model 0 (the constant)\nModel 2 is better than Model 1\n\n\n\nHierarchical multiple regression: Running and comparing 2 models\n\n## run regression using the same method as above\nmodel0 <- lm(data = regression_data, aggression_level ~ 1)\nmodel1 <- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group)\nmodel2 <- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group + trust_score)\n\n## use the aov() command to compare the models\nanova(model0,model1,model2)\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(>F)\n\n\n\n99\n455.2727\nNA\nNA\nNA\nNA\n\n\n97\n218.2601\n2\n237.0125863\n52.2194515\n0.000000\n\n\n96\n217.8614\n1\n0.3986883\n0.1756808\n0.676048\n\n\n\n\n\n\n\nWe can see that:\n\nModel 1 (treatment duration and treatment group) is significant relative to the constant (Model 0)\nModel 2 (treatment duration, treatment group and trust score) shows no significant change compared to Model 1\n\n\n\n6.7.5 Stepwise multiple regression: computational selection of predictors\n\nStepwise multiple regression is controversial because:\n\nThe computer selects which predictors to include based on Akaike information criterion (AIC)\n\nThis is a calculation of the quality of statistical models when they are compared to each other\n\n\n\n\n\n6.7.6 What’s the problem?\n\nThis selection is not based on any underlying theory or understanding of the real-life relationship between the variables\n\n\n\n\n6.7.7 Stepwise multiple regression: loading the MASS package and run the full model\n\ninstall and load the MASS package\nrun a regression model with all of the variables\nuse the stepAIC() command on the full model to run stepwise regression\nView the best model\n\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.2.3\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# Run the full model \nfull.model <- lm(data = regression_data, aggression_level ~ treatment_duration + treatment_group + trust_score)\n\n\n6.7.8 Stepwise multiple regression: Use stepAIC( ) with options\n\n\nTrace (TRUE or FALSE): do we want to see the steps that were involved in selecting the best model ?\n\nDirection (“forward”, “backward” or “both”):\n\nstart with no variables and add them (forward)\n\nstart with all variables and subtract them (backward)\n\nuse both approaches (both)\n\n\n\n\n\n\n# Run stepwise\nstep.model <- stepAIC(full.model, direction = \"both\", trace = TRUE)\n\nStart:  AIC=85.87\naggression_level ~ treatment_duration + treatment_group + trust_score\n\n                     Df Sum of Sq    RSS     AIC\n- trust_score         1     0.399 218.26  84.052\n<none>                            217.86  85.869\n- treatment_group     1    17.877 235.74  91.755\n- treatment_duration  1   188.709 406.57 146.259\n\nStep:  AIC=84.05\naggression_level ~ treatment_duration + treatment_group\n\n                     Df Sum of Sq    RSS     AIC\n<none>                            218.26  84.052\n+ trust_score         1     0.399 217.86  85.869\n- treatment_group     1    17.547 235.81  89.785\n- treatment_duration  1   193.515 411.78 145.531\n\n\n\n\n6.7.9 Stepwise multiple regression: Display the best model\n\ninstall and load the MASS package\nrun a regression model with all of the variables\nuse the stepAIC() command on the full model to run stepwise regression\nView best model\n\n\n#view the stepwise output\nsummary(step.model)\n\n\nCall:\nlm(formula = aggression_level ~ treatment_duration + treatment_group, \n    data = regression_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9468 -1.1104  0.0205  0.9621  3.4481 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             11.58713    0.77331  14.984  < 2e-16 ***\ntreatment_duration      -0.66024    0.07119  -9.274 4.96e-15 ***\ntreatment_grouptherapy2  0.85032    0.30449   2.793   0.0063 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.5 on 97 degrees of freedom\nMultiple R-squared:  0.5206,    Adjusted R-squared:  0.5107 \nF-statistic: 52.67 on 2 and 97 DF,  p-value: 3.267e-16"
  },
  {
    "objectID": "multipleRegression.html#catreg",
    "href": "multipleRegression.html#catreg",
    "title": "\n6  Multiple Regression\n",
    "section": "\n6.8 Using regression with categorical predictors (more information)",
    "text": "6.8 Using regression with categorical predictors (more information)\nIn the below video, you can click the icon in the top right of the video to change the layout (and remove my face, if you want!)\n\n\nPeople are often taught to use ANOVA to compare groups (i.e. if you have a categorical IV) and regression if you have continuous IVs. However, ANOVA and regression are the same thing, so it is possible to use regression to do analysis instead of ANOVA or ANCOVA.\nHowever, it might be difficult to understand how this is, so let’s look at an example. The dataset Baumann compares 3 different methods of teaching reading comprehension. For this example, we will just look at the variable post.test.1 as the DV.\n\n6.8.1 ANOVA Approach\nANOVA asks the question in the following way:\n\nIs there a difference in reading comprehension scores between teaching groups?\n\nThe analysis takes the following approach:\n\nWhat are the means of groups 1,2 and 3?\nAre the means of groups 1,2 and 3 different?\nIs the difference in means of groups 1,2 and 3 statistically significant?\n\nIf we were to summarise the data, we might present it in the following way:\n\n\n\n\ngroup\nmean\nsd\n\n\n\nBasal\n6.681818\n2.766920\n\n\nDRTA\n9.772727\n2.724349\n\n\nStrat\n7.772727\n3.927095\n\n\n\n\n\nIn the table above we can see that the mean scores are different and highest in the DRTA group.\nIf we were to run an ANOVA on the data, we might present it in the following way:\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\ngroup\n2\n108.1212\n54.06061\n5.317437\n0.0073468\n\n\nResiduals\n63\n640.5000\n10.16667\nNA\nNA\n\n\n\n\n\nNotice that the ANOVA output tells us that the difference between groups is significant (p < 0.05) but we cannot tell yet which of the 3 groups are significantly different from each other.\n\n6.8.2 Regression approach\nRegression asks the question the following way:\n\nDoes teaching group predict reading comprehension score?\n\nThe analysis takes the following approach:\n\nLet’s use the mean of group 1 as a reference point (i.e. the intercept).\nWhat’s the difference between the intercept and the mean scores of the other groups (i.e. the coefficients)?\nAre any of the coefficients statistically significant?\n\nIf we run a regression analysis, we might present the results like this:\n\n\n\n\nR2\n\n\n0.1444271\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\ngroup\n2\n108.1212\n54.06061\n5.317437\n0.0073468\n\n\nResiduals\n63\n640.5000\n10.16667\nNA\nNA\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n6.681818\n0.6797950\n9.829167\n0.0000000\n\n\ngroupDRTA\n3.090909\n0.9613753\n3.215091\n0.0020583\n\n\ngroupStrat\n1.090909\n0.9613753\n1.134738\n0.2607841\n\n\n\n\n\n\n6.8.3 Interpreting regression output\nIf we look at the coefficient (estimate) for the intercept (see regression output above), we can see that the value is the same as the mean of the Basal group in the previous section (See table of mean and sd, above).\nFurthermore, if we look at the estimates of DRTA and Strat, we can see that the values are the difference between their mean score, and the score for of the intercept (BASAL) group. So we can see whether DTRA and STRAT groups are significantly different from the BASAL group.\nIf we wanted to compare the groups differently (e.g. using Strat as the reference point), we can use the relevel function and run the regression analysis again (See Using categorical predictors in R)"
  },
  {
    "objectID": "moderation.html#overview",
    "href": "moderation.html#overview",
    "title": "\n7  Moderation analysis\n",
    "section": "\n7.1 Overview",
    "text": "7.1 Overview\n\nWhat is moderation?\nModeration analysis in more detail\nGrand Mean Centering\nChecking Assumptions\nInterpreting Moderation\nBootstrapping Moderation"
  },
  {
    "objectID": "moderation.html#what-is-moderation",
    "href": "moderation.html#what-is-moderation",
    "title": "\n7  Moderation analysis\n",
    "section": "\n7.2 What is moderation?",
    "text": "7.2 What is moderation?\nThere is a direct relationship between X and Y but it is affected by a moderator (M)\n\n\nA moderated relationship\n\n\n\n\n\nIn the above model, we theorise that Time in counselling predicts General Wellbeing but the strength of the relationship is affected by the level of Rapport with counsellor"
  },
  {
    "objectID": "moderation.html#what-packages-do-we-need",
    "href": "moderation.html#what-packages-do-we-need",
    "title": "\n7  Moderation analysis\n",
    "section": "\n7.3 What packages do we need?",
    "text": "7.3 What packages do we need?\n\n\ngvlma (for checking assumptions)\n\ninteractions (for generating interaction plot)\n\nRockchalk (for testing simple slopes)\n\ncar (includes a Boot() function to bootstrap regression models )"
  },
  {
    "objectID": "moderation.html#what-is-moderation-1",
    "href": "moderation.html#what-is-moderation-1",
    "title": "\n7  Moderation analysis\n",
    "section": "\n7.4 What is moderation?",
    "text": "7.4 What is moderation?\n\nThe relationship between a predictor (X) and outcome (Y) is affected by another variable (M)\nThis is referred to as an interaction (similar to interaction in standard regression)\nA moderator can effect the direction and/or strength of a relationship between X and Y\n\n\n\nA moderated relationship\n\n\n\n\n\nHere we might find that the relationship between Time in counselling and General Wellbeing is strong for those who have a strong rapport with their counselling psychologist and weak for those who do not have good rapport with their counselling psychologist.\n\n\nVery similar to multiple regression\nlm(Y ~ X + M + X*M)\n\nModeration analysis includes X, Z and the interaction between X and Z\nIf we find a moderation effect it becomes the focus of our analysis (the independent role of X and Z becomes less important)\n\n\n\n\n\n\n\n\n\nIn the plot above:\n\nThe blue line is the “standard” regression line\nThe black line is when the moderator is “low” (-1sd)\nThe dotted line is when the moderator is “high” (+1sd)"
  },
  {
    "objectID": "moderation.html#moderation-step-by-step",
    "href": "moderation.html#moderation-step-by-step",
    "title": "\n7  Moderation analysis\n",
    "section": "\n7.5 Moderation: step-by-step",
    "text": "7.5 Moderation: step-by-step\n\n7.5.1 Step 1: Grand Mean Centering\n\nRegression coefficients (b values) are based on predicting Y when X = 0\nNot all measures actually have a zero value\nTo make results easier to interpret, we can centre our data around the grand mean of the data (making the mean 0)\n\nThe mean of the full sample is subtracted from the value\n\n\nThis is similar to z-score (i.e. a standardised score)\n\nTo do this in R, we can use the scale() function:\n\n    timeInCounselling_centred    <- scale(timeInCounselling, center=TRUE, scale=FALSE) #Centering X; \n    rapportLevel_centred    <- scale(rapportLevel,  center=TRUE, scale=FALSE) #Centering M;\n\nWe then use the centred data in our analysis\nWe can see that the difference between the original data is the mean of the data.\n\n  timeInCounselling_centred    <- scale(timeInCounselling, center=TRUE, scale=FALSE) #Centering X; \n  \n  timeInCounselling\n \n  head(timeInCounselling_centred)\n  \n   mean(timeInCounselling)\n  timeInCounselling[1]-timeInCounselling_centred[1]\n\n  [1]  3.7580974  5.0792900 12.2348333  6.2820336  6.5171509 12.8602599\n  [7]  7.8436648  0.9397551  3.2525886  4.2173521 10.8963272  7.4392553\n [13]  7.6030858  6.4427309  3.7766355 13.1476525  7.9914019  1.8664686\n [19]  8.8054236  4.1088344  1.7287052  5.1281003  1.8959822  3.0844351\n [25]  3.4998429  0.7467732  9.3511482  6.6134925  1.4474523 11.0152597\n [31]  7.7058569  4.8197141  9.5805026  9.5125340  9.2863243  8.7545610\n [37]  8.2156706  5.7523532  4.7761493  4.4781160  3.2211721  5.1683309\n [43]  0.9384146 14.6758239 10.8318480  1.5075657  4.3884607  4.1333786\n [49]  9.1198605  5.6665237  7.0132741  5.8858130  5.8285182 11.4744091\n [55]  5.0969161 12.0658824  0.1950112  8.3384550  6.4954170  6.8637663\n [61]  7.5185579  3.9907062  4.6671705  1.9256985  1.7128351  7.2141146\n [67]  7.7928391  6.2120169  9.6890699 14.2003387  4.0358753  3.2366755\n [73] 10.0229541  3.1631969  3.2479655 10.1022855  4.8609080  1.1171292\n [79]  6.7252139  5.4444346  6.0230567  7.5411216  4.5173599  8.5775062\n [85]  5.1180538  7.3271279 10.3873561  7.7407260  4.6962737 10.5952305\n [91]  9.9740154  8.1935878  6.9549269  3.4883757 11.4426098  3.5989617\n [97] 14.7493320 12.1304425  5.0571986  1.8943164\n            [,1]\n[1,] -2.72442479\n[2,] -1.40323216\n[3,]  5.75231105\n[4,] -0.20048864\n[5,]  0.03462873\n[6,]  6.37773774\n[1] 6.482522\n[1] 6.482522\n\n\n\n#Centering Data\nModdata$timeInCounselling_centred    <- c(scale(timeInCounselling, center=TRUE, scale=FALSE)) \n\n#Centering IV; \nModdata$rapportLevel_centred    <- c(scale(rapportLevel,  center=TRUE, scale=FALSE)) #Centering moderator; \n\n#Moderation \"By Hand\" with centred data\nlibrary(gvlma)\nfitMod <- lm(generalWellbeing ~ timeInCounselling_centred *rapportLevel_centred  , data = Moddata) #Model interacts IV & moderator\n\nlibrary(interactions)\n ip <- interact_plot(fitMod, pred = timeInCounselling_centred, modx = rapportLevel_centred)\n ip\n\n\n\n\n\n\n\n\n7.5.1.1 Do I need to mean centre my data?\nIt is worth noting:\n\nIt does not change the results of your interaction (coefficient, standard error or significance tests).\nIt will change the results of the direct effects (the individual predictors in your model).\nIt is a step that tries to ensure that the coefficients of the predictor and moderator are meaningful in relation to each other.\nIn some cases, it might not be necessary to mean centre at all. However, there is no harm in doing so, and it could potentially be helpful.\n\nHayes (2013) discusses mean centering, pp. 282-290.\nrapportLevel_centredClelland, G. H., Irwin, J. R., Disatnik, D., & Sivan, L. (2017). Multicollinearity is a red herring in the search for moderator variables: A guide to interpreting moderated multiple regression models and a critique of Iacobucci, Schneider, Popovich, and Bakamitsos (2016). Behavior research methods, 49(1), 394-402.\n\n7.5.2 Step 2: Check assumptions\nWe can use the gvlma function to check regression assumptions\n\nlibrary(gvlma)\ngvlma(fitMod)\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, \n    data = Moddata)\n\nCoefficients:\n                                   (Intercept)  \n                                       21.1851  \n                     timeInCounselling_centred  \n                                        0.8971  \n                          rapportLevel_centred  \n                                        0.5842  \ntimeInCounselling_centred:rapportLevel_centred  \n                                        0.1495  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitMod) \n\n                    Value p-value                   Decision\nGlobal Stat        9.6949 0.04589 Assumptions NOT satisfied!\nSkewness           7.7571 0.00535 Assumptions NOT satisfied!\nKurtosis           1.2182 0.26972    Assumptions acceptable.\nLink Function      0.5287 0.46716    Assumptions acceptable.\nHeteroscedasticity 0.1910 0.66207    Assumptions acceptable.\n\n\nThe “global stat” is an attempt to check multiple assumptions of linear model: Pena, E. A., & Slate, E. H. (2006). Global validation of linear model assumptions. Journal of the American Statistical Association, 101(473), 341-354.\nSince one of the underlying assumptions is violated, the overall stat is also not acceptable.\nThe data looks skewed, we should transform it or perhaps use bootstrapping\n\n7.5.3 Step 3: Moderation Analysis\n\nfitMod <- lm(generalWellbeing ~ timeInCounselling_centred *rapportLevel_centred  , data = Moddata) #Model interacts IV & moderator\n #Model interacts IV & moderator\nsummary(fitMod)\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling_centred * rapportLevel_centred, \n    data = Moddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.121  -8.938  -0.670   5.840  37.396 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                    21.18508    1.14115  18.565\ntimeInCounselling_centred                       0.89707    0.33927   2.644\nrapportLevel_centred                            0.58416    0.15117   3.864\ntimeInCounselling_centred:rapportLevel_centred  0.14948    0.04022   3.716\n                                               Pr(>|t|)    \n(Intercept)                                     < 2e-16 ***\ntimeInCounselling_centred                      0.009569 ** \nrapportLevel_centred                           0.000203 ***\ntimeInCounselling_centred:rapportLevel_centred 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.33 on 96 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.251 \nF-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07\n\n\nThe results above show that there is a moderated effect\n\n7.5.3.1 Visualising the moderation effect\nWe use an approach called simple slopes to visualise the moderation effect\ninteract_plot(fitMod, pred = timeInCounselling_centred, modx = rapportLevel_centred)\n\n\n\n\n\n\n\n\nThe rockchalk package includes useful functions for visualising simple slopes\n\nlibrary(rockchalk)\n\nfitMod <- lm(generalWellbeing ~ timeInCounselling *rapportLevel  , data = Moddata)\nsummary(fitMod)\nslopes <- plotSlopes(fitMod, modx = \"rapportLevel\", plotx = \"timeInCounselling\")\n\n\n\n\n\n\ntestSlopes <- testSlopes(slopes)\nplot(testSlopes)\n\n\n\n\n\n\n\n\nCall:\nlm(formula = generalWellbeing ~ timeInCounselling * rapportLevel, \n    data = Moddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.121  -8.938  -0.670   5.840  37.396 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                    17.28006    3.17944   5.435 4.15e-07 ***\ntimeInCounselling               0.15510    0.42033   0.369  0.71296    \nrapportLevel                   -0.38484    0.29916  -1.286  0.20140    \ntimeInCounselling:rapportLevel  0.14948    0.04022   3.716  0.00034 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.33 on 96 degrees of freedom\nMultiple R-squared:  0.2737,    Adjusted R-squared:  0.251 \nF-statistic: 12.06 on 3 and 96 DF,  p-value: 9.12e-07\n\nValues of rapportLevel OUTSIDE this interval:\n        lo         hi \n-11.580166   3.634439 \ncause the slope of (b1 + b2*rapportLevel)timeInCounselling to be statistically significant\n\n\n\n7.5.4 Step 4: Bootstrapping\nThe car package includes a function to bootstrap regression\n\nlibrary(car)\n\nLoading required package: carData\n\n\nWarning: package 'carData' was built under R version 4.2.3\n\nbootstrapModel <- Boot(fitMod, R=999)\n\nconfint(fitMod)\nconfint(bootstrapModel)\nsummary(bootstrapModel)\nhist(bootstrapModel)\n\n\n\n\n\n\n\n                                     2.5 %     97.5 %\n(Intercept)                    10.96891826 23.5912086\ntimeInCounselling              -0.67926290  0.9894532\nrapportLevel                   -0.97866229  0.2089882\ntimeInCounselling:rapportLevel  0.06963667  0.2293205\nBootstrap bca confidence intervals\n\n                                     2.5 %     97.5 %\n(Intercept)                    11.57230420 23.7222700\ntimeInCounselling              -0.61780918  1.0397199\nrapportLevel                   -0.90786799  0.2558502\ntimeInCounselling:rapportLevel  0.05806412  0.2146814\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\noriginal\nbootBias\nbootSE\nbootMed\n\n\n\n(Intercept)\n999\n17.2800634\n-0.1366710\n3.1653009\n17.0543132\n\n\ntimeInCounselling\n999\n0.1550951\n0.0163712\n0.3995498\n0.1592933\n\n\nrapportLevel\n999\n-0.3848371\n0.0071663\n0.2940609\n-0.3821849\n\n\ntimeInCounselling:rapportLevel\n999\n0.1494786\n-0.0005284\n0.0385163\n0.1497413"
  },
  {
    "objectID": "mediation.html#overview",
    "href": "mediation.html#overview",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.1 Overview",
    "text": "8.1 Overview\n\nWhat are mediation and moderation?\nMediation analysis example\nPackages needed\nBaron and Kenny approach in R\nMediation package approach in R"
  },
  {
    "objectID": "mediation.html#what-is-mediation",
    "href": "mediation.html#what-is-mediation",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.2 What is mediation?",
    "text": "8.2 What is mediation?\nWhere the relationship between a predictor (X) and an outcome (Y) is mediated by another variable (M).\n\n\n\n\n\nA mediated relationship\n\n\nIn the above model, we theorise that socio-economic status predicts education level, which predicts future prospects."
  },
  {
    "objectID": "mediation.html#what-is-moderation",
    "href": "mediation.html#what-is-moderation",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.3 What is moderation?",
    "text": "8.3 What is moderation?\nThere is a direct relationship between X and Y but it is affected by a moderator (M)\n\n\n\n\n\nA moderated relationship\n\n\nIn the above model, we theorise that socio-economic status predicts future prospects but the strength of the relationship is changed by education level"
  },
  {
    "objectID": "mediation.html#why-different-models",
    "href": "mediation.html#why-different-models",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.4 Why different models?",
    "text": "8.4 Why different models?\n\n\n\n\n\nA mediated relationship\n\n\nThis might be more appropriate if higher education costs money\n\n\n\n\n\nA moderated relationship\n\n\nThis might be more appropriate if access to higher education is free"
  },
  {
    "objectID": "mediation.html#mediation-analysis",
    "href": "mediation.html#mediation-analysis",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.5 Mediation analysis",
    "text": "8.5 Mediation analysis\n\n8.5.1 What is a mediation design?\nWhether a mediation analysis is appropriate is determined as much by the design as by statistical criteria.\n\n\n\n\n\nA mediated relationship\n\n\nWe must consider whether it makes sense to predict this relationship between variables\n\n8.5.2 What is mediation analysis?\n\nBased on regression\n\nA summary of the logic of mediation:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\n\n\n\n\n\nThe direct relationship between X and Y should be significant\n The relationship between X and M should be significant \n The relationship between M and Y (controlling for X) should be significant \n When controlling for M, the strength of the relationship between X and Y decreases and is not significant \n\n\nBaron & Kenny (1986) originally used a 4-step regression model to test each of these relationships.\n\n\n8.5.3 What packages do we need?\n\n    library(mediation) #Mediation package\n    \n    library(multilevel) #Sobel Test\n    \n    library(bda) #Another Sobel Test option\n    \n    library(gvlma) #Testing Model Assumptions \n    \n    library(stargazer) #Handy regression tables"
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "href": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.6 Mediation analysis (the Baron and Kenny Approach)",
    "text": "8.6 Mediation analysis (the Baron and Kenny Approach)\n\n8.6.1 Conducting mediation analysis (the Baron and Kenny Approach)\n\nBaron & Kenny (1986) originally used a 4-step regression model to test each of these relationships.\nThe sobel test is then used to test the significance of mediation\n\n\n\n\n\n8.6.2 Step 1: Total Effect\n\n#1. Total Effect\nfit <- lm(Y ~ X, data=Meddata)\nsummary(fit)\n\n\nCall:\nlm(formula = Y ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.917  -3.738  -0.259   2.910  12.540 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 19.88368   14.26371   1.394   0.1665  \nX            0.16899    0.08116   2.082   0.0399 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.16 on 98 degrees of freedom\nMultiple R-squared:  0.04237,   Adjusted R-squared:  0.0326 \nF-statistic: 4.336 on 1 and 98 DF,  p-value: 0.03993\n\n\n\n8.6.3 Step 2: Path A (X on M)\n\n#2. Path A (X on M)\nfita <- lm(M ~ X, data=Meddata)\nsummary(fita)\n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5367 -3.4175 -0.4375  2.9032 16.4520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.04494   13.41692   0.451    0.653    \nX            0.66252    0.07634   8.678 8.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.854 on 98 degrees of freedom\nMultiple R-squared:  0.4346,    Adjusted R-squared:  0.4288 \nF-statistic: 75.31 on 1 and 98 DF,  p-value: 8.872e-14\n\n\n\n8.6.4 Step 3: Path B (M on Y, controlling for X)\n\n#3. Path B (M on Y, controlling for X)\nfitb <- lm(Y ~ M + X, data=Meddata)\nsummary(fitb)\n\n\nCall:\nlm(formula = Y ~ M + X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3651 -3.3037 -0.6222  3.1068 10.3991 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.32177   13.16216   1.316    0.191    \nM            0.42381    0.09899   4.281 4.37e-05 ***\nX           -0.11179    0.09949  -1.124    0.264    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.756 on 97 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1779 \nF-statistic: 11.72 on 2 and 97 DF,  p-value: 2.771e-05\n\n\n\n8.6.5 Step 4: Reversed Path C (Y on X, controlling for M)\n\n#4. Reversed Path C (Y on X, controlling for M)\nfitc <- lm(X ~ Y + M, data=Meddata)\nsummary(fitc)\n\n\nCall:\nlm(formula = X ~ Y + M, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.438  -2.573  -0.030   3.010  11.779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 96.11234    9.27663  10.361  < 2e-16 ***\nY           -0.11493    0.10229  -1.124    0.264    \nM            0.69619    0.08356   8.332 5.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.823 on 97 degrees of freedom\nMultiple R-squared:  0.4418,    Adjusted R-squared:  0.4303 \nF-statistic: 38.39 on 2 and 97 DF,  p-value: 5.233e-13\n\n\n\n8.6.6 Viewing output\nSummary Table\nstargazer(fit, fita, fitb, fitc, type = \"text\", title = \"Baron and Kenny Method\")\n\n\n\n\n\n\n\n\n\n8.6.7 Interpreting Baron and Kenny approach\nA reminder of the logic of mediation:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\n8.6.8 Running the Sobel test\n\nThe Sobel test checks the singificance of indirect effects\n\n\n#Sobel Test\nlibrary(multilevel)\nsobel(Meddata$X, Meddata$M, Meddata$Y)\n\n$`Mod1: Y~X`\n              Estimate Std. Error  t value   Pr(>|t|)\n(Intercept) 19.8836805 14.2637142 1.394004 0.16646905\npred         0.1689931  0.0811601 2.082220 0.03992761\n\n$`Mod2: Y~X+M`\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 17.3217682 13.16215851  1.316028 1.912663e-01\npred        -0.1117904  0.09949262 -1.123605 2.639537e-01\nmed          0.4238113  0.09899469  4.281152 4.371472e-05\n\n$`Mod3: M~X`\n             Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 6.0449365 13.41692114 0.4505457 6.533122e-01\npred        0.6625203  0.07634187 8.6783345 8.871741e-14\n\n$Indirect.Effect\n[1] 0.2807836\n\n$SE\n[1] 0.07313234\n\n$z.value\n[1] 3.83939\n\n$N\n[1] 100\n\n\nHowever, the above code only gives us to information we need to test the significance of the indirect effect, not the significance itself. Thereore, we can use the following, to get the actual significance of the indirect effect:\n\nlibrary(bda)\n\nWarning: package 'bda' was built under R version 4.2.3\n\n\nLoading required package: boot\n\n\nbda v15 (Bin Wang, 2021)\n\nmediation.test(Meddata$M, Meddata$X, Meddata$Y)\n\n\n\n\n\nSobel\nAroian\nGoodman\n\n\n\nz.value\n3.8393902\n3.819053\n3.8600563\n\n\np.value\n0.0001233\n0.000134\n0.0001134"
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-mediation-package",
    "href": "mediation.html#mediation-analysis-the-mediation-package",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.7 Mediation analysis (the Mediation package)",
    "text": "8.7 Mediation analysis (the Mediation package)\n\n8.7.1 Preacher & Hayes (2004) mediation approach\n\nMediation package in R uses the Preacher & Hayes (2004) bootstrapping approach\nThey argue that few people test the significance of the indirect effect\n\n\n\n“Baron and Kenny simply state that perfect mediation has occurred if c’ becomes nonsignificant after controlling for M, so researchers have focused on that requirement.” (Preacher & Hayes, 2004, p. 719)\n\n\n\nSobel test has low power (requires larger sample sizes)\nSobel test assumes normality (often violated)\n\n8.7.2 What is bootstrapping?\n\n\n“Bootstrapping is a nonparametric approach to effect-size estimation and hypothesis testing that makes no assumptions about the shape of the distributions of the variables or the sampling distribution of the statistic” (Preacer & Hayes, 2004, p. 722)\n\n\n\nBootstrapping takes a large number of samples from our data and runs the analysis on each of these samples\nThe sampling is done randomly with replacement, and each sample in the bootstrap is the same size as our dataset\nUsing this method, we can create estimates with that fall within a narrower confidence interval (since we have now run the analysis on 100’s of samples)\nBootstrapping overcomes concerns about the distribution of our original dataset\n\n8.7.3 Mediation example\nIs the relationship between No of hours awake and wakefulness mediated by caffiene consumption?\n\nThis example is from Demos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14)\n\n\n\n\n\n\n8.7.4 Step 1: Run the models\n\n#Mediate package\nlibrary(mediation)\n\nfitM <- lm(M ~ X,     data=Meddata) #IV on M; Hours since waking predicting coffee consumption\nfitY <- lm(Y ~ X + M, data=Meddata) #IV and M on DV; Hours since dawn and coffee predicting wakefulness\n\n\n8.7.5 Step 2: Check assumptions\n\ngvlma(fitM) \n\n# We can see that the data is positively skewed. We might need to transform the data (we will discuss this another time).\n\ngvlma(fitY)\n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nCoefficients:\n(Intercept)            X  \n     6.0449       0.6625  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitM) \n\n                   Value p-value                   Decision\nGlobal Stat        8.833 0.06542    Assumptions acceptable.\nSkewness           6.314 0.01198 Assumptions NOT satisfied!\nKurtosis           1.219 0.26949    Assumptions acceptable.\nLink Function      1.076 0.29959    Assumptions acceptable.\nHeteroscedasticity 0.223 0.63674    Assumptions acceptable.\n\nCall:\nlm(formula = Y ~ X + M, data = Meddata)\n\nCoefficients:\n(Intercept)            X            M  \n    17.3218      -0.1118       0.4238  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitY) \n\n                     Value p-value                Decision\nGlobal Stat        3.41844  0.4904 Assumptions acceptable.\nSkewness           1.85648  0.1730 Assumptions acceptable.\nKurtosis           0.77788  0.3778 Assumptions acceptable.\nLink Function      0.71512  0.3977 Assumptions acceptable.\nHeteroscedasticity 0.06896  0.7929 Assumptions acceptable.\n\n\n\n8.7.6 Step 3.1: Run the mediation analysis on the models\nThe mediate function gives us:\n\nAverage Causal Mediation Effects (ACME)\nAverage Direct Effects (ADE)\ncombined indirect and direct effects (Total Effect)\nthe ratio of these estimates (Prop. Mediated).\n\nThe ACME here is the indirect effect of M (total effect - direct effect) and thus this value tells us if our mediation effect is significant.\n\nfitMed <- mediate(fitM, fitY, treat=\"X\", mediator=\"M\")\nsummary(fitMed)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.2808       0.1437         0.42  <2e-16 ***\nADE             -0.1133      -0.3116         0.09   0.258    \nTotal Effect     0.1674       0.0208         0.34   0.028 *  \nProp. Mediated   1.6428       0.5631         8.44   0.028 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 1000 \n\n\n\n8.7.7 Step 3.2: Plot the mediation analysis of the models\nThe plot below reiterates what was on the previous slide:\n\nThe confidence intervals of Total Effect and ACME are significant\nThe confidence interval of ADE is not significant\n\nTranslation:\n\nTotal effect is signficant: there is a relationship between X and Y (direct and indirect)\nADE is not significant: the relationship between X and Y is not direct\nACME is significant: the relationship between X and Y is mediated by M\n\n\nplot(fitMed)\n\n\n\n\n\n\n\n\n8.7.8 Step 4: Bootstrap the mediation model\nThe plot below changes our interpretation slightly:\n\nThe confidence interval ACME is significant\nThe confidence interval of Total Effect and ADE are not significant\n\nTranslation:\n\nTotal effect is not signficant: the relationship between X and Y is not significant when we combine direct and indirect effects\nADE is not significant: the relationship between X and Y is not direct\nACME is significant: the relationship between X and Y is mediated by M\n\n\nfitMedBoot <- mediate(fitM, fitY, boot=TRUE, sims=999, treat=\"X\", mediator=\"M\")\n\nRunning nonparametric bootstrap\n\nsummary(fitMedBoot)\nplot(fitMedBoot) ##\n\n\n\n\n\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.2808       0.1409         0.42  <2e-16 ***\nADE             -0.1118      -0.3080         0.12    0.31    \nTotal Effect     0.1690      -0.0123         0.34    0.07 .  \nProp. Mediated   1.6615      -3.7235        11.33    0.07 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 999"
  },
  {
    "objectID": "mediation.html#references",
    "href": "mediation.html#references",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.8 References",
    "text": "8.8 References\nDemos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14). https://ademos.people.uic.edu/ Accessed Jan 2020.\nPreacher, K. J., & Hayes, A. F. (2004). SPSS and SAS procedures for estimating indirect effects in simple mediation models. Behavior research methods, instruments, & computers, 36(4), 717-731."
  },
  {
    "objectID": "factorAnalysis.html#overview",
    "href": "factorAnalysis.html#overview",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.1 Overview",
    "text": "9.1 Overview\n\nWhat is factor analysis\nCFA versus PCA\nVariance in factor analysis\nConsidertations for factor analysis\nIdentifying / extracting factors\nRotation\nCronbach’s alpha"
  },
  {
    "objectID": "factorAnalysis.html#exploratory-factor-analysis",
    "href": "factorAnalysis.html#exploratory-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.2 Exploratory Factor analysis",
    "text": "9.2 Exploratory Factor analysis\n\nIdentify the relational structure between a set of variables in order to reduce them to a smaller set of factors\n\nThe process of dimension reduction (identify new variables) or data summarisation (summarise what is already there)\n\n\n\n\n9.2.1 Dimension reducton\n\n\nLatent Variables: Not directly observable. Rather they are inferred from other responses\n\nMany psychological constructs (e.g. anxiety) are latent variables that we cannot directly measure.\nRather, we can measure behaviours, cognitions and other variables that are related to the construct.\n\n\n\n\nWe might concptualise this as: “Responses to the questions are indicative of levels of underlying anxiety”\n\n\n\n\n\n\n\n\n\n\n9.2.2 Data summarisation\n\nIndex Variables or Components: A weighted summary of measured variables that contribute to the component variable\n“Principal components are variables of maximal variance constructed from linear combinations of the input features”\n\n\nWe might conceptualise this as: “We can reduce these measures/questions to a smaller set of higher order, independent, composite variables”"
  },
  {
    "objectID": "factorAnalysis.html#variance-in-exploratory-factor-analysis",
    "href": "factorAnalysis.html#variance-in-exploratory-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.3 Variance in exploratory factor analysis",
    "text": "9.3 Variance in exploratory factor analysis\nThere are two common methods of exploratory factor analysis: Common Factor analysis and Principal Component Analysis\n\nCFA assumes that there are two types of variance: common and unique\n\n\n\n\n\n\n\n\n\n\n9.3.1 Variance in PCA\n\nPCA only assumes common variance\n\n\n\n\n\n\n\n\n\n\n9.3.2 Variance in CFA\n\nDue to these different approaches, PCA is considered to be reflective of the current sample but not generalisable to the wider population\nWhereas, CFA is considered appropriate for hypothesis testing and making inferences to the population"
  },
  {
    "objectID": "factorAnalysis.html#what-is-factor-analysis",
    "href": "factorAnalysis.html#what-is-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.4 What is factor analysis?",
    "text": "9.4 What is factor analysis?\n\nIf we measure several variables (or questions), we can examine the correlation between sets of these variables\n\nSuch a correlation matrix is known as an R Matrix (r because correlation)\n\n\nIf there are clusters of correlations between a number of the variables (or questions), this indicates that they might be linked to the same underlying dimension (or latent variable)\nThe researcher should use informed judgement when assessing the appropriateness of variables for inclusion\n\n\n\n\n\n\n\n\n\nAn r matrix example"
  },
  {
    "objectID": "factorAnalysis.html#considerations-with-factor-analysis",
    "href": "factorAnalysis.html#considerations-with-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.5 Considerations with factor analysis",
    "text": "9.5 Considerations with factor analysis\n\nSample size:\n\nMust be more data points than variables being measured\nA common rule of thumb is at least 10 per variable\nThere are tests to assess sample size adequacy (e.g. Kaiser-Meyer test should be greater than 0.5)\n\n\nInter-correlation:\n\nThere must be sufficient correlation between the variables being measured\nA high number of correlations over 0.3\nCan be tested using Bartlett test of sphericity (sig. result means factor analysis can be used)\n\n\n\nOther things to check (see Field, 2018) \n\nThe quality of analysis depends upon the quality of the data (GI/GO).\nAvoid multicollinearity:\n\nseveral variables highly correlated, r > .80.\nDeterminent: should be greater than 0.00001\n\n\nAvoid singularity:\n\nsome variables perfectly correlated, r = 1.\n\n\nScreen the correlation matrix, eliminate any variables that obviously cause concern."
  },
  {
    "objectID": "factorAnalysis.html#representing-factor-analysis",
    "href": "factorAnalysis.html#representing-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.6 Representing factor analysis",
    "text": "9.6 Representing factor analysis\n\nWe can represent factors visually based on the strength of their inter-correlations - Here, the axis of the graph represents a factor or latent variable\n\n\n\n\n\n\n\n\n\n\nWe can also represent factor analysis using a regression equation - Here the beta values represent the extent to which the variable “loads onto” a particular factor\n\n\n\n\n\n\n\n\n\nExample: Statistics anxiety\n\nMany people get anxious about statistics\nWe can ask them about their experience in a number of ways (e.g. questions compiled by students in a stats class)\n\nTheir responses might indicate that stats anxiety has a number of dimensions\n\ni.e. it is a multi-dimensional construct, as opposed to a unitary construct"
  },
  {
    "objectID": "factorAnalysis.html#step-1-create-a-correlation-matrix",
    "href": "factorAnalysis.html#step-1-create-a-correlation-matrix",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.7 Step 1: Create a correlation matrix",
    "text": "9.7 Step 1: Create a correlation matrix\n\nraq.matrix <- cor(raq)\n\nraq.matrix\n\n             Q01         Q02        Q03         Q04         Q05         Q06\nQ01  1.000000000 -0.09872403 -0.3366489  0.43586018  0.40243992  0.21673399\nQ02 -0.098724032  1.00000000  0.3183902 -0.11185965 -0.11934658 -0.07420968\nQ03 -0.336648879  0.31839020  1.0000000 -0.38046016 -0.31030879 -0.22674048\nQ04  0.435860179 -0.11185965 -0.3804602  1.00000000  0.40067225  0.27820154\nQ05  0.402439917 -0.11934658 -0.3103088  0.40067225  1.00000000  0.25746014\nQ06  0.216733985 -0.07420968 -0.2267405  0.27820154  0.25746014  1.00000000\nQ07  0.305365139 -0.15917448 -0.3819533  0.40861502  0.33939179  0.51358048\nQ08  0.330737608 -0.04962257 -0.2586342  0.34942939  0.26862697  0.22283175\nQ09 -0.092339458  0.31464054  0.2998036 -0.12454637 -0.09570151 -0.11264384\nQ10  0.213681706 -0.08400316 -0.1933887  0.21581010  0.25820925  0.32223023\nQ11  0.356786290 -0.14382984 -0.3506397  0.36865655  0.29782882  0.32807072\nQ12  0.345381133 -0.19486946 -0.4099513  0.44164706  0.34674325  0.31250937\nQ13  0.354646283 -0.14274026 -0.3179193  0.34429168  0.30182159  0.46640487\nQ14  0.337879655 -0.16469991 -0.3707551  0.35080964  0.31533810  0.40224407\nQ15  0.245752635 -0.16499581 -0.3123968  0.33423089  0.26137190  0.35989309\nQ16  0.498618057 -0.16755228 -0.4186478  0.41586725  0.39491795  0.24433888\nQ17  0.370550512 -0.08699527 -0.3273715  0.38273945  0.31041722  0.28226121\nQ18  0.347118037 -0.16389415 -0.3752329  0.38200149  0.32209148  0.51332164\nQ19 -0.189011027  0.20329748  0.3415737 -0.18597751 -0.16532210 -0.16675017\nQ20  0.213897945 -0.20159437 -0.3248338  0.24291796  0.19966945  0.10092489\nQ21  0.329153138 -0.20461730 -0.4171878  0.41029317  0.33461494  0.27233273\nQ22 -0.104408664  0.23087487  0.2036569 -0.09838349 -0.13253593 -0.16513541\nQ23 -0.004480593  0.09967828  0.1502065 -0.03381815 -0.04165684 -0.06868743\n            Q07         Q08         Q09         Q10         Q11         Q12\nQ01  0.30536514  0.33073761 -0.09233946  0.21368171  0.35678629  0.34538113\nQ02 -0.15917448 -0.04962257  0.31464054 -0.08400316 -0.14382984 -0.19486946\nQ03 -0.38195325 -0.25863421  0.29980362 -0.19338871 -0.35063969 -0.40995127\nQ04  0.40861502  0.34942939 -0.12454637  0.21581010  0.36865655  0.44164706\nQ05  0.33939179  0.26862697 -0.09570151  0.25820925  0.29782882  0.34674325\nQ06  0.51358048  0.22283175 -0.11264384  0.32223023  0.32807072  0.31250937\nQ07  1.00000000  0.29749696 -0.12829828  0.28372299  0.34474770  0.42298591\nQ08  0.29749696  1.00000000  0.01573316  0.15860850  0.62929768  0.25198582\nQ09 -0.12829828  0.01573316  1.00000000 -0.13418658 -0.11552479 -0.16739436\nQ10  0.28372299  0.15860850 -0.13418658  1.00000000  0.27143657  0.24582591\nQ11  0.34474770  0.62929768 -0.11552479  0.27143657  1.00000000  0.33529466\nQ12  0.42298591  0.25198582 -0.16739436  0.24582591  0.33529466  1.00000000\nQ13  0.44211926  0.31424716 -0.16743882  0.30196707  0.42316548  0.48871303\nQ14  0.44070276  0.28058958 -0.12150197  0.25468730  0.32532025  0.43270398\nQ15  0.39136675  0.29968600 -0.18657099  0.29523438  0.36482687  0.33179910\nQ16  0.38854534  0.32149420 -0.18886556  0.29058576  0.36907763  0.40805908\nQ17  0.39074283  0.59014022 -0.03681556  0.21832214  0.58683495  0.33269383\nQ18  0.50086685  0.27974433 -0.14957782  0.29250304  0.37341373  0.49296482\nQ19 -0.26912031 -0.15947671  0.24931170 -0.12723487 -0.19965203 -0.26665953\nQ20  0.22095420  0.17515089 -0.15864747  0.08406520  0.25533736  0.29802585\nQ21  0.48300388  0.29571756 -0.13594310  0.19313633  0.34643407  0.44063832\nQ22 -0.16820488 -0.07917265  0.25684622 -0.13090831 -0.16198921 -0.16728557\nQ23 -0.07029016 -0.05023839  0.17077441 -0.06191796 -0.08637256 -0.04642506\n            Q13         Q14         Q15         Q16         Q17         Q18\nQ01  0.35464628  0.33787966  0.24575263  0.49861806  0.37055051  0.34711804\nQ02 -0.14274026 -0.16469991 -0.16499581 -0.16755228 -0.08699527 -0.16389415\nQ03 -0.31791928 -0.37075510 -0.31239678 -0.41864780 -0.32737145 -0.37523290\nQ04  0.34429168  0.35080964  0.33423089  0.41586725  0.38273945  0.38200149\nQ05  0.30182159  0.31533810  0.26137190  0.39491795  0.31041722  0.32209148\nQ06  0.46640487  0.40224407  0.35989309  0.24433888  0.28226121  0.51332164\nQ07  0.44211926  0.44070276  0.39136675  0.38854534  0.39074283  0.50086685\nQ08  0.31424716  0.28058958  0.29968600  0.32149420  0.59014022  0.27974433\nQ09 -0.16743882 -0.12150197 -0.18657099 -0.18886556 -0.03681556 -0.14957782\nQ10  0.30196707  0.25468730  0.29523438  0.29058576  0.21832214  0.29250304\nQ11  0.42316548  0.32532025  0.36482687  0.36907763  0.58683495  0.37341373\nQ12  0.48871303  0.43270398  0.33179910  0.40805908  0.33269383  0.49296482\nQ13  1.00000000  0.44978632  0.34219704  0.35837775  0.40837657  0.53293713\nQ14  0.44978632  1.00000000  0.38011484  0.41841820  0.35374183  0.49830615\nQ15  0.34219704  0.38011484  1.00000000  0.45427861  0.37310235  0.34287045\nQ16  0.35837775  0.41841820  0.45427861  1.00000000  0.40976309  0.42197911\nQ17  0.40837657  0.35374183  0.37310235  0.40976309  1.00000000  0.37560681\nQ18  0.53293713  0.49830615  0.34287045  0.42197911  0.37560681  1.00000000\nQ19 -0.22697105 -0.25405813 -0.20980230 -0.26704702 -0.16288096 -0.25663183\nQ20  0.20396327  0.22592173  0.20625622  0.26514025  0.20523013  0.23518040\nQ21  0.37443078  0.39938896  0.29971557  0.42054273  0.36349147  0.43010427\nQ22 -0.19535632 -0.16983754 -0.16790617 -0.15579385 -0.12629066 -0.15982631\nQ23 -0.05298304 -0.04847418 -0.06200665 -0.08152195 -0.09167243 -0.08041698\n           Q19         Q20         Q21         Q22          Q23\nQ01 -0.1890110  0.21389794  0.32915314 -0.10440866 -0.004480593\nQ02  0.2032975 -0.20159437 -0.20461730  0.23087487  0.099678285\nQ03  0.3415737 -0.32483385 -0.41718781  0.20365686  0.150206522\nQ04 -0.1859775  0.24291796  0.41029317 -0.09838349 -0.033818152\nQ05 -0.1653221  0.19966945  0.33461494 -0.13253593 -0.041656841\nQ06 -0.1667502  0.10092489  0.27233273 -0.16513541 -0.068687430\nQ07 -0.2691203  0.22095420  0.48300388 -0.16820488 -0.070290157\nQ08 -0.1594767  0.17515089  0.29571756 -0.07917265 -0.050238392\nQ09  0.2493117 -0.15864747 -0.13594310  0.25684622  0.170774410\nQ10 -0.1272349  0.08406520  0.19313633 -0.13090831 -0.061917956\nQ11 -0.1996520  0.25533736  0.34643407 -0.16198921 -0.086372565\nQ12 -0.2666595  0.29802585  0.44063832 -0.16728557 -0.046425059\nQ13 -0.2269710  0.20396327  0.37443078 -0.19535632 -0.052983042\nQ14 -0.2540581  0.22592173  0.39938896 -0.16983754 -0.048474181\nQ15 -0.2098023  0.20625622  0.29971557 -0.16790617 -0.062006650\nQ16 -0.2670470  0.26514025  0.42054273 -0.15579385 -0.081521950\nQ17 -0.1628810  0.20523013  0.36349147 -0.12629066 -0.091672426\nQ18 -0.2566318  0.23518040  0.43010427 -0.15982631 -0.080416984\nQ19  1.0000000 -0.24859386 -0.27489793  0.23392259  0.122434401\nQ20 -0.2485939  1.00000000  0.46770448 -0.09970186 -0.034665293\nQ21 -0.2748979  0.46770448  1.00000000 -0.12902148 -0.067664367\nQ22  0.2339226 -0.09970186 -0.12902148  1.00000000  0.230369402\nQ23  0.1224344 -0.03466529 -0.06766437  0.23036940  1.000000000"
  },
  {
    "objectID": "factorAnalysis.html#step-2-lets-check-for-inter-correlation",
    "href": "factorAnalysis.html#step-2-lets-check-for-inter-correlation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.8 Step 2: Let’s check for Inter-correlation \n",
    "text": "9.8 Step 2: Let’s check for Inter-correlation \n\n\nlibrary(corrplot)\ncorrplot(raq.matrix, method = \"number\")\n\n\n\n\n\n\n\n\nWe can use bartlett’s test from the psych package\n\n\nlibrary(psych)\n\ncortest.bartlett(raq.matrix, n=2571)\n\n$chisq\n[1] 19334.49\n\n$p.value\n[1] 0\n\n$df\n[1] 253"
  },
  {
    "objectID": "factorAnalysis.html#step-3-check-sampling-adequacy",
    "href": "factorAnalysis.html#step-3-check-sampling-adequacy",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.9 Step 3: Check sampling adequacy",
    "text": "9.9 Step 3: Check sampling adequacy\n\nOverall should be > 0.5\n\n\nKMO(raq)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = raq)\nOverall MSA =  0.93\nMSA for each item = \n Q01  Q02  Q03  Q04  Q05  Q06  Q07  Q08  Q09  Q10  Q11  Q12  Q13  Q14  Q15  Q16 \n0.93 0.87 0.95 0.96 0.96 0.89 0.94 0.87 0.83 0.95 0.91 0.95 0.95 0.97 0.94 0.93 \n Q17  Q18  Q19  Q20  Q21  Q22  Q23 \n0.93 0.95 0.94 0.89 0.93 0.88 0.77"
  },
  {
    "objectID": "factorAnalysis.html#step-4-identify-number-of-factors",
    "href": "factorAnalysis.html#step-4-identify-number-of-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.10 Step 4: Identify number of factors",
    "text": "9.10 Step 4: Identify number of factors\n\nBased on Eigenvalues:\n\nKaiser (1960) – retain factors with eigen values > 1.\nJoliffe (1972) – retain factors with eigen values > .70.\n\n\nUse a scree plot: Cattell (1966): use ‘point of inflexion’.\n\n\n9.10.1 Which rule?\n\nUse Kaiser’s extraction when\n\nLess than 30 variables, communalities after extraction > 0.7\nSample size > 250 and mean communality > 0.6\n\n\nScree plot is good if sample size is > 200\n\n9.10.2 Scree plot\n\nscree(raq)\n\n\n\n\n\n\n\n\nWe are looking for the point of inflection\nWhere there is a drop-off\n\n\nOne approach: See how many factors we can draw a line through\n\n\n9.10.3 Parallel analysis\n\nHow many dimensions of stats anxiety are captured in the questionnaire?\n\n\nWe can run a parallel analysis to get an indication of the number of factors contained within the data\nParallel Analysis:\n\nSimulates data within the same range of values as our data set\nSuggests that we retain, at maximum, the factors with eigenvalues larger than those extracted from simulated data.\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(psych)\n\n parallel_analysis <- fa.parallel(raq)\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  6  and the number of components =  4 \n\n\n\nparallel_analysis\n\nCall: fa.parallel(x = raq)\nParallel analysis suggests that the number of factors =  6  and the number of components =  4 \n\n Eigen Values of \n  Original factors Resampled data Simulated data Original components\n1             6.64           0.25           0.20                7.29\n2             0.91           0.16           0.15                1.74\n3             0.63           0.13           0.13                1.32\n4             0.48           0.11           0.12                1.23\n5             0.29           0.10           0.10                0.99\n6             0.13           0.08           0.08                0.90\n  Resampled components Simulated components\n1                 1.18                 1.17\n2                 1.15                 1.15\n3                 1.12                 1.12\n4                 1.10                 1.11\n5                 1.09                 1.09\n6                 1.07                 1.08"
  },
  {
    "objectID": "factorAnalysis.html#step-5-perform-factor-analysis-with-initial-recommended-factors",
    "href": "factorAnalysis.html#step-5-perform-factor-analysis-with-initial-recommended-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.11 Step 5: Perform factor analysis (with initial recommended # factors)",
    "text": "9.11 Step 5: Perform factor analysis (with initial recommended # factors)\n\npaf <- fa(raq,\nnfactors = 6,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"none\")\n\n\npaf\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 6, rotate = \"none\", max.iter = 100, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2   PA3   PA4   PA5   PA6   h2   u2 com\nQ01  0.57  0.13 -0.12  0.23 -0.28 -0.19 0.52 0.48 2.3\nQ02 -0.28  0.37  0.17  0.12 -0.03  0.01 0.26 0.74 2.6\nQ03 -0.60  0.25  0.20 -0.02 -0.01  0.03 0.46 0.54 1.6\nQ04  0.61  0.08 -0.06  0.18 -0.09 -0.03 0.42 0.58 1.3\nQ05  0.52  0.04 -0.02  0.15 -0.17 -0.08 0.33 0.67 1.5\nQ06  0.55  0.02  0.49 -0.17  0.07 -0.01 0.57 0.43 2.2\nQ07  0.66 -0.03  0.22  0.03  0.11  0.06 0.50 0.50 1.3\nQ08  0.55  0.49 -0.27 -0.21  0.10 -0.02 0.66 0.34 2.9\nQ09 -0.27  0.46  0.12  0.21  0.10  0.03 0.35 0.65 2.4\nQ10  0.40 -0.01  0.17 -0.09 -0.15  0.02 0.22 0.78 1.8\nQ11  0.64  0.31 -0.20 -0.27  0.08 -0.04 0.63 0.37 2.1\nQ12  0.64 -0.10  0.06  0.15  0.05 -0.07 0.45 0.55 1.2\nQ13  0.65  0.02  0.22 -0.06  0.06 -0.13 0.50 0.50 1.4\nQ14  0.63 -0.04  0.16  0.06  0.01  0.01 0.42 0.58 1.2\nQ15  0.58 -0.01  0.07 -0.15 -0.19  0.44 0.59 0.41 2.3\nQ16  0.66 -0.02 -0.11  0.14 -0.28  0.09 0.56 0.44 1.6\nQ17  0.63  0.36 -0.15 -0.15  0.04  0.01 0.57 0.43 1.9\nQ18  0.68 -0.04  0.28  0.04  0.09 -0.10 0.57 0.43 1.4\nQ19 -0.40  0.27  0.11  0.06 -0.05  0.02 0.25 0.75 2.0\nQ20  0.41 -0.17 -0.25  0.19  0.24  0.11 0.37 0.63 3.5\nQ21  0.64 -0.10 -0.11  0.27  0.28  0.10 0.60 0.40 2.0\nQ22 -0.28  0.29  0.05  0.28  0.05  0.11 0.26 0.74 3.4\nQ23 -0.13  0.18  0.08  0.23  0.01  0.08 0.12 0.88 3.1\n\n                       PA1  PA2  PA3  PA4  PA5  PA6\nSS loadings           6.79 1.14 0.83 0.67 0.45 0.32\nProportion Var        0.30 0.05 0.04 0.03 0.02 0.01\nCumulative Var        0.30 0.34 0.38 0.41 0.43 0.44\nProportion Explained  0.67 0.11 0.08 0.07 0.04 0.03\nCumulative Proportion 0.67 0.78 0.86 0.92 0.97 1.00\n\nMean item complexity =  2\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 130  and the objective function was  0.23 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  2571 with the empirical chi square  364.66  with prob <  3.9e-24 \nThe total n.obs was  2571  with Likelihood Chi Square =  578.65  with prob <  7.6e-58 \n\nTucker Lewis Index of factoring reliability =  0.954\nRMSEA index =  0.037  and the 90 % confidence intervals are  0.034 0.04\nBIC =  -442.12\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   PA1  PA2  PA3  PA4   PA5\nCorrelation of (regression) scores with factors   0.97 0.83 0.80 0.75  0.70\nMultiple R square of scores with factors          0.93 0.68 0.64 0.56  0.48\nMinimum correlation of possible factor scores     0.87 0.37 0.27 0.12 -0.03\n                                                    PA6\nCorrelation of (regression) scores with factors    0.65\nMultiple R square of scores with factors           0.42\nMinimum correlation of possible factor scores     -0.17\n\n\n\n9.11.1 Check the factor matrix\n\nWe are looking high levels of variance explained with SS loadings > 1\n\n\nprint(paf$loadings, cutoff=0, digits=3)\n\n\nLoadings:\n    PA1    PA2    PA3    PA4    PA5    PA6   \nQ01  0.567  0.129 -0.120  0.229 -0.275 -0.188\nQ02 -0.280  0.369  0.172  0.115 -0.029  0.009\nQ03 -0.603  0.245  0.199 -0.022 -0.006  0.030\nQ04  0.606  0.082 -0.056  0.184 -0.090 -0.033\nQ05  0.523  0.043 -0.020  0.154 -0.167 -0.083\nQ06  0.548  0.024  0.488 -0.166  0.073 -0.006\nQ07  0.662 -0.026  0.223  0.030  0.107  0.057\nQ08  0.545  0.488 -0.272 -0.214  0.096 -0.020\nQ09 -0.266  0.462  0.124  0.210  0.097  0.032\nQ10  0.405 -0.005  0.172 -0.090 -0.148  0.024\nQ11  0.644  0.312 -0.199 -0.270  0.085 -0.037\nQ12  0.641 -0.099  0.063  0.154  0.047 -0.067\nQ13  0.650  0.024  0.223 -0.058  0.061 -0.134\nQ14  0.626 -0.036  0.161  0.056  0.011  0.013\nQ15  0.580 -0.007  0.072 -0.152 -0.188  0.436\nQ16  0.661 -0.016 -0.109  0.138 -0.283  0.094\nQ17  0.629  0.355 -0.155 -0.150  0.038  0.006\nQ18  0.683 -0.039  0.277  0.041  0.092 -0.099\nQ19 -0.395  0.267  0.110  0.060 -0.052  0.022\nQ20  0.412 -0.171 -0.250  0.190  0.241  0.114\nQ21  0.644 -0.099 -0.110  0.270  0.283  0.099\nQ22 -0.279  0.291  0.050  0.284  0.047  0.114\nQ23 -0.130  0.182  0.081  0.235  0.011  0.077\n\n                 PA1   PA2   PA3   PA4   PA5   PA6\nSS loadings    6.786 1.140 0.827 0.667 0.452 0.324\nProportion Var 0.295 0.050 0.036 0.029 0.020 0.014\nCumulative Var 0.295 0.345 0.381 0.410 0.429 0.443\n\n\n\n9.11.2 Check the structure matrix\n\nprint(paf$Structure, cutoff=0, digits=3)\n\n\nLoadings:\n    PA1    PA2    PA3    PA4    PA5    PA6   \nQ01  0.567  0.129 -0.120  0.229 -0.275 -0.188\nQ02 -0.280  0.369  0.172  0.115 -0.029  0.009\nQ03 -0.603  0.245  0.199 -0.022 -0.006  0.030\nQ04  0.606  0.082 -0.056  0.184 -0.090 -0.033\nQ05  0.523  0.043 -0.020  0.154 -0.167 -0.083\nQ06  0.548  0.024  0.488 -0.166  0.073 -0.006\nQ07  0.662 -0.026  0.223  0.030  0.107  0.057\nQ08  0.545  0.488 -0.272 -0.214  0.096 -0.020\nQ09 -0.266  0.462  0.124  0.210  0.097  0.032\nQ10  0.405 -0.005  0.172 -0.090 -0.148  0.024\nQ11  0.644  0.312 -0.199 -0.270  0.085 -0.037\nQ12  0.641 -0.099  0.063  0.154  0.047 -0.067\nQ13  0.650  0.024  0.223 -0.058  0.061 -0.134\nQ14  0.626 -0.036  0.161  0.056  0.011  0.013\nQ15  0.580 -0.007  0.072 -0.152 -0.188  0.436\nQ16  0.661 -0.016 -0.109  0.138 -0.283  0.094\nQ17  0.629  0.355 -0.155 -0.150  0.038  0.006\nQ18  0.683 -0.039  0.277  0.041  0.092 -0.099\nQ19 -0.395  0.267  0.110  0.060 -0.052  0.022\nQ20  0.412 -0.171 -0.250  0.190  0.241  0.114\nQ21  0.644 -0.099 -0.110  0.270  0.283  0.099\nQ22 -0.279  0.291  0.050  0.284  0.047  0.114\nQ23 -0.130  0.182  0.081  0.235  0.011  0.077\n\n                 PA1   PA2   PA3   PA4   PA5   PA6\nSS loadings    6.786 1.140 0.827 0.667 0.452 0.324\nProportion Var 0.295 0.050 0.036 0.029 0.020 0.014\nCumulative Var 0.295 0.345 0.381 0.410 0.429 0.443\n\n\n\n9.11.3 Check eigenvalues\n\npaf$e.values[1:6]\n\n[1] 7.2900471 1.7388287 1.3167515 1.2271982 0.9878779 0.8953304\n\n\n\n9.11.4 Check communalities\n\nCommunality for each variable: the percentage of variance that can be explained by the retained factors.\nRetained factors should explain more of the variance in each variable.\n\n\npaf$communality\n\n      Q01       Q02       Q03       Q04       Q05       Q06       Q07       Q08 \n0.5170176 0.2585136 0.4643374 0.4196524 0.3341637 0.5720655 0.5042725 0.6649413 \n      Q09       Q10       Q11       Q12       Q13       Q14       Q15       Q16 \n0.3542281 0.2240464 0.6328967 0.4544862 0.4973541 0.4223263 0.5902303 0.5571656 \n      Q17       Q18       Q19       Q20       Q21       Q22       Q23 \n0.5700891 0.5655104 0.2467731 0.3686202 0.5991875 0.2606533 0.1178839"
  },
  {
    "objectID": "factorAnalysis.html#step-6-perform-factor-analysis-with-reduced-number-of-factors",
    "href": "factorAnalysis.html#step-6-perform-factor-analysis-with-reduced-number-of-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.12 Step 6: Perform factor analysis (with reduced number of factors)",
    "text": "9.12 Step 6: Perform factor analysis (with reduced number of factors)\n\npaf1 <- fa(raq,\nnfactors = 2,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"none\")\n\npaf1\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 2, rotate = \"none\", max.iter = 100, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2    h2   u2 com\nQ01  0.56  0.12 0.324 0.68 1.1\nQ02 -0.28  0.39 0.228 0.77 1.8\nQ03 -0.61  0.25 0.430 0.57 1.3\nQ04  0.61  0.09 0.377 0.62 1.0\nQ05  0.52  0.05 0.276 0.72 1.0\nQ06  0.53  0.04 0.282 0.72 1.0\nQ07  0.66 -0.01 0.437 0.56 1.0\nQ08  0.53  0.40 0.445 0.56 1.9\nQ09 -0.27  0.46 0.287 0.71 1.6\nQ10  0.40  0.00 0.163 0.84 1.0\nQ11  0.63  0.27 0.472 0.53 1.3\nQ12  0.64 -0.08 0.421 0.58 1.0\nQ13  0.65  0.04 0.421 0.58 1.0\nQ14  0.63 -0.02 0.396 0.60 1.0\nQ15  0.56  0.00 0.315 0.68 1.0\nQ16  0.65 -0.01 0.428 0.57 1.0\nQ17  0.63  0.34 0.511 0.49 1.5\nQ18  0.68 -0.02 0.461 0.54 1.0\nQ19 -0.40  0.28 0.238 0.76 1.8\nQ20  0.40 -0.15 0.187 0.81 1.3\nQ21  0.63 -0.07 0.403 0.60 1.0\nQ22 -0.28  0.29 0.161 0.84 2.0\nQ23 -0.13  0.19 0.053 0.95 1.8\n\n                       PA1  PA2\nSS loadings           6.67 1.04\nProportion Var        0.29 0.05\nCumulative Var        0.29 0.34\nProportion Explained  0.86 0.14\nCumulative Proportion 0.86 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 208  and the objective function was  1.23 \n\nThe root mean square of the residuals (RMSR) is  0.05 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2571 with the empirical chi square  3114.53  with prob <  0 \nThe total n.obs was  2571  with Likelihood Chi Square =  3155.34  with prob <  0 \n\nTucker Lewis Index of factoring reliability =  0.812\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.072 0.077\nBIC =  1522.12\nFit based upon off diagonal values = 0.97\nMeasures of factor score adequacy             \n                                                   PA1  PA2\nCorrelation of (regression) scores with factors   0.96 0.78\nMultiple R square of scores with factors          0.92 0.61\nMinimum correlation of possible factor scores     0.83 0.23\n\n\n\nplot(paf1)"
  },
  {
    "objectID": "factorAnalysis.html#factor-analysis-rotation",
    "href": "factorAnalysis.html#factor-analysis-rotation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.13 Factor analysis rotation",
    "text": "9.13 Factor analysis rotation\nWhat is rotation?\n\nIt is possible that variables load “highly” onto one factor and “medium” onto another\nBy rotating the factor axes, the variables are aligned with the factors that they load onto most\nThis helps us discriminate between factors\n\nThere are different methods of rotation\n\n\nOrthogonal rotation: Assumes that factors are unrelated and keeps them that way\n\nOblique rotation: Assumes that factors might be related and allows them to be correlated after rotation\n\n\nAre factors related? -Theoretical: Do we have logical reason for thinking they could be connected? -Based on data: Does the factor plot suggest independence or relatedness?"
  },
  {
    "objectID": "factorAnalysis.html#step-7-rotation",
    "href": "factorAnalysis.html#step-7-rotation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.14 Step 7: Rotation",
    "text": "9.14 Step 7: Rotation\n\nPerform factor analysis (with rotation)\n\n\npaf2 <- fa(raq,\nnfactors = 2,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"oblimin\")\n\npaf2\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 2, rotate = \"oblimin\", max.iter = 100, \n    fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2    h2   u2 com\nQ01  0.56  0.12 0.324 0.68 1.1\nQ02 -0.28  0.39 0.228 0.77 1.8\nQ03 -0.61  0.25 0.430 0.57 1.3\nQ04  0.61  0.09 0.377 0.62 1.0\nQ05  0.52  0.05 0.276 0.72 1.0\nQ06  0.53  0.04 0.282 0.72 1.0\nQ07  0.66 -0.01 0.437 0.56 1.0\nQ08  0.53  0.40 0.445 0.56 1.9\nQ09 -0.27  0.46 0.287 0.71 1.6\nQ10  0.40  0.00 0.163 0.84 1.0\nQ11  0.63  0.27 0.472 0.53 1.3\nQ12  0.64 -0.08 0.421 0.58 1.0\nQ13  0.65  0.04 0.421 0.58 1.0\nQ14  0.63 -0.02 0.396 0.60 1.0\nQ15  0.56  0.00 0.315 0.68 1.0\nQ16  0.65 -0.01 0.428 0.57 1.0\nQ17  0.63  0.34 0.511 0.49 1.5\nQ18  0.68 -0.02 0.461 0.54 1.0\nQ19 -0.40  0.28 0.238 0.76 1.8\nQ20  0.40 -0.15 0.187 0.81 1.3\nQ21  0.63 -0.07 0.403 0.60 1.0\nQ22 -0.28  0.29 0.161 0.84 2.0\nQ23 -0.13  0.19 0.053 0.95 1.8\n\n                       PA1  PA2\nSS loadings           6.67 1.04\nProportion Var        0.29 0.05\nCumulative Var        0.29 0.34\nProportion Explained  0.86 0.14\nCumulative Proportion 0.86 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 208  and the objective function was  1.23 \n\nThe root mean square of the residuals (RMSR) is  0.05 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2571 with the empirical chi square  3114.53  with prob <  0 \nThe total n.obs was  2571  with Likelihood Chi Square =  3155.34  with prob <  0 \n\nTucker Lewis Index of factoring reliability =  0.812\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.072 0.077\nBIC =  1522.12\nFit based upon off diagonal values = 0.97\nMeasures of factor score adequacy             \n                                                   PA1  PA2\nCorrelation of (regression) scores with factors   0.96 0.78\nMultiple R square of scores with factors          0.92 0.61\nMinimum correlation of possible factor scores     0.83 0.23\n\n\n\nplot(paf1)\n\n\n\n\n\n\n\n\nplot(paf2)"
  },
  {
    "objectID": "factorAnalysis.html#reliability-internal-consistency",
    "href": "factorAnalysis.html#reliability-internal-consistency",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.15 Reliability / internal consistency",
    "text": "9.15 Reliability / internal consistency\n\n9.15.1 Cronbach’s Alpha\n\nAn expansion of the split-half reliability concept\nAlpha takes all possible combination of items and assesses their relationship to each other\nHigh values above 0.7 suggest internal consistency among items\n\n9.15.2 Chronbach’s Alpha in R\n\nWe can use the alpha() function in the psych package\n\n\nlibrary(psych)\n\nalpha(raq)\n\nSome items ( Q02 Q03 Q09 Q19 Q22 Q23 ) were negatively correlated with the total scale and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\nReliability analysis   \nCall: alpha(x = raq)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.75      0.77    0.83      0.13 3.4 0.0065  3.3 0.39     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.74  0.75  0.77\nDuhachek  0.74  0.75  0.77\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nQ01      0.73      0.76    0.82      0.12 3.1   0.0071 0.071  0.23\nQ02      0.77      0.79    0.84      0.15 3.8   0.0061 0.071  0.25\nQ03      0.79      0.81    0.85      0.16 4.2   0.0055 0.059  0.25\nQ04      0.73      0.75    0.82      0.12 3.0   0.0072 0.070  0.22\nQ05      0.74      0.76    0.82      0.12 3.1   0.0071 0.072  0.22\nQ06      0.73      0.76    0.82      0.12 3.1   0.0072 0.072  0.23\nQ07      0.73      0.75    0.82      0.12 3.0   0.0074 0.069  0.22\nQ08      0.73      0.76    0.82      0.12 3.1   0.0071 0.072  0.23\nQ09      0.78      0.79    0.84      0.15 3.8   0.0058 0.071  0.25\nQ10      0.74      0.76    0.83      0.13 3.3   0.0068 0.074  0.23\nQ11      0.73      0.75    0.81      0.12 3.0   0.0072 0.069  0.22\nQ12      0.73      0.75    0.82      0.12 3.1   0.0072 0.069  0.22\nQ13      0.73      0.75    0.82      0.12 3.0   0.0073 0.069  0.22\nQ14      0.73      0.75    0.82      0.12 3.1   0.0072 0.070  0.22\nQ15      0.73      0.76    0.82      0.12 3.1   0.0071 0.071  0.22\nQ16      0.73      0.75    0.82      0.12 3.0   0.0072 0.069  0.22\nQ17      0.73      0.75    0.81      0.12 3.0   0.0072 0.070  0.22\nQ18      0.72      0.75    0.81      0.12 3.0   0.0074 0.068  0.22\nQ19      0.78      0.80    0.85      0.15 4.0   0.0057 0.067  0.26\nQ20      0.75      0.77    0.83      0.13 3.3   0.0067 0.073  0.25\nQ21      0.73      0.75    0.82      0.12 3.1   0.0072 0.069  0.22\nQ22      0.77      0.79    0.84      0.15 3.8   0.0059 0.071  0.26\nQ23      0.77      0.79    0.84      0.14 3.7   0.0061 0.074  0.26\n\n Item statistics \n       n   raw.r  std.r  r.cor r.drop mean   sd\nQ01 2571  0.5598  0.581  0.564  0.492  3.6 0.83\nQ02 2571 -0.0116 -0.018 -0.114 -0.105  4.4 0.85\nQ03 2571 -0.3356 -0.361 -0.465 -0.435  3.4 1.08\nQ04 2571  0.6064  0.618  0.606  0.533  3.2 0.95\nQ05 2571  0.5365  0.546  0.516  0.454  3.3 0.96\nQ06 2571  0.5709  0.560  0.547  0.478  3.8 1.12\nQ07 2571  0.6409  0.636  0.635  0.560  3.1 1.10\nQ08 2571  0.5646  0.582  0.578  0.493  3.8 0.87\nQ09 2571  0.0587  0.020 -0.068 -0.081  3.2 1.26\nQ10 2571  0.4300  0.442  0.391  0.346  3.7 0.88\nQ11 2571  0.6078  0.628  0.633  0.540  3.7 0.88\nQ12 2571  0.5909  0.602  0.593  0.519  2.8 0.92\nQ13 2571  0.6288  0.637  0.634  0.559  3.6 0.95\nQ14 2571  0.6056  0.609  0.596  0.528  3.1 1.00\nQ15 2571  0.5433  0.550  0.526  0.457  3.2 1.01\nQ16 2571  0.5965  0.615  0.612  0.525  3.1 0.92\nQ17 2571  0.6329  0.650  0.653  0.568  3.5 0.88\nQ18 2571  0.6534  0.653  0.656  0.578  3.4 1.05\nQ19 2571 -0.1316 -0.157 -0.264 -0.248  3.7 1.10\nQ20 2571  0.3705  0.375  0.326  0.265  2.4 1.04\nQ21 2571  0.5922  0.598  0.591  0.514  2.8 0.98\nQ22 2571 -0.0063 -0.027 -0.127 -0.121  3.1 1.04\nQ23 2571  0.1030  0.084 -0.014 -0.013  2.6 1.04\n\nNon missing response frequency for each item\n       1    2    3    4    5 miss\nQ01 0.02 0.07 0.29 0.52 0.11    0\nQ02 0.01 0.04 0.08 0.31 0.56    0\nQ03 0.03 0.17 0.34 0.26 0.19    0\nQ04 0.05 0.17 0.36 0.37 0.05    0\nQ05 0.04 0.18 0.29 0.43 0.06    0\nQ06 0.06 0.10 0.13 0.44 0.27    0\nQ07 0.09 0.24 0.26 0.34 0.07    0\nQ08 0.03 0.06 0.19 0.58 0.15    0\nQ09 0.08 0.28 0.23 0.20 0.20    0\nQ10 0.02 0.10 0.18 0.57 0.14    0\nQ11 0.02 0.06 0.22 0.53 0.16    0\nQ12 0.09 0.23 0.46 0.20 0.02    0\nQ13 0.03 0.12 0.25 0.48 0.12    0\nQ14 0.07 0.18 0.38 0.31 0.06    0\nQ15 0.06 0.18 0.30 0.39 0.07    0\nQ16 0.06 0.16 0.42 0.33 0.04    0\nQ17 0.03 0.10 0.27 0.52 0.08    0\nQ18 0.06 0.12 0.31 0.37 0.14    0\nQ19 0.02 0.15 0.22 0.33 0.29    0\nQ20 0.22 0.37 0.25 0.15 0.02    0\nQ21 0.09 0.29 0.34 0.26 0.02    0\nQ22 0.05 0.26 0.34 0.26 0.10    0\nQ23 0.12 0.42 0.27 0.12 0.06    0\n\n\n\nHere we get a warning that some of the items are negatively correlated and we should probably reverse them.\nThe decision to do so should be based on the logic of the questions themselves - check first\nHowever, since cronbach’s alpha is designed to check internal consistency related to a single construct, we would expect that negative correlations would only result from:\n\nItems that are designed to be reverse-scored\nQuestions that are related to another factor or construct\n\n\nLet’s check the questionnaire\n\n(Q02, Q03, Q09, Q19, Q22, Q23):\n\n\n\n\n\n\n\n\n\n\n\n\nIt is possible to run the analysis with automatic reversal of negatively-correlated items\n\n\nalpha(raq, check.keys=TRUE)\n\n\nReliability analysis   \nCall: alpha(x = raq, check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.89      0.89    0.91      0.27 8.3 0.0031  3.1 0.54     0.27\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88  0.89   0.9\nDuhachek  0.88  0.89   0.9\n\n Reliability if an item is dropped:\n     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nQ01       0.88      0.89    0.90      0.26 7.9   0.0032 0.016  0.27\nQ02-      0.89      0.89    0.91      0.28 8.4   0.0031 0.016  0.28\nQ03-      0.88      0.89    0.90      0.26 7.8   0.0033 0.017  0.26\nQ04       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ05       0.89      0.89    0.90      0.27 8.0   0.0032 0.017  0.27\nQ06       0.88      0.89    0.90      0.27 8.0   0.0032 0.016  0.27\nQ07       0.88      0.89    0.90      0.26 7.7   0.0034 0.016  0.26\nQ08       0.89      0.89    0.90      0.27 8.0   0.0032 0.016  0.27\nQ09-      0.89      0.89    0.91      0.28 8.4   0.0030 0.016  0.28\nQ10       0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.28\nQ11       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ12       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ13       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ14       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ15       0.88      0.89    0.90      0.26 7.9   0.0033 0.017  0.27\nQ16       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ17       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ18       0.88      0.88    0.90      0.26 7.7   0.0034 0.016  0.26\nQ19-      0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.29\nQ20       0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.28\nQ21       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ22-      0.89      0.89    0.91      0.28 8.4   0.0031 0.016  0.29\nQ23-      0.89      0.90    0.91      0.28 8.7   0.0030 0.014  0.29\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nQ01  2571  0.55  0.57  0.54   0.50  3.6 0.83\nQ02- 2571  0.36  0.36  0.31   0.30  1.6 0.85\nQ03- 2571  0.65  0.64  0.62   0.59  2.6 1.08\nQ04  2571  0.61  0.61  0.59   0.55  3.2 0.95\nQ05  2571  0.54  0.55  0.52   0.48  3.3 0.96\nQ06  2571  0.56  0.55  0.53   0.49  3.8 1.12\nQ07  2571  0.67  0.67  0.65   0.62  3.1 1.10\nQ08  2571  0.51  0.53  0.51   0.46  3.8 0.87\nQ09- 2571  0.37  0.35  0.30   0.28  2.8 1.26\nQ10  2571  0.44  0.45  0.40   0.38  3.7 0.88\nQ11  2571  0.63  0.64  0.63   0.58  3.7 0.88\nQ12  2571  0.65  0.65  0.64   0.60  2.8 0.92\nQ13  2571  0.65  0.65  0.64   0.60  3.6 0.95\nQ14  2571  0.64  0.64  0.62   0.59  3.1 1.00\nQ15  2571  0.59  0.59  0.56   0.53  3.2 1.01\nQ16  2571  0.66  0.67  0.65   0.61  3.1 0.92\nQ17  2571  0.61  0.62  0.61   0.56  3.5 0.88\nQ18  2571  0.68  0.68  0.67   0.63  3.4 1.05\nQ19- 2571  0.47  0.46  0.42   0.40  2.3 1.10\nQ20  2571  0.45  0.45  0.41   0.38  2.4 1.04\nQ21  2571  0.64  0.64  0.63   0.59  2.8 0.98\nQ22- 2571  0.37  0.36  0.31   0.30  2.9 1.04\nQ23- 2571  0.23  0.22  0.15   0.15  3.4 1.04\n\nNon missing response frequency for each item\n       1    2    3    4    5 miss\nQ01 0.02 0.07 0.29 0.52 0.11    0\nQ02 0.01 0.04 0.08 0.31 0.56    0\nQ03 0.03 0.17 0.34 0.26 0.19    0\nQ04 0.05 0.17 0.36 0.37 0.05    0\nQ05 0.04 0.18 0.29 0.43 0.06    0\nQ06 0.06 0.10 0.13 0.44 0.27    0\nQ07 0.09 0.24 0.26 0.34 0.07    0\nQ08 0.03 0.06 0.19 0.58 0.15    0\nQ09 0.08 0.28 0.23 0.20 0.20    0\nQ10 0.02 0.10 0.18 0.57 0.14    0\nQ11 0.02 0.06 0.22 0.53 0.16    0\nQ12 0.09 0.23 0.46 0.20 0.02    0\nQ13 0.03 0.12 0.25 0.48 0.12    0\nQ14 0.07 0.18 0.38 0.31 0.06    0\nQ15 0.06 0.18 0.30 0.39 0.07    0\nQ16 0.06 0.16 0.42 0.33 0.04    0\nQ17 0.03 0.10 0.27 0.52 0.08    0\nQ18 0.06 0.12 0.31 0.37 0.14    0\nQ19 0.02 0.15 0.22 0.33 0.29    0\nQ20 0.22 0.37 0.25 0.15 0.02    0\nQ21 0.09 0.29 0.34 0.26 0.02    0\nQ22 0.05 0.26 0.34 0.26 0.10    0\nQ23 0.12 0.42 0.27 0.12 0.06    0"
  },
  {
    "objectID": "introductiontoR.html",
    "href": "introductiontoR.html",
    "title": "1  Introduction to R and R Studio",
    "section": "",
    "text": "Revision Quiz"
  },
  {
    "objectID": "workingWithData.html#identify-different-data-structuresa-data-structure-that-aggregates-data-such-as-a-vector-list-matrix-or-data-frame-and-variable-types",
    "href": "workingWithData.html#identify-different-data-structuresa-data-structure-that-aggregates-data-such-as-a-vector-list-matrix-or-data-frame-and-variable-types",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.6 Identify different data structuresA data structure that aggregates data, such as a vector, list, matrix, or data frame and variable types",
    "text": "2.6 Identify different data structuresA data structure that aggregates data, such as a vector, list, matrix, or data frame and variable types\n\n2.6.1 Data structures (sometimes referred to as “data containersA data structure that aggregates data, such as a vector, list, matrix, or data frame”)\nThere are many different types of data structures that R can work with. The most common type of data for most people tends to be a data frame. A data frame is what you might consider a “normal” 2-dimensional dataset, with rows of data and columns of variables:\n\n\nA dataframe example\n\nR can also use other data structures.\nA vector is a one-dimensional set of values:\n\n# a vector example\n\nscores &lt;- c(1,4,6,8,3,4,6,7)\n\nA matrix is a multi-dimensional set of values. The below example is a 3-dimensional matrix, there are 2 groups of 2 rows and 3 columns:\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n\n\nWe will primarily work with dataframes (and sometimes vectors), as this is how the data in psychology research is usually structured.\n\n\n2.6.2 Types of numerical data\nWith numerical data, there are 4 key data types:\n\n\nfactor (data type) or nominal (a category, group or factor)\n\nordinalDiscrete variables that have an inherent order, such as level of education or dislike/like. (a ranking)\n\ninterval (data type) (scale data that can include negative values)\n\nratio (data type) (scale data that cannot include negative values)\n\n R can use all of these variable types:\n\n\nNominal variables are called factors\n\n\nOrdinal variables are called ordered factors\n\n\nInterval and ratio variables are called numeric data and can sometimes be called integers (if they are only whole numbers) or doubles (if they all have decimal points)\n\nR can also use other data types that are not numerical such as text (characterA data type representing strings of text.) data.\n\n2.6.3 Convert variables from one data type to another\nWhen we first import data into R, it might not recognise the data types correctly. For example, in the below data, we can see the intervention variable :\n\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n4\n2\n6.245260\n\n\n7\n2\n8.745944\n\n\n9\n2\n8.906846\n\n\n13\n2\n9.199057\n\n\n8\n2\n9.301780\n\n\n5\n2\n9.381039\n\n\n16\n1\n9.446345\n\n\n3\n1\n9.909773\n\n\n18\n2\n10.017880\n\n\n17\n2\n10.075152\n\n\n\n\n\n\nIn the intervention variable, the numbers 1 and 2 refer to different intervention groups. Therefore, the variable is a factor (data type) variable. To ensure that R understands this, we can resave the intervention variable as a factor using the as.factor() function:\n\nhappinessSample$intervention &lt;- as.factor(happinessSample$intervention)"
  },
  {
    "objectID": "practicals/practical1.html",
    "href": "practicals/practical1.html",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "",
    "text": "B Working with script files in R Studio\nTo work with R, it is possible to type commands directly into the console and see the results. However, this is not a good way to work with R, because:\nInstead, it is better to write scripts that contain the commands that you want to run amd save them, so they can be reused and shared with others."
  },
  {
    "objectID": "practicals/practical1.html#create-a-folder-for-your-data-and-scripts",
    "href": "practicals/practical1.html#create-a-folder-for-your-data-and-scripts",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.1 Create a folder for your data and scripts",
    "text": "B.1 Create a folder for your data and scripts\nBefore you start, create a folder on your computer to store your data and scripts. This will make it easier to find your files later. You can call this folder anything you like, but it is a good idea to give it a name that is related to your project. For example, for this session, you might create a folder called Practical1.\n\n\n\n\n\n\nTask\n\n\n\nCreate a folder on your computer to store your data and scripts from this session."
  },
  {
    "objectID": "practicals/practical1.html#set-the-working-directory",
    "href": "practicals/practical1.html#set-the-working-directory",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.2 Set the working directory",
    "text": "B.2 Set the working directory\nThe working directory is the folder that R will look in for files. To set the working directory, click on the Session -&gt; Set Working Directory -&gt; Choose Directory and select the folder you created for your data and script.\n\n\n\n\n\n\nTask\n\n\n\nSet the working directory to the folder you created for your data and scripts."
  },
  {
    "objectID": "practicals/practical1.html#create-a-new-script-file",
    "href": "practicals/practical1.html#create-a-new-script-file",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.3 Create a new script file",
    "text": "B.3 Create a new script file\nTo create a new script file, click on the File -&gt; New File -&gt; R Script. This will open a new window, which is where you will write your script. It is important to save your script as you work. To do this, click on the File -&gt; Save As and give your script a name. It is a good idea to save your script in the same folder as your data.\n\n\n\n\n\n\nTask\n\n\n\nCreate a new script file called **Practical1* and save it in the folder you created for your data and scripts. It should have the extension .R - i.e.: Practical1.R."
  },
  {
    "objectID": "practicals/practical1.html#writing-commands-in-a-script",
    "href": "practicals/practical1.html#writing-commands-in-a-script",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.5 Writing commands in a script",
    "text": "B.5 Writing commands in a script\nIn an R script, any text that is not a comment is a command. Commands are instructions that tell R to do something.\nFor example:\n\n# Add two numbers together\n\n23 + 52\nThe code above is a command that tells R to add 23 and 52. However, it will not do anything until you tell R to run it.\n\n\n\n\n\n\nTask\n\n\n\nAsk the ages of the people sitting next to you and add them together. Write the command in your script."
  },
  {
    "objectID": "practicals/practical1.html#running-code-in-a-script",
    "href": "practicals/practical1.html#running-code-in-a-script",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.6 Running code in a script",
    "text": "B.6 Running code in a script\nTo run code in a script, you can either highlight the code and click on the Run button, or you can use the keyboard shortcut Ctrl + Enter. This will run the code and print the results in the console.\n\n\n\n\n\n\nTask\n\n\n\nIf you have not already done so, run the code you wrote in the previous step."
  },
  {
    "objectID": "practicals/practical1.html#adding-comments-to-a-script",
    "href": "practicals/practical1.html#adding-comments-to-a-script",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.4 Adding comments to a script",
    "text": "B.4 Adding comments to a script\nIt is possible to add information to a script that will not be run as a command. This is called a comment. Comments are useful for adding information about what you are doing in your script. To add a comment, you need to start the line with a hash symbol (#). Anything after the hash symbol will be ignored by R. For example:\n# This is a comment\n\n\n\n\n\n\nTask\n\n\n\nWrite a comment in your script that describes what you are doing in this session. For example: “This script contains the commands for the first practical session in PSY4179 Research Methods 1”."
  },
  {
    "objectID": "slides/Lecture1.html",
    "href": "slides/Lecture1.html",
    "title": "2  Introduction to clinical research designs",
    "section": "",
    "text": "3 We will return to this later…\n“any type of psychological research investigating processes that are involved in the development and/or maintenance of psychopathology across any level of explanation (e.g., biological, cognitive, behavioral)” (Ehring et al., 2022)"
  },
  {
    "objectID": "slides/Lecture1.html#overview",
    "href": "slides/Lecture1.html#overview",
    "title": "2  Introduction to clinical research designs",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThere are many ongoing discussions in clinical psychology research. They will be picked up in other sessions this year. Today we will discuss:\n\nTranslating research into effective treatment\nCinical analogue studies\nOpen and reproducable research"
  },
  {
    "objectID": "slides/Lecture1.html#research-designs-in-clinical-psychology",
    "href": "slides/Lecture1.html#research-designs-in-clinical-psychology",
    "title": "2  Introduction to clinical research designs",
    "section": "2.2 Research designs in clinical psychology",
    "text": "2.2 Research designs in clinical psychology\n\nA wide range of approaches are commonly used: Experimental and quasi-experimental, correlational, case study, and qualitative designs.\nTo investigate psychological disorders, assessment, treatment, prevention, professional training, ethics, and cultural diversity.\n\n\n\n(Blampied, 1998; Comer & Kendall, 2013; Kazdin, 2021; Roberts & Ilardi, 2005; Sanderson & Barlow, 1991)"
  },
  {
    "objectID": "slides/Lecture1.html#types-of-psychopathology-research",
    "href": "slides/Lecture1.html#types-of-psychopathology-research",
    "title": "2  Introduction to clinical research designs",
    "section": "2.3 Types of psychopathology research",
    "text": "2.3 Types of psychopathology research\n\n(Comer & Kendall, 2013)"
  },
  {
    "objectID": "slides/Lecture1.html#experimental-psychopathology-research-example",
    "href": "slides/Lecture1.html#experimental-psychopathology-research-example",
    "title": "2  Introduction to clinical research designs",
    "section": "2.4 Experimental psychopathology research example",
    "text": "2.4 Experimental psychopathology research example\nPeters et al. (2011) employed an experimental approach to test the hypothesis that attributional style may be one causative factor of depression vulnerability.\n\nUndergraduate students, without a history of depression, were randomly assigned to one of two experimental conditions: resilience condition or vulnerability condition\nThe resiliency condition involved exposing participants to 60 descriptions that promoted a selfworthy, stable attribution of a positive event and 60 descriptions that promoted an unstable attribution unrelated to self-worth for a negative event.\nThe vulnerability condition involved exposing participants to 60 descriptions that promoted a self-deﬁcient, stable attribution of a negative event and 60 descriptions that promoted an unstable attribution unrelated to self-worth for a positive event.\nFollowing exposure to the assigned descriptions, all participants subsequently completed a stressor task (i.e., Cognitive Ability Test).\nPeters and colleagues measured the change in mood state from before to after manipulation.\nResults indicated that individuals in the resilience condition reported less depressed mood (compared to the vulnerability condition) in response to the academic stressor"
  },
  {
    "objectID": "slides/Lecture1.html#experimental-psychopathology-research",
    "href": "slides/Lecture1.html#experimental-psychopathology-research",
    "title": "2  Introduction to clinical research designs",
    "section": "2.5 Experimental psychopathology research",
    "text": "2.5 Experimental psychopathology research\nSometimes referred to as “Type 1” clinical research (Comer & Kendall, 2013)\n\nAble to identify putative causal variables that are directly manipulable (i.e., How or why might psychopathology develop?).\nMay serve as the “building blocks” of future intervention eﬀorts.\nRarely followed through to the point of application."
  },
  {
    "objectID": "slides/Lecture1.html#quasi-experimental-research-example",
    "href": "slides/Lecture1.html#quasi-experimental-research-example",
    "title": "2  Introduction to clinical research designs",
    "section": "2.6 Quasi experimental research example",
    "text": "2.6 Quasi experimental research example\nFelmingham et al. (2010) recorded functional magnetic resonance imaging data in both male and female participants with a diagnosis of PTSD.\n\nTrauma-exposed controls, and non– trauma-exposed controls while they viewed masked facial expressions of fear.\nBy examining neural activation to threat, Felmingham and colleagues sought to elucidate one of the possible pathways through which women have a greater propensity than men to develop PTSD following trauma.\nFindings indicated that exposure to trauma was associated with enhanced brainstem activity to fear in women, regardless of the presence of PTSD; however, in men, brainstem activity was associated only with the development of PTSD.\nMoreover, men with PTSD (compared to women) displayed greater hippocampal activity to fear, possibly suggesting that men have an enhanced capacity for contextualizing fear-related stimuli"
  },
  {
    "objectID": "slides/Lecture1.html#quasi-experimental-psychopathology-research",
    "href": "slides/Lecture1.html#quasi-experimental-psychopathology-research",
    "title": "2  Introduction to clinical research designs",
    "section": "2.7 Quasi experimental psychopathology research",
    "text": "2.7 Quasi experimental psychopathology research\nSometimes referred to as “Type II” clinical research (Comer & Kendall, 2013)\n\nType II research can help identify independent variables that exacerbate, or modify the expression of, existing forms of abnormal behavior\nCannot to answer causal questions about the development of psychopathology due to pre-existence in sample and interaction with so many other variables\n\nDiagnosis is a summary of cumulative history but said history cannot be used to infer what caused diagnosis\n\nIf the IV is a clinical intervention, then could examine variables that either prevent or ameliorate psychopathology"
  },
  {
    "objectID": "slides/Lecture1.html#why-does-basic-research-matter",
    "href": "slides/Lecture1.html#why-does-basic-research-matter",
    "title": "2  Introduction to clinical research designs",
    "section": "5.1 Why does basic research matter?",
    "text": "5.1 Why does basic research matter?\nInformation about core psychological processes is essential to:\n\nclarify how particular psychological disorders develop, are maintained, and ultimately how they may be prevented\nguide treatment development if the psychopathology process of interest can be modeled with sufficient detail\n\n\n\n(Forsyth & Zvolensky, 2001)."
  },
  {
    "objectID": "slides/Lecture1.html#how-much-are-treatments-informed-by-research",
    "href": "slides/Lecture1.html#how-much-are-treatments-informed-by-research",
    "title": "2  Introduction to clinical research designs",
    "section": "5.2 How much are treatments informed by research?",
    "text": "5.2 How much are treatments informed by research?\n\nOnly 23% of treatments showed a very strong link between basic research and the development of the intervention, and further 20% showed a strong link\n\n\n\n(Ehring et al., 2022)."
  },
  {
    "objectID": "slides/Lecture1.html#some-of-the-issues-with-translating-cinical-psychology-research-into-treatmentintervention",
    "href": "slides/Lecture1.html#some-of-the-issues-with-translating-cinical-psychology-research-into-treatmentintervention",
    "title": "2  Introduction to clinical research designs",
    "section": "5.3 Some of the issues with translating cinical psychology research into treatment/intervention",
    "text": "5.3 Some of the issues with translating cinical psychology research into treatment/intervention\n\nLack of stability and replicability of basic research findings\nLack of basic studies establishing causality before moving to testing complex clinical intervention\nFat-handed interventions and easy-to-vary theories\nImbalance between research focused on efficacy compared to research on mediators, mechanisms of change, and moderators"
  },
  {
    "objectID": "slides/Lecture1.html#lack-of-stability-and-replicability-of-basic-research-findings",
    "href": "slides/Lecture1.html#lack-of-stability-and-replicability-of-basic-research-findings",
    "title": "2  Introduction to clinical research designs",
    "section": "5.4 Lack of stability and replicability of basic research findings",
    "text": "5.4 Lack of stability and replicability of basic research findings\nResearch findings in clinical psychology are not as reliable as assumed, due to:\n\nUse of unreliable measures\nUnderpowered samples\nPublication bias\nLack of transparency in research practices"
  },
  {
    "objectID": "slides/Lecture1.html#lack-of-basic-studies-establishing-causality-before-moving-to-testing-complex-clinical-intervention",
    "href": "slides/Lecture1.html#lack-of-basic-studies-establishing-causality-before-moving-to-testing-complex-clinical-intervention",
    "title": "2  Introduction to clinical research designs",
    "section": "5.5 Lack of basic studies establishing causality before moving to testing complex clinical intervention",
    "text": "5.5 Lack of basic studies establishing causality before moving to testing complex clinical intervention\n\nTargets for intervention are often identified via correlational studies\nOnce identified, they are often tested in applied studies using novel or modified interventions\nThis misses two important steps:\n\n\nIs the target a cause of psychopathology?\nHow do we modify interventions to change this target?\n\n\nThese questions need to be answered through basic research."
  },
  {
    "objectID": "slides/Lecture1.html#overly-broad-interventions-and-easy-to-vary-theories",
    "href": "slides/Lecture1.html#overly-broad-interventions-and-easy-to-vary-theories",
    "title": "2  Introduction to clinical research designs",
    "section": "5.6 Overly-broad interventions and easy-to-vary theories",
    "text": "5.6 Overly-broad interventions and easy-to-vary theories\n\nEstablishing causality is difficult in psychological research\nInterventions often lack precision in terms of what they are targeting\nCan lack clear theoretical framework of the mechanisms of change and how psychological processes (e.g., attention, perception, memory, emotion) are involved\nTheories are often easy to vary, and can be used to explain almost any outcome"
  },
  {
    "objectID": "slides/Lecture1.html#imbalance-between-research-focused-on-efficacy-compared-to-research-on-mediators-mechanisms-of-change-and-moderators",
    "href": "slides/Lecture1.html#imbalance-between-research-focused-on-efficacy-compared-to-research-on-mediators-mechanisms-of-change-and-moderators",
    "title": "2  Introduction to clinical research designs",
    "section": "5.7 Imbalance between research focused on efficacy compared to research on mediators, mechanisms of change, and moderators",
    "text": "5.7 Imbalance between research focused on efficacy compared to research on mediators, mechanisms of change, and moderators\n\nIn order to improve treatments, we need to understand how and why they work, not just if they work\nTo get a complete understanding of a treatment, we need to understand: Efficacy, Mediators, Mechanisms of change, Moderators\nThis requires whole programs of research, not just a single study"
  },
  {
    "objectID": "slides/Lecture1.html#what-are-clinical-analogue-studies",
    "href": "slides/Lecture1.html#what-are-clinical-analogue-studies",
    "title": "2  Introduction to clinical research designs",
    "section": "6.1 What are clinical analogue studies?",
    "text": "6.1 What are clinical analogue studies?\n\nClinical analogue studies are studies that use non-clinical samples to study processes related to psychopathology.\n\nFor example: The role of attention in PTSD\nIs this a form of attentional bias towrds threat stimuli or an inability to disengage from threat stimuli?\n\nThey allow us to study processes related to psychopathology in a controlled environment.\nThey allow specific variables to be manipulated to identify mechanisms of change."
  },
  {
    "objectID": "slides/Lecture1.html#why-use-clinical-analogue-studies-1",
    "href": "slides/Lecture1.html#why-use-clinical-analogue-studies-1",
    "title": "2  Introduction to clinical research designs",
    "section": "6.2 Why use clinical analogue studies? #1",
    "text": "6.2 Why use clinical analogue studies? #1\n\nResearch with clinical groups is often correlational which does not allow drawing conclusions about what causation.\nThe use of experimental designs with clinical population can be ethically problematic, as exposure to stimuli (e.g., stressors) to measure “in the moment” effects, could be traumatic.\nRetrospective reports lack objective information about external stumuli or order of events.\n\n\n\n(Ehring et al., 2011)"
  },
  {
    "objectID": "slides/Lecture1.html#why-use-clinical-analogue-studies-2",
    "href": "slides/Lecture1.html#why-use-clinical-analogue-studies-2",
    "title": "2  Introduction to clinical research designs",
    "section": "6.3 Why use clinical analogue studies? #2",
    "text": "6.3 Why use clinical analogue studies? #2\n\nSubclinical measurements can allow accurate modelling of relevant processes (e.g. Depression: Hill et al. (1987)).\nAllows design of studies to better understand the relationship between:\n\ntreatment -&gt; process\n\nprocess -&gt; outcome\n\n\n(Ehring et al., 2022)\n\nAllows control and focus on specific variables to identify mechanisms of change (Ehring et al., 2022)"
  },
  {
    "objectID": "slides/Lecture1.html#intervention---process---outcome",
    "href": "slides/Lecture1.html#intervention---process---outcome",
    "title": "2  Introduction to clinical research designs",
    "section": "6.4 Intervention -> process -> outcome",
    "text": "6.4 Intervention -&gt; process -&gt; outcome"
  },
  {
    "objectID": "slides/Lecture1.html#clinical-relevance-of-analogue-studies",
    "href": "slides/Lecture1.html#clinical-relevance-of-analogue-studies",
    "title": "2  Introduction to clinical research designs",
    "section": "6.5 Clinical relevance of analogue studies",
    "text": "6.5 Clinical relevance of analogue studies\n\n“the nature and intensity of the target problem, not the clinical status of the subjects, are the critical variables in analogue research” (e.g., fear and phobias: Borkovec & Rachman (1979), p. 253).\nBasic research has helped identify etiological factors in the development of many disorders (e.g. OCD: Gibbs (1996))\nSymptoms can be prevalent in non-clinical populations, with similar qualitative expressions of experience and similar causal and maintenance factors Puckett et al. (n.d.)"
  },
  {
    "objectID": "slides/Lecture1.html#limitations-of-clinical-analogue-studies-1",
    "href": "slides/Lecture1.html#limitations-of-clinical-analogue-studies-1",
    "title": "2  Introduction to clinical research designs",
    "section": "6.6 Limitations of clinical analogue studies? #1",
    "text": "6.6 Limitations of clinical analogue studies? #1\nPotential issues with clinical analogue studies include:\n\nEcological validity: the extent to which the findings of a research study are able to be generalized to real-life settings.\nExternal validity: the extent to which the findings of a research study are able to be generalized to other people, settings, and times.\nComorbidity: the presence of one or more additional disorders (or diseases) co-occurring with a primary disorder or disease."
  },
  {
    "objectID": "slides/Lecture1.html#limitations-of-clinical-analogue-studies-2",
    "href": "slides/Lecture1.html#limitations-of-clinical-analogue-studies-2",
    "title": "2  Introduction to clinical research designs",
    "section": "6.7 Limitations of clinical analogue studies? #2",
    "text": "6.7 Limitations of clinical analogue studies? #2\nHowever, it has been argued that the limitations of clinical analogue studies are often overstated (Abramowitz et al., 2014) and that the weaknesses of clinical analogue studies are often shared with clinical studies (i.e. poor research is poor research, regardless of the sample):\n\nLack of sufficient power in studies\nReliance on findings from single studies that have yet to be replicated\nWeakness in research designs\nOver-reliance on or misunderstanding of NHST\nDiagnostic unreliability\nSelective reporting of results"
  },
  {
    "objectID": "slides/Lecture1.html#what-is-reproducable-and-open-research",
    "href": "slides/Lecture1.html#what-is-reproducable-and-open-research",
    "title": "2  Introduction to clinical research designs",
    "section": "7.1 What is reproducable and open research?",
    "text": "7.1 What is reproducable and open research?\n\nReproducible research is the idea that research is published with their hypotheses, research plan, materials, data and software code so that others can try to replicate the results and verify the findings.\nStudies are pre-registered, so that researchers can see if the study was conducted as planned and the results support the researchers’ original hypotheses."
  },
  {
    "objectID": "slides/Lecture1.html#why-reproducable-research",
    "href": "slides/Lecture1.html#why-reproducable-research",
    "title": "2  Introduction to clinical research designs",
    "section": "7.2 Why Reproducable Research?",
    "text": "7.2 Why Reproducable Research?\n\nOne of the major limitations of clinical research can be lack of transparency and reproducibility\nLinked to the “easy to vary” problem\nLinked to the “underpowered” problem\nLinked to the “publication bias” problem\nLinked to the “file drawer” problem"
  },
  {
    "objectID": "slides/Lecture1.html#how-does-open-research-help-address-these-problems",
    "href": "slides/Lecture1.html#how-does-open-research-help-address-these-problems",
    "title": "2  Introduction to clinical research designs",
    "section": "7.3 How does open research help address these problems?",
    "text": "7.3 How does open research help address these problems?\n\nResearchers outline their goals and theories before they begin their research, so that they can be held accountable for their results.\nSample sizes are determined before the study begins, so that the study is adequately powered to detect the effect size of interest.\nResearchers are encouraged to make all of their results available, regardless of whether the results are statistically significant (where possible).\nMaterials, data and code are made available, so that others can try to replicate the results and verify the findings."
  },
  {
    "objectID": "slides/Lecture1.html#how-to-make-your-research-reproducable",
    "href": "slides/Lecture1.html#how-to-make-your-research-reproducable",
    "title": "2  Introduction to clinical research designs",
    "section": "7.4 How to make your research reproducable?",
    "text": "7.4 How to make your research reproducable?\n\nPre-register your study\n\nAsPredicted\nOpen Science Framework\nProspero\n\nConduct a power anaysis before you begin your study\n\nG*Power\nWebpower\nR, SPSS etc.\n\nUpdate your registration with any changes to your study and explain why you made the changes\nAfter your study has been completed, upload your materials, data and code to a repository\n\nOpen Science Framework\nFigshare\nZenodo\nDryad\nDataverse\nHarvard Dataverse\nOpenICPSR\nOpenNeuro\nOpenfMRI"
  },
  {
    "objectID": "slides/Lecture1.html#references",
    "href": "slides/Lecture1.html#references",
    "title": "2  Introduction to clinical research designs",
    "section": "7.5 References",
    "text": "7.5 References\n\n\n\n\nAbramowitz, J. S., Fabricant, L. E., Taylor, S., Deacon, B. J., McKay, D., & Storch, E. A. (2014). The relevance of analogue studies for understanding obsessions and compulsions. Clinical Psychology Review, 34(3), 206–217. https://doi.org/10.1016/j.cpr.2014.01.004\n\n\nAbramowitz, J. S., Tolin, D. F., & Street, G. P. (2001). Paradoxical effects of thought suppression: A meta-analysis of controlled studies. Clinical Psychology Review, 21(5), 683–703. https://doi.org/10.1016/S0272-7358(00)00057-X\n\n\nBlampied, N. M. (1998). Research Designs and Methods in Psychiatry Edited M Fava and J.F. Rosenbaum Series: Techniques in the Behavioral and Neural Sciences; Volume 9; Amsterdam: Elsevier; 1992; US98.50 (paperback). Behaviour Change, 15(2), 147–149.\n\n\nBorkovec, T., & Rachman, S. (1979). The utility of analogue research. Behaviour Research and Therapy, 17(3), 253–261. https://doi.org/10.1016/0005-7967(79)90040-8\n\n\nComer, J. S., & Kendall, P. C. (Eds.). (2013). The Oxford Handbook of Research Strategies for Clinical Psychology (1st ed.). Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199793549.001.0001\n\n\nEhring, T., Kleim, B., & Ehlers, A. (2011). Combining Clinical Studies and Analogue Experiments to Investigate Cognitive Mechanisms in Posttraumatic Stress Disorder. International Journal of Cognitive Therapy, 4(2), 165–177. https://doi.org/10.1521/ijct.2011.4.2.165\n\n\nEhring, T., Limburg, K., Kunze, A. E., Wittekind, C. E., Werner, G. G., Wolkenstein, L., Guzey, M., & Cludius, B. (2022). (When and how) does basic research in clinical psychology lead to more effective psychological treatment for mental disorders? Clinical Psychology Review, 95, 102163. https://doi.org/10.1016/j.cpr.2022.102163\n\n\nFelmingham, K., Williams, L. M., Kemp, A. H., Liddell, B., Falconer, E., Peduto, A., & Bryant, R. (2010). Neural responses to masked fear faces: Sex differences and trauma exposure in posttraumatic stress disorder. Journal of Abnormal Psychology, 119(1), 241–247. https://doi.org/10.1037/a0017551\n\n\nForsyth, J. P., & Zvolensky, M. J. (2001). Experimental psychopathology, clinical science, and practice: An irrelevant or indispensable alliance? Applied & Preventive Psychology, 10(4), 243–264. https://doi.org/10.1016/s0962-1849(01)80002-0\n\n\nGibbs, N. A. (1996). Nonclinical populations in research on obsessive-compulsive disorder: A critical review. Clinical Psychology Review, 16(8), 729–773. https://doi.org/10.1016/S0272-7358(96)00043-8\n\n\nHill, A. B., Kemp-Wheeler, S. M., & Jones, S. A. (1987). Subclinical and clinical depression: Are analogue studies justifiable? Personality and Individual Differences, 8(1), 113–120. https://doi.org/10.1016/0191-8869(87)90017-1\n\n\nKazdin, A. E. (2021). Research Design in Clinical Psychology.\n\n\nPeters, K. D., Constans, J. I., & Mathews, A. (2011). Experimental modification of attribution processes. Journal of Abnormal Psychology, 120(1), 168–173. https://doi.org/10.1037/a0021899\n\n\nPuckett, J., Sood, M., & Newman-Taylor, K. (n.d.). Does disorganised attachment lead to auditory hallucinations via dissociation? An experimental study with an analogue sample. Psychology and Psychotherapy: Theory, Research and Practice, n/a(n/a). https://doi.org/10.1111/papt.12477\n\n\nRoberts, M. C., & Ilardi, S. S. (Eds.). (2005). Handbook of Research Methods in Clinical Psychology.\n\n\nSanderson, W. C., & Barlow, D. H. (1991). Research Strategies in Clinical Psychology. In Clinical Psychology (pp. 37–49). Springer US."
  },
  {
    "objectID": "practicals/practical1.html#saving-objects-in-r",
    "href": "practicals/practical1.html#saving-objects-in-r",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.7 Saving objects in R",
    "text": "B.7 Saving objects in R\nWhen you run a command in R, you can save the results to an object. For example, when you run the command to add two numbers together, you can save this object by assigning it to a name. For example:\n# Add two numbers together\n\nresult &lt;- 23 + 52\nThe code above is a command that tells R to add 23 and 52 and save the result to an object called result. You can save many things as objects including the results of analyses, data, and graphs.\n\n\n\n\n\n\nTask\n\n\n\nRun the code you wrote in the previous step. Then, run the following command to print the object ages in the console."
  },
  {
    "objectID": "practicals/practical1.html#using-functions-in-r",
    "href": "practicals/practical1.html#using-functions-in-r",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.8 Using functions in R",
    "text": "B.8 Using functions in R\nA function is a command that performs a specific task. For example, the function mean() calculates the mean of a set of numbers. To use a function, you need to provide it with some information. For example, to calculate the mean of a set of numbers, you need to tell the function which numbers to use. This is called an argument. For example:\n# Calculate the mean of my previous result\n\nmean(result)\nIn the code above, the function mean() is used to calculate the mean of the object result. The object result is the argument for the function mean().\n\n\n\n\n\n\nTask\n\n\n\n\nCalculate the mean of the object ages and save it to an object called mean_age. Print the object mean_age in the console.\nUse the sd() function to calculate the standard deviation of the object ages and save it to an object called sd_age. Print the object sd_age in the console.\nCreate a new object called ages_doubled that contains the object ages multiplied by 2. Print the object ages_doubled in the console.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote how each of the values in the ages_doubled object is twice the value of the corresponding value in the ages object."
  },
  {
    "objectID": "practicals/practical1.html#using-packages-in-r",
    "href": "practicals/practical1.html#using-packages-in-r",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.9 Using packages in R",
    "text": "B.9 Using packages in R\nTo install a package, you can use the install.packages() function. For example, to install the package psych, you would use the following command:\ninstall.packages(\"psych\")\nTo load the package, you can use the library() function. For example, to load the package psych, you would use the following command:\nlibrary(psych)\n\n\n\n\n\n\nTask\n\n\n\n\nInstall the package psych. We will use it in a later practical session.\nLoad the package psych."
  },
  {
    "objectID": "practicals/practical1.html#saving-your-script",
    "href": "practicals/practical1.html#saving-your-script",
    "title": "Appendix A — Practical 1 Introduction to R and R Studio",
    "section": "B.10 Saving your script",
    "text": "B.10 Saving your script\nRemember to save your script regularly as you work. To save your script, click on the File -&gt; Save or use the keyboard shortcut Ctrl + S.\n\n\n\n\n\n\nTask\n\n\n\nSave your script."
  },
  {
    "objectID": "practicals/practical2.html",
    "href": "practicals/practical2.html",
    "title": "Appendix B — Practical 2 - Power and effect size",
    "section": "",
    "text": "C Calculate power for a between groups t-test"
  },
  {
    "objectID": "practicals/practical2.html#figure-out-the-effect-size-we-are-looking-for",
    "href": "practicals/practical2.html#figure-out-the-effect-size-we-are-looking-for",
    "title": "Appendix B — Practical 2 - Power and effect size",
    "section": "\nC.1 Figure out the effect size we are looking for",
    "text": "C.1 Figure out the effect size we are looking for\nThe most common measure of effect size for a between groups t-test is Cohen’s d. Cohen’s d is calculated as the difference between the means of the two groups divided by the pooled standard deviation.\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p}\\]\nwhere \\(\\bar{x}_1\\) is the mean of group 1, \\(\\bar{x}_2\\) is the mean of group 2, and \\(s_p\\) is the pooled standard deviation.\n\n\n\n\n\n\nPooling standard deviation\n\n\n\n\n\nPooled standard deviation is calculated as:\n\\[s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\\]\nwhere \\(n_1\\) is the sample size of group 1, \\(n_2\\) is the sample size of group 2, \\(s_1^2\\) is the variance of group 1, and \\(s_2^2\\) is the variance of group 2.\n\n\n\nFor this exercise, let’s base the effect size on the results of a previous study. In the previous study, the mean score for the control group was 10.5 and the mean score for the experimental group was 12.5. The pooled standard deviation was 3.5.\n\n\n\n\n\n\nTask\n\n\n\nCalculate the effect size of the previous study\n\n\n\nThe effect size of the previous study was:"
  }
]