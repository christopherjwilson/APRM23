[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Psychological Research Methods",
    "section": "",
    "text": "Welcome to the module\nThis site accompanies PSY4179-N and is specifically focused on the quantitative research design, stats, R sessions delivered by Christopher Wilson.\nInformation about the learning outcomes, assessments and other teaching sessions on the module can be found on the module Blackboard site."
  },
  {
    "objectID": "instructions.html#sec-setup",
    "href": "instructions.html#sec-setup",
    "title": "1  How to Use this Book",
    "section": "\n1.1 Setup",
    "text": "1.1 Setup\n\n1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  - introductiontoR.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\n\n\nTable 1.1: The authors of this book\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/."
  },
  {
    "objectID": "instructions.html#sec-layout",
    "href": "instructions.html#sec-layout",
    "title": "1  How to Use this Book",
    "section": "\n1.2 Layout",
    "text": "1.2 Layout\n\n1.2.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools > Global Options… > Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.2.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.2.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nmy_theme <- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.2.3 Tables\nShorter tables and PDFs print using kableExtra and longer tables in HTML format use DT for interactive paged tables.\n\nhead(beaver1)\n\n\n\n\nTable 1.2: Shorter table\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n\n\n\n\n1.2.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nDanger\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.2.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na <- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n1.2.6 Fonts"
  },
  {
    "objectID": "instructions.html#sec-extras",
    "href": "instructions.html#sec-extras",
    "title": "1  How to Use this Book",
    "section": "\n1.3 Extras",
    "text": "1.3 Extras\n\n1.3.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"): The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\n\n term \n    definition \n  \n\n\n alpha \n    The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed. \n  \n\n beta \n    The second letter of the Greek alphabet \n  \n\n p-value \n    The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true. \n  \n\n\n\n\n\n1.3.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n{{< fa dragon >}}\n{{< fa brands github size = 5x title=\"(github logo)\" >}}\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis"
  },
  {
    "objectID": "index.html#statistical-analysis-software",
    "href": "index.html#statistical-analysis-software",
    "title": "Advanced Psychological Research Methods",
    "section": "Statistical analysis software",
    "text": "Statistical analysis software\nYou may be familiar with SPSS from your undergraduate statistics teaching. Please note that we do not use SPSS for teaching and instead use R Statistics. The reason for this is that R is a free statistical package, meaning that it can be accessed in NHS settings that do not have funding for SPSS. This will enable you to run statistical analyses whilst on placement where required, and also enables you to conduct statistical analyses as a qualified Psychologist without incurring any software costs. R is also more flexible than SPSS and has greater functionality. During the teaching you will be shown how to set up and install R, and how to run statistical analyses in this software.\nDownloading R and R Studio software\n\n\nYou can obtain R and R Studio from the following links:\nhttps://cran.r-project.org/\nhttps://rstudio.com/"
  },
  {
    "objectID": "index.html#textbooks-that-can-be-accessed-online",
    "href": "index.html#textbooks-that-can-be-accessed-online",
    "title": "Advanced Psychological Research Methods",
    "section": "Textbooks that can be accessed online",
    "text": "Textbooks that can be accessed online\ne-books can be accessed from the library website: https://www.tees.ac.uk/depts/lis/\nResearch Methods and Statistics\nCoolican, 2019. Research Methods and Statistics in Psychology. Taylor & Francis Group\nBarker, C., Pistrang, N., & Elliott, R. (2015). Research methods in clinical psychology: An introduction for students and practitioners (3rd ed.). Chichester, West Sussex: Wiley Blackwell.\nWeiner, I. B., Schinka, J. A., & Velicer, W. F. (2012). Handbook of psychology, research methods in psychology (2. Aufl. ed.). Somerset: Wiley.\nWorking with R and RStudio to do analysis\nNavarro, D. (2017) Learning statistics with R.\nPhillips N. D. (2018) YaRrr! The Pirate’s Guide to R\nHorton, Pruim and Kaplan (2015) A Student’s Guide to R\nMather, M. (2019) R for Academics\nWickham and Grolemund (2019). R For Data Science\nAllaire and Grolemund (2019). R Markdown: The Definitive Guide\nBasics of RStudio\nData Import\nData Transformation\nData Visualisation with GGPlot"
  },
  {
    "objectID": "index.html#data-analysis-with-r",
    "href": "index.html#data-analysis-with-r",
    "title": "Advanced Psychological Research Methods",
    "section": "Data analysis with R",
    "text": "Data analysis with R\nYou may be familiar with SPSS from your undergraduate statistics teaching. Please note that we do not use SPSS for teaching and instead use R Statistics. The reason for this is that R is a free statistical package, meaning that it can be accessed in NHS settings that do not have funding for SPSS. This will enable you to run statistical analyses whilst on placement where required, and also enables you to conduct statistical analyses as a qualified Psychologist without incurring any software costs. R is also more flexible than SPSS and has greater functionality. During the teaching you will be shown how to set up and install R, and how to run statistical analyses in this software.\nDownloading R and R Studio software\n\n\nYou can obtain R and R Studio from the following links:\nhttps://cran.r-project.org/\nhttps://rstudio.com/"
  },
  {
    "objectID": "introductiontoR.html#by-the-end-of-this-section-you-should-be-able-to",
    "href": "introductiontoR.html#by-the-end-of-this-section-you-should-be-able-to",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.1 By the end of this section, you should be able to:",
    "text": "1.1 By the end of this section, you should be able to:\n\nDownload R and R studio\nIdentify the R script, R console, Data environment and file browser in R studio\nWrite and run R code from a script\nInstall and load R packages"
  },
  {
    "objectID": "introductiontoR.html#why-learn-use-r",
    "href": "introductiontoR.html#why-learn-use-r",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.2 Why learn / use R?",
    "text": "1.2 Why learn / use R?\n\n1.2.1 Some information about R\n\nR is developed and used by scientists and researchers around the world\nOpen source = no cost\nConstant development\nConnects to other data science/research tools\nWorldwide community: training widely available\nEncourages transparency and reproducibility\nPublication-ready outputs\n\n1.2.2 Moving from other software to R\n\nWorkflow is different\n\nOrganise files and data differently\nWorkspace can contain data and outputs\nCan manage multiple datasets within a workspace\n\n\nLearning curve can be steep initially\n\ne.g. Variables and coding, scripts\n\n\nNeed to know what you want\n\ne.g. building your regression model / ANOVA error terms"
  },
  {
    "objectID": "introductiontoR.html#r-has-many-advantages",
    "href": "introductiontoR.html#r-has-many-advantages",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.3 R has many advantages",
    "text": "1.3 R has many advantages\n\nUsing scripts means analysis is easy to follow and reproduce\nR scripts are small, online collaboration, no SPSS “older version” problems\nData can be organised and reorganised however you need it (tidyr)\nPackages are available for “cutting edge” analysis: e.g. Big Data & Machine Learning\nA robust language for precise plots and graphics (ggplot)\nR analysis code can be embdeded into documents and presentations (R Markdown)"
  },
  {
    "objectID": "introductiontoR.html#download-r-and-r-studio",
    "href": "introductiontoR.html#download-r-and-r-studio",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.4 Download R and R Studio",
    "text": "1.4 Download R and R Studio\n\n\nClick on these links to download:\n\nR project\nRStudio"
  },
  {
    "objectID": "introductiontoR.html#the-r-studio-environment",
    "href": "introductiontoR.html#the-r-studio-environment",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.5 The R Studio environment",
    "text": "1.5 The R Studio environment\n\nThe interface for R Studio looks daunting at first. However, there are 4 main sections, 2 on the left and 2 on the right.\n\nMAIN TOP: R Script files or R Document Files\n\nWhere we usually type our code as a script before we run it. Script files are usually saved so we can work on them and rerun the code again later (.R files).\n\n\nMAIN BOTTOM: Console\n\nShows the output of our R code. We can type R code directly into the console and the answer will ouput immediately. However, it is more convenient to use script files.\n\n\nRIGHT TOP: Environment\n\nContains all of the objects (e.g. data, analysis, equations, plots) that are currently stored in memory. We can save all of this to a file and load it later (.RData files).\n\n\nRIGHT BOTTOM: File Browser\n\nThe folder that R is working from is called ‘the working directory’ and it will automatically look for files there if we try to import something (e.g. a data file). Using the more button on the file browser allows you to set your desired working directory."
  },
  {
    "objectID": "introductiontoR.html#working-with-a-script",
    "href": "introductiontoR.html#working-with-a-script",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.6 Working with a script",
    "text": "1.6 Working with a script\nScripts can be opened from the File menu.\n\n\nCreating a new script\n\n\nThe purpose of scripts is to allow you to type your analysis code and save it for use later. Scripts include, for example:\n\nCode for importing data into R\nYour analysis code (e.g. t-test or descriptive statistics)\nCode for graphs and tables\nComments and notes (preceded by the ‘#’ symbol)\n\n\n\nExample of an R script\n\n\nTo run a script, you click the Run button. You can choose to:\n\nRun the whole script\nRun the selected line of code\n\n\n\nThe run button\n\n\nWhen you run the script, you will normally see output in the console.\n\n\nOutput appears in the console\n\n\nIf your script contains code for a plot (graph), it will appear in the Plots window in the bottom right.\n\n\nPlots appear in the plot window"
  },
  {
    "objectID": "introductiontoR.html#installing-and-loading-packages",
    "href": "introductiontoR.html#installing-and-loading-packages",
    "title": "1  Introduction to R and R Studio",
    "section": "\n1.7 Installing and loading packages",
    "text": "1.7 Installing and loading packages\n\n\n\ninstall Packages from RStudio, Inc. on Vimeo.\n\nPackages add functionality to R and allow us to do new types of analysis.\n\nThey can be installed via the menu (Tools -> Install Packages)\n\nThe can also be installed using code:\n    install.packages()\n\n\nFor example, TidyR is a package that contains functions for sorting and organising data. To install the package:\n\n\nInstalling a package in RStudio\n\n\nor use the code:\n  install.packages(“tidyr”)\nOnce a package is has been installed, you need tp load it using the library() command. For example:\n\n   library(tidyr)"
  },
  {
    "objectID": "workingWithData.html#by-the-end-of-this-section-you-will-be-able-to",
    "href": "workingWithData.html#by-the-end-of-this-section-you-will-be-able-to",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.1 By the end of this section, you will be able to:",
    "text": "2.1 By the end of this section, you will be able to:\n\nImport data into R from excel, SPSS and csv files\nSave data to objects\nIdentify different data structures and variable types\nConvert variables from one type to another\nOrder, filter and group data\nSummarise data\nCreate new variables from data"
  },
  {
    "objectID": "workingWithData.html#in-this-section-we-will-use-the-tidyverse-set-of-packages",
    "href": "workingWithData.html#in-this-section-we-will-use-the-tidyverse-set-of-packages",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.2 In this section, we will use the Tidyverse set of packages",
    "text": "2.2 In this section, we will use the Tidyverse set of packages\n\nA ‘toolkit’ of packages that are very useful for organsing and manipulating data\nWe will use the haven package to import SPSS files\nWe will use the dplyr to organise data\nAlso includes the ggplot2 and tidyR packages which we will use later\n\nTo install:\n\ninstall.packages(“tidyverse”)\n\n(See the previous section on installing packages)"
  },
  {
    "objectID": "workingWithData.html#import-data-into-r-from-excel-spss-and-csv-files",
    "href": "workingWithData.html#import-data-into-r-from-excel-spss-and-csv-files",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.3 Import data into R from excel, SPSS and csv files",
    "text": "2.3 Import data into R from excel, SPSS and csv files\nWe can import data from a range of sources using the Import Dataset button in the Environment tab:\n\n\nImporting data\n\n\nIt is also possible to import data using code, for example:\n\n # importing a .csv file\n    \n    library(readr)\n    studentData <- read_csv(\"Datasets/studentData.csv\")\n\n    #importing an SPSS file\n    \n    library(haven)\n    mySPSSData <- read_sav(\"datasets/salesData.sav\")\n\nOnce the data are imported, it will be visible in the environment:\n\n\nImported data in the environment"
  },
  {
    "objectID": "workingWithData.html#restructuring-and-reorganising-data-in-r-long-versus-wide-data",
    "href": "workingWithData.html#restructuring-and-reorganising-data-in-r-long-versus-wide-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.4 Restructuring and reorganising data in R (long versus wide data)",
    "text": "2.4 Restructuring and reorganising data in R (long versus wide data)"
  },
  {
    "objectID": "workingWithData.html#understanding-objects-in-r",
    "href": "workingWithData.html#understanding-objects-in-r",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.5 Understanding objects in R",
    "text": "2.5 Understanding objects in R\nIn R, an object is anything that is saved to memory. For example, we might do some analysis:\n\n    mean(happiness)\n\nHowever, in the example above, the result would appear in the console but not be saved anywhere. To store the result for reuse later, we save it to an object:\n\nhappinessMean <- mean(happiness)\n\nIn the above code (reading left to right):\n\nWe name the object “happinessMean”. This name can be anything we want.\nThe arrow means that the result of the code on the right will be saved to the object on the left.\nThe code on the right of the arrow calculates the mean of happiness data\n\nWhen this code is run, happinessMean will be stored in the environment window:\n\n\nResult of a calculation in the environment\n\n\nTo recall an object from the environment, we can simply type its name. For example:\n\n happinessMean\n\n[1] 10.42048\n\n\n\nIts important to note that anything can be stored as an object in R and recalled later. This includes, dataframes, the results of statistical calculations, plots etc."
  },
  {
    "objectID": "workingWithData.html#identify-different-data-structures-and-variable-types",
    "href": "workingWithData.html#identify-different-data-structures-and-variable-types",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.6 Identify different data structures and variable types",
    "text": "2.6 Identify different data structures and variable types\n\n2.6.1 Data structures\nThere are many different types of data that R can work with. The most common type of data for most people tends to be a dataframe. A dataframe is what you might consider a “normal” 2-dimensional dataset, with rows of data and columns of variables:\n\n\nA dataframe example\n\n\nR can also use other data types.\nA vector is a one-dimensional set of values:\n\n# a vector example\n\nscores <- c(1,4,6,8,3,4,6,7)\n\nA matrix is a multi-dimensional set of values. The below example is a 3-dimensional matrix, there are 2 groups of 2 rows and 3 columns:\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n\n\nWe will primarily work with dataframes (and sometimes vectors), as this is how the data in psychology research is usually structured.\n\n\n2.6.2 Variable types\nWith numerical data, there are 4 key data types:\n\nNominal (a category, group or factor)\nOrdinal (a ranking)\nInterval (scale data that can include negative values)\nRatio (scale data that cannot include negative values)\n\n R can use all of these variable types:\n\n\nNominal variables are called factors\n\n\nOrdinal variables are called ordered factors\n\n\nInterval and ratio variables are called numeric data and can sometimes be called integers (if they are only whole numbers) or doubles (if they all have decimal points)\n\nR can also use other data types such as text (character) data.\n\n2.6.3 Convert variables from one type to another\nWhen we first import data into R, it might not recognise the data types correctly. For example, in the below data, we can see the intervention variable :\n\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n19\n1\n8.472506\n\n\n20\n1\n8.812926\n\n\n15\n2\n9.117198\n\n\n4\n2\n9.505235\n\n\n2\n2\n9.660847\n\n\n6\n2\n9.669608\n\n\n12\n1\n9.764612\n\n\n3\n1\n9.954183\n\n\n1\n2\n10.090664\n\n\n10\n1\n10.099453\n\n\n\n\n\n\nIn the intervention variable, the numbers 1 and 2 refer to different intervention groups. Therefore, the variable is a factor variable. To ensure that R understands this, we can resave the intervention variable as a factor using the as.factor() function:\n\nhappinessSample$intervention <- as.factor(happinessSample$intervention)"
  },
  {
    "objectID": "workingWithData.html#working-with-dataframes",
    "href": "workingWithData.html#working-with-dataframes",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.7 Working with dataframes",
    "text": "2.7 Working with dataframes\nDataframes are the more standard data format that were are used to (think of how a dataset looks in SPSS or Excel).\nIn a dataframe, variables are columns and each row usually reperesents one measurement or one participant.\n\n2.7.1 View dataframe\nTo view a dataframe, we can click on it in the environment window and it will display:\n\n\nClicking on datasets in hte environment will open them up for viewing\n\n\n\n\nViewing a dataframe\n\n\n\n2.7.2 Refer to variables (columns) in a dataframe\nColumns in a dataframe are accessed using the “$” sign. For example, to access the happiness column in the happinessSample dataframe, we would type:\n\nhappinessSample$happiness\n\n [1] 10.090664  9.660847  9.954183  9.505235 11.725063  9.669608 10.588609\n [8] 10.459963 12.244699 10.099453 10.420561  9.764612 12.700549 11.592937\n[15]  9.117198 11.415905 11.018959 11.095057  8.472506  8.812926\n\n\nAs we can see above, the result is then displayed."
  },
  {
    "objectID": "workingWithData.html#order-filter-and-group-data",
    "href": "workingWithData.html#order-filter-and-group-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.8 Order, filter and group data",
    "text": "2.8 Order, filter and group data\nIf you have the tidyverse package loaded, it is easy to organise and filter data.\n\narrange(happinessSample, happiness)\narrange(happinessSample, desc(happiness)) # Arrange in descending order\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n19\n1\n8.472506\n\n\n20\n1\n8.812926\n\n\n15\n2\n9.117198\n\n\n4\n2\n9.505235\n\n\n2\n2\n9.660847\n\n\n6\n2\n9.669608\n\n\n12\n1\n9.764612\n\n\n3\n1\n9.954183\n\n\n1\n2\n10.090664\n\n\n10\n1\n10.099453\n\n\n11\n2\n10.420561\n\n\n8\n1\n10.459963\n\n\n7\n1\n10.588609\n\n\n17\n1\n11.018959\n\n\n18\n2\n11.095057\n\n\n16\n2\n11.415905\n\n\n14\n1\n11.592937\n\n\n5\n1\n11.725063\n\n\n9\n2\n12.244699\n\n\n13\n2\n12.700549\n\n\n\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n13\n2\n12.700549\n\n\n9\n2\n12.244699\n\n\n5\n1\n11.725063\n\n\n14\n1\n11.592937\n\n\n16\n2\n11.415905\n\n\n18\n2\n11.095057\n\n\n17\n1\n11.018959\n\n\n7\n1\n10.588609\n\n\n8\n1\n10.459963\n\n\n11\n2\n10.420561\n\n\n10\n1\n10.099453\n\n\n1\n2\n10.090664\n\n\n3\n1\n9.954183\n\n\n12\n1\n9.764612\n\n\n6\n2\n9.669608\n\n\n2\n2\n9.660847\n\n\n4\n2\n9.505235\n\n\n15\n2\n9.117198\n\n\n20\n1\n8.812926\n\n\n19\n1\n8.472506\n\n\n\n\n\n\n\nShow clients with a happiness score of less than 4\n\n\nfilter(happinessSample, happiness < 4)\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n\n\n\n\nShow Intervention group 2 with happiness scores above 7\n\n\nfilter(happinessSample, happiness > 7 & intervention == 2)\n\n\n\n\nparticipant\nintervention\nhappiness\n\n\n\n1\n2\n10.090664\n\n\n2\n2\n9.660847\n\n\n4\n2\n9.505235\n\n\n6\n2\n9.669608\n\n\n9\n2\n12.244699\n\n\n11\n2\n10.420561\n\n\n13\n2\n12.700549\n\n\n15\n2\n9.117198\n\n\n16\n2\n11.415905\n\n\n18\n2\n11.095057\n\n\n\n\n\n\n\nGroup by intervention and show the mean happiness score\n\n\nhappinessSample %>% group_by(intervention) %>% summarise(mean = mean(happiness))\n\n\n\n\nintervention\nmean\n\n\n\n1\n10.24892\n\n\n2\n10.59203"
  },
  {
    "objectID": "workingWithData.html#create-new-variables-from-data",
    "href": "workingWithData.html#create-new-variables-from-data",
    "title": "\n2  Working with data in R\n",
    "section": "\n2.9 Create new variables from data",
    "text": "2.9 Create new variables from data\nTo create new variables from data, we can use the mutate() function.\nFor example, let’s say we wanted to calculate the difference between each person’s happiness score and the mean happiness score.\nWe could do the following:\n\nhappinessSample %>% mutate(difference = happiness - mean(happiness))\n\n\n\n\nparticipant\nintervention\nhappiness\ndifference\n\n\n\n1\n2\n10.090664\n-0.3298127\n\n\n2\n2\n9.660847\n-0.7596298\n\n\n3\n1\n9.954183\n-0.4662934\n\n\n4\n2\n9.505235\n-0.9152415\n\n\n5\n1\n11.725063\n1.3045862\n\n\n6\n2\n9.669608\n-0.7508684\n\n\n7\n1\n10.588609\n0.1681318\n\n\n8\n1\n10.459963\n0.0394866\n\n\n9\n2\n12.244699\n1.8242222\n\n\n10\n1\n10.099453\n-0.3210239\n\n\n11\n2\n10.420561\n0.0000843\n\n\n12\n1\n9.764612\n-0.6558653\n\n\n13\n2\n12.700549\n2.2800722\n\n\n14\n1\n11.592937\n1.1724603\n\n\n15\n2\n9.117198\n-1.3032785\n\n\n16\n2\n11.415905\n0.9954285\n\n\n17\n1\n11.018959\n0.5984825\n\n\n18\n2\n11.095057\n0.6745805\n\n\n19\n1\n8.472506\n-1.9479712\n\n\n20\n1\n8.812926\n-1.6075505"
  },
  {
    "objectID": "mediation.html#overview",
    "href": "mediation.html#overview",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.1 Overview",
    "text": "8.1 Overview\n\nWhat are mediation and moderation?\nMediation analysis example\nPackages needed\nBaron and Kenny approach in R\nMediation package approach in R"
  },
  {
    "objectID": "mediation.html#what-is-mediation",
    "href": "mediation.html#what-is-mediation",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.2 What is mediation?",
    "text": "8.2 What is mediation?\nWhere the relationship between a predictor (X) and an outcome (Y) is mediated by another variable (M).\n\n\n\n\n\nA mediated relationship\n\n\nIn the above model, we theorise that socio-economic status predicts education level, which predicts future prospects."
  },
  {
    "objectID": "mediation.html#what-is-moderation",
    "href": "mediation.html#what-is-moderation",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.3 What is moderation?",
    "text": "8.3 What is moderation?\nThere is a direct relationship between X and Y but it is affected by a moderator (M)\n\n\n\n\n\nA moderated relationship\n\n\nIn the above model, we theorise that socio-economic status predicts future prospects but the strength of the relationship is changed by education level"
  },
  {
    "objectID": "mediation.html#why-different-models",
    "href": "mediation.html#why-different-models",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.4 Why different models?",
    "text": "8.4 Why different models?\n\n\n\n\n\nA mediated relationship\n\n\nThis might be more appropriate if higher education costs money\n\n\n\n\n\nA moderated relationship\n\n\nThis might be more appropriate if access to higher education is free"
  },
  {
    "objectID": "mediation.html#mediation-analysis",
    "href": "mediation.html#mediation-analysis",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.5 Mediation analysis",
    "text": "8.5 Mediation analysis\n\n8.5.1 What is a mediation design?\nWhether a mediation analysis is appropriate is determined as much by the design as by statistical criteria.\n\n\n\n\n\nA mediated relationship\n\n\nWe must consider whether it makes sense to predict this relationship between variables\n\n8.5.2 What is mediation analysis?\n\nBased on regression\n\nA summary of the logic of mediation:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\n\n\n\n\n\nThe direct relationship between X and Y should be significant\n The relationship between X and M should be significant \n The relationship between M and Y (controlling for X) should be significant \n When controlling for M, the strength of the relationship between X and Y decreases and is not significant \n\n\nBaron & Kenny (1986) originally used a 4-step regression model to test each of these relationships.\n\n\n8.5.3 What packages do we need?\nlibrary(mediation) #Mediation package\n\nlibrary(multilevel) #Sobel Test\n\nlibrary(bda) #Another Sobel Test option\n\nlibrary(gvlma) #Testing Model Assumptions \n\nlibrary(stargazer) #Handy regression tables"
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "href": "mediation.html#mediation-analysis-the-baron-and-kenny-approach",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.6 Mediation analysis (the Baron and Kenny Approach)",
    "text": "8.6 Mediation analysis (the Baron and Kenny Approach)\n\n8.6.1 Conducting mediation analysis (the Baron and Kenny Approach)\n\nBaron & Kenny (1986) originally used a 4-step regression model to test each of these relationships.\nThe sobel test is then used to test the significance of mediation\n\n\n\n\n\n8.6.2 Step 1: Total Effect\n\n#1. Total Effect\nfit <- lm(Y ~ X, data=Meddata)\nsummary(fit)\n\n\nCall:\nlm(formula = Y ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.917  -3.738  -0.259   2.910  12.540 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 19.88368   14.26371   1.394   0.1665  \nX            0.16899    0.08116   2.082   0.0399 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.16 on 98 degrees of freedom\nMultiple R-squared:  0.04237,   Adjusted R-squared:  0.0326 \nF-statistic: 4.336 on 1 and 98 DF,  p-value: 0.03993\n\n\n\n8.6.3 Step 2: Path A (X on M)\n\n#2. Path A (X on M)\nfita <- lm(M ~ X, data=Meddata)\nsummary(fita)\n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5367 -3.4175 -0.4375  2.9032 16.4520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.04494   13.41692   0.451    0.653    \nX            0.66252    0.07634   8.678 8.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.854 on 98 degrees of freedom\nMultiple R-squared:  0.4346,    Adjusted R-squared:  0.4288 \nF-statistic: 75.31 on 1 and 98 DF,  p-value: 8.872e-14\n\n\n\n8.6.4 Step 3: Path B (M on Y, controlling for X)\n\n#3. Path B (M on Y, controlling for X)\nfitb <- lm(Y ~ M + X, data=Meddata)\nsummary(fitb)\n\n\nCall:\nlm(formula = Y ~ M + X, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.3651 -3.3037 -0.6222  3.1068 10.3991 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.32177   13.16216   1.316    0.191    \nM            0.42381    0.09899   4.281 4.37e-05 ***\nX           -0.11179    0.09949  -1.124    0.264    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.756 on 97 degrees of freedom\nMultiple R-squared:  0.1946,    Adjusted R-squared:  0.1779 \nF-statistic: 11.72 on 2 and 97 DF,  p-value: 2.771e-05\n\n\n\n8.6.5 Step 4: Reversed Path C (Y on X, controlling for M)\n\n#4. Reversed Path C (Y on X, controlling for M)\nfitc <- lm(X ~ Y + M, data=Meddata)\nsummary(fitc)\n\n\nCall:\nlm(formula = X ~ Y + M, data = Meddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.438  -2.573  -0.030   3.010  11.779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 96.11234    9.27663  10.361  < 2e-16 ***\nY           -0.11493    0.10229  -1.124    0.264    \nM            0.69619    0.08356   8.332 5.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.823 on 97 degrees of freedom\nMultiple R-squared:  0.4418,    Adjusted R-squared:  0.4303 \nF-statistic: 38.39 on 2 and 97 DF,  p-value: 5.233e-13\n\n\n\n8.6.6 Viewing output\nSummary Table\nstargazer(fit, fita, fitb, fitc, type = \"text\", title = \"Baron and Kenny Method\")\n\n\n\n\n\n\n\n\n\n8.6.7 Interpreting Baron and Kenny approach\nA reminder of the logic of mediation:\n\nThe direct relationship between X and Y should be significant\nThe relationship between X and M should be significant\nThe relationship between M and Y (controlling for X) should be significant\nWhen controlling for M, the strength of the relationship between X and Y decreases and is not significant\n\n8.6.8 Running the Sobel test\n\nThe Sobel test checks the singificance of indirect effects\n\n\n#Sobel Test\nlibrary(multilevel)\nsobel(Meddata$X, Meddata$M, Meddata$Y)\n\n$`Mod1: Y~X`\n              Estimate Std. Error  t value   Pr(>|t|)\n(Intercept) 19.8836805 14.2637142 1.394004 0.16646905\npred         0.1689931  0.0811601 2.082220 0.03992761\n\n$`Mod2: Y~X+M`\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 17.3217682 13.16215851  1.316028 1.912663e-01\npred        -0.1117904  0.09949262 -1.123605 2.639537e-01\nmed          0.4238113  0.09899469  4.281152 4.371472e-05\n\n$`Mod3: M~X`\n             Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 6.0449365 13.41692114 0.4505457 6.533122e-01\npred        0.6625203  0.07634187 8.6783345 8.871741e-14\n\n$Indirect.Effect\n[1] 0.2807836\n\n$SE\n[1] 0.07313234\n\n$z.value\n[1] 3.83939\n\n$N\n[1] 100\n\n\nHowever, the above code only gives us to information we need to test the significance of the indirect effect, not the significance itself. Thereore, we can use the following, to get the actual significance of the indirect effect:\n\nlibrary(bda)\n\nWarning: package 'bda' was built under R version 4.2.3\n\n\nLoading required package: boot\n\n\nbda v15 (Bin Wang, 2021)\n\nmediation.test(Meddata$M, Meddata$X, Meddata$Y)\n\n\n\n\n\nSobel\nAroian\nGoodman\n\n\n\nz.value\n3.8393902\n3.819053\n3.8600563\n\n\np.value\n0.0001233\n0.000134\n0.0001134"
  },
  {
    "objectID": "mediation.html#mediation-analysis-the-mediation-package",
    "href": "mediation.html#mediation-analysis-the-mediation-package",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.7 Mediation analysis (the Mediation package)",
    "text": "8.7 Mediation analysis (the Mediation package)\n\n8.7.1 Preacher & Hayes (2004) mediation approach\n\nMediation package in R uses the Preacher & Hayes (2004) bootstrapping approach\nThey argue that few people test the significance of the indirect effect\n\n\n\n“Baron and Kenny simply state that perfect mediation has occurred if c’ becomes nonsignificant after controlling for M, so researchers have focused on that requirement.” (Preacher & Hayes, 2004, p. 719)\n\n\n\nSobel test has low power (requires larger sample sizes)\nSobel test assumes normality (often violated)\n\n8.7.2 What is bootstrapping?\n\n\n“Bootstrapping is a nonparametric approach to effect-size estimation and hypothesis testing that makes no assumptions about the shape of the distributions of the variables or the sampling distribution of the statistic” (Preacer & Hayes, 2004, p. 722)\n\n\n\nBootstrapping takes a large number of samples from our data and runs the analysis on each of these samples\nThe sampling is done randomly with replacement, and each sample in the bootstrap is the same size as our dataset\nUsing this method, we can create estimates with that fall within a narrower confidence interval (since we have now run the analysis on 100’s of samples)\nBootstrapping overcomes concerns about the distribution of our original dataset\n\n8.7.3 Mediation example\nIs the relationship between No of hours awake and wakefulness mediated by caffiene consumption?\n\nThis example is from Demos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14)\n\n\n\n\n\n\n8.7.4 Step 1: Run the models\n\n#Mediate package\nlibrary(mediation)\n\nfitM <- lm(M ~ X,     data=Meddata) #IV on M; Hours since waking predicting coffee consumption\nfitY <- lm(Y ~ X + M, data=Meddata) #IV and M on DV; Hours since dawn and coffee predicting wakefulness\n\n\n8.7.5 Step 2: Check assumptions\n\ngvlma(fitM) \n\n# We can see that the data is positively skewed. We might need to transform the data (we will discuss this another time).\n\ngvlma(fitY)\n\n\nCall:\nlm(formula = M ~ X, data = Meddata)\n\nCoefficients:\n(Intercept)            X  \n     6.0449       0.6625  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitM) \n\n                   Value p-value                   Decision\nGlobal Stat        8.833 0.06542    Assumptions acceptable.\nSkewness           6.314 0.01198 Assumptions NOT satisfied!\nKurtosis           1.219 0.26949    Assumptions acceptable.\nLink Function      1.076 0.29959    Assumptions acceptable.\nHeteroscedasticity 0.223 0.63674    Assumptions acceptable.\n\nCall:\nlm(formula = Y ~ X + M, data = Meddata)\n\nCoefficients:\n(Intercept)            X            M  \n    17.3218      -0.1118       0.4238  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitY) \n\n                     Value p-value                Decision\nGlobal Stat        3.41844  0.4904 Assumptions acceptable.\nSkewness           1.85648  0.1730 Assumptions acceptable.\nKurtosis           0.77788  0.3778 Assumptions acceptable.\nLink Function      0.71512  0.3977 Assumptions acceptable.\nHeteroscedasticity 0.06896  0.7929 Assumptions acceptable.\n\n\n\n8.7.6 Step 3.1: Run the mediation analysis on the models\nThe mediate function gives us:\n\nAverage Causal Mediation Effects (ACME)\nAverage Direct Effects (ADE)\ncombined indirect and direct effects (Total Effect)\nthe ratio of these estimates (Prop. Mediated).\n\nThe ACME here is the indirect effect of M (total effect - direct effect) and thus this value tells us if our mediation effect is significant.\n\nfitMed <- mediate(fitM, fitY, treat=\"X\", mediator=\"M\")\nsummary(fitMed)\n\n\nCausal Mediation Analysis \n\nQuasi-Bayesian Confidence Intervals\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.2808       0.1437         0.42  <2e-16 ***\nADE             -0.1133      -0.3116         0.09   0.258    \nTotal Effect     0.1674       0.0208         0.34   0.028 *  \nProp. Mediated   1.6428       0.5631         8.44   0.028 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 1000 \n\n\n\n8.7.7 Step 3.2: Plot the mediation analysis of the models\nThe plot below reiterates what was on the previous slide:\n\nThe confidence intervals of Total Effect and ACME are significant\nThe confidence interval of ADE is not significant\n\nTranslation:\n\nTotal effect is signficant: there is a relationship between X and Y (direct and indirect)\nADE is not significant: the relationship between X and Y is not direct\nACME is significant: the relationship between X and Y is mediated by M\n\n\nplot(fitMed)\n\n\n\n\n\n\n\n\n8.7.8 Step 4: Bootstrap the mediation model\nThe plot below changes our interpretation slightly:\n\nThe confidence interval ACME is significant\nThe confidence interval of Total Effect and ADE are not significant\n\nTranslation:\n\nTotal effect is not signficant: the relationship between X and Y is not significant when we combine direct and indirect effects\nADE is not significant: the relationship between X and Y is not direct\nACME is significant: the relationship between X and Y is mediated by M\n\n\nfitMedBoot <- mediate(fitM, fitY, boot=TRUE, sims=999, treat=\"X\", mediator=\"M\")\n\nRunning nonparametric bootstrap\n\nsummary(fitMedBoot)\nplot(fitMedBoot) ##\n\n\n\n\n\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.2808       0.1409         0.42  <2e-16 ***\nADE             -0.1118      -0.3080         0.12    0.31    \nTotal Effect     0.1690      -0.0123         0.34    0.07 .  \nProp. Mediated   1.6615      -3.7235        11.33    0.07 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 100 \n\n\nSimulations: 999"
  },
  {
    "objectID": "mediation.html#references",
    "href": "mediation.html#references",
    "title": "\n8  Mediation analysis\n",
    "section": "\n8.8 References",
    "text": "8.8 References\nDemos & Salas (2019). A Language, not a Letter: Learning Statistics in R (Chapter 14). https://ademos.people.uic.edu/ Accessed Jan 2020.\nPreacher, K. J., & Hayes, A. F. (2004). SPSS and SAS procedures for estimating indirect effects in simple mediation models. Behavior research methods, instruments, & computers, 36(4), 717-731."
  },
  {
    "objectID": "factorAnalysis.html#overview",
    "href": "factorAnalysis.html#overview",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.1 Overview",
    "text": "9.1 Overview\n\nWhat is factor analysis\nCFA versus PCA\nVariance in factor analysis\nConsidertations for factor analysis\nIdentifying / extracting factors\nRotation\nCronbach’s alpha"
  },
  {
    "objectID": "factorAnalysis.html#exploratory-factor-analysis",
    "href": "factorAnalysis.html#exploratory-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.2 Exploratory Factor analysis",
    "text": "9.2 Exploratory Factor analysis\n\nIdentify the relational structure between a set of variables in order to reduce them to a smaller set of factors\n\nThe process of dimension reduction (identify new variables) or data summarisation (summarise what is already there)\n\n\n\n\n9.2.1 Dimension reducton\n\n\nLatent Variables: Not directly observable. Rather they are inferred from other responses\n\nMany psychological constructs (e.g. anxiety) are latent variables that we cannot directly measure.\nRather, we can measure behaviours, cognitions and other variables that are related to the construct.\n\n\n\n\nWe might concptualise this as: “Responses to the questions are indicative of levels of underlying anxiety”\n\n\n\n\n\n\n\n\n\n\n9.2.2 Data summarisation\n\nIndex Variables or Components: A weighted summary of measured variables that contribute to the component variable\n“Principal components are variables of maximal variance constructed from linear combinations of the input features”\n\n\nWe might conceptualise this as: “We can reduce these measures/questions to a smaller set of higher order, independent, composite variables”"
  },
  {
    "objectID": "factorAnalysis.html#variance-in-exploratory-factor-analysis",
    "href": "factorAnalysis.html#variance-in-exploratory-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.3 Variance in exploratory factor analysis",
    "text": "9.3 Variance in exploratory factor analysis\nThere are two common methods of exploratory factor analysis: Common Factor analysis and Principal Component Analysis\n\nCFA assumes that there are two types of variance: common and unique\n\n\n\n\n\n\n\n\n\n\n9.3.1 Variance in PCA\n\nPCA only assumes common variance\n\n\n\n\n\n\n\n\n\n\n9.3.2 Variance in CFA\n\nDue to these different approaches, PCA is considered to be reflective of the current sample but not generalisable to the wider population\nWhereas, CFA is considered appropriate for hypothesis testing and making inferences to the population"
  },
  {
    "objectID": "factorAnalysis.html#what-is-factor-analysis",
    "href": "factorAnalysis.html#what-is-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.4 What is factor analysis?",
    "text": "9.4 What is factor analysis?\n\nIf we measure several variables (or questions), we can examine the correlation between sets of these variables\n\nSuch a correlation matrix is known as an R Matrix (r because correlation)\n\n\nIf there are clusters of correlations between a number of the variables (or questions), this indicates that they might be linked to the same underlying dimension (or latent variable)\nThe researcher should use informed judgement when assessing the appropriateness of variables for inclusion\n\n\n\n\n\n\n\n\n\nAn r matrix example"
  },
  {
    "objectID": "factorAnalysis.html#considerations-with-factor-analysis",
    "href": "factorAnalysis.html#considerations-with-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.5 Considerations with factor analysis",
    "text": "9.5 Considerations with factor analysis\n\nSample size:\n\nMust be more data points than variables being measured\nA common rule of thumb is at least 10 per variable\nThere are tests to assess sample size adequacy (e.g. Kaiser-Meyer test should be greater than 0.5)\n\n\nInter-correlation:\n\nThere must be sufficient correlation between the variables being measured\nA high number of correlations over 0.3\nCan be tested using Bartlett test of sphericity (sig. result means factor analysis can be used)\n\n\n\nOther things to check (see Field, 2018) \n\nThe quality of analysis depends upon the quality of the data (GI/GO).\nAvoid multicollinearity:\n\nseveral variables highly correlated, r > .80.\nDeterminent: should be greater than 0.00001\n\n\nAvoid singularity:\n\nsome variables perfectly correlated, r = 1.\n\n\nScreen the correlation matrix, eliminate any variables that obviously cause concern."
  },
  {
    "objectID": "factorAnalysis.html#representing-factor-analysis",
    "href": "factorAnalysis.html#representing-factor-analysis",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.6 Representing factor analysis",
    "text": "9.6 Representing factor analysis\n\nWe can represent factors visually based on the strength of their inter-correlations - Here, the axis of the graph represents a factor or latent variable\n\n\n\n\n\n\n\n\n\n\nWe can also represent factor analysis using a regression equation - Here the beta values represent the extent to which the variable “loads onto” a particular factor\n\n\n\n\n\n\n\n\n\nExample: Statistics anxiety\n\nMany people get anxious about statistics\nWe can ask them about their experience in a number of ways (e.g. questions compiled by students in a stats class)\n\nTheir responses might indicate that stats anxiety has a number of dimensions\n\ni.e. it is a multi-dimensional construct, as opposed to a unitary construct"
  },
  {
    "objectID": "factorAnalysis.html#step-1-create-a-correlation-matrix",
    "href": "factorAnalysis.html#step-1-create-a-correlation-matrix",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.7 Step 1: Create a correlation matrix",
    "text": "9.7 Step 1: Create a correlation matrix\n\nraq.matrix <- cor(raq)\n\nraq.matrix\n\n             Q01         Q02        Q03         Q04         Q05         Q06\nQ01  1.000000000 -0.09872403 -0.3366489  0.43586018  0.40243992  0.21673399\nQ02 -0.098724032  1.00000000  0.3183902 -0.11185965 -0.11934658 -0.07420968\nQ03 -0.336648879  0.31839020  1.0000000 -0.38046016 -0.31030879 -0.22674048\nQ04  0.435860179 -0.11185965 -0.3804602  1.00000000  0.40067225  0.27820154\nQ05  0.402439917 -0.11934658 -0.3103088  0.40067225  1.00000000  0.25746014\nQ06  0.216733985 -0.07420968 -0.2267405  0.27820154  0.25746014  1.00000000\nQ07  0.305365139 -0.15917448 -0.3819533  0.40861502  0.33939179  0.51358048\nQ08  0.330737608 -0.04962257 -0.2586342  0.34942939  0.26862697  0.22283175\nQ09 -0.092339458  0.31464054  0.2998036 -0.12454637 -0.09570151 -0.11264384\nQ10  0.213681706 -0.08400316 -0.1933887  0.21581010  0.25820925  0.32223023\nQ11  0.356786290 -0.14382984 -0.3506397  0.36865655  0.29782882  0.32807072\nQ12  0.345381133 -0.19486946 -0.4099513  0.44164706  0.34674325  0.31250937\nQ13  0.354646283 -0.14274026 -0.3179193  0.34429168  0.30182159  0.46640487\nQ14  0.337879655 -0.16469991 -0.3707551  0.35080964  0.31533810  0.40224407\nQ15  0.245752635 -0.16499581 -0.3123968  0.33423089  0.26137190  0.35989309\nQ16  0.498618057 -0.16755228 -0.4186478  0.41586725  0.39491795  0.24433888\nQ17  0.370550512 -0.08699527 -0.3273715  0.38273945  0.31041722  0.28226121\nQ18  0.347118037 -0.16389415 -0.3752329  0.38200149  0.32209148  0.51332164\nQ19 -0.189011027  0.20329748  0.3415737 -0.18597751 -0.16532210 -0.16675017\nQ20  0.213897945 -0.20159437 -0.3248338  0.24291796  0.19966945  0.10092489\nQ21  0.329153138 -0.20461730 -0.4171878  0.41029317  0.33461494  0.27233273\nQ22 -0.104408664  0.23087487  0.2036569 -0.09838349 -0.13253593 -0.16513541\nQ23 -0.004480593  0.09967828  0.1502065 -0.03381815 -0.04165684 -0.06868743\n            Q07         Q08         Q09         Q10         Q11         Q12\nQ01  0.30536514  0.33073761 -0.09233946  0.21368171  0.35678629  0.34538113\nQ02 -0.15917448 -0.04962257  0.31464054 -0.08400316 -0.14382984 -0.19486946\nQ03 -0.38195325 -0.25863421  0.29980362 -0.19338871 -0.35063969 -0.40995127\nQ04  0.40861502  0.34942939 -0.12454637  0.21581010  0.36865655  0.44164706\nQ05  0.33939179  0.26862697 -0.09570151  0.25820925  0.29782882  0.34674325\nQ06  0.51358048  0.22283175 -0.11264384  0.32223023  0.32807072  0.31250937\nQ07  1.00000000  0.29749696 -0.12829828  0.28372299  0.34474770  0.42298591\nQ08  0.29749696  1.00000000  0.01573316  0.15860850  0.62929768  0.25198582\nQ09 -0.12829828  0.01573316  1.00000000 -0.13418658 -0.11552479 -0.16739436\nQ10  0.28372299  0.15860850 -0.13418658  1.00000000  0.27143657  0.24582591\nQ11  0.34474770  0.62929768 -0.11552479  0.27143657  1.00000000  0.33529466\nQ12  0.42298591  0.25198582 -0.16739436  0.24582591  0.33529466  1.00000000\nQ13  0.44211926  0.31424716 -0.16743882  0.30196707  0.42316548  0.48871303\nQ14  0.44070276  0.28058958 -0.12150197  0.25468730  0.32532025  0.43270398\nQ15  0.39136675  0.29968600 -0.18657099  0.29523438  0.36482687  0.33179910\nQ16  0.38854534  0.32149420 -0.18886556  0.29058576  0.36907763  0.40805908\nQ17  0.39074283  0.59014022 -0.03681556  0.21832214  0.58683495  0.33269383\nQ18  0.50086685  0.27974433 -0.14957782  0.29250304  0.37341373  0.49296482\nQ19 -0.26912031 -0.15947671  0.24931170 -0.12723487 -0.19965203 -0.26665953\nQ20  0.22095420  0.17515089 -0.15864747  0.08406520  0.25533736  0.29802585\nQ21  0.48300388  0.29571756 -0.13594310  0.19313633  0.34643407  0.44063832\nQ22 -0.16820488 -0.07917265  0.25684622 -0.13090831 -0.16198921 -0.16728557\nQ23 -0.07029016 -0.05023839  0.17077441 -0.06191796 -0.08637256 -0.04642506\n            Q13         Q14         Q15         Q16         Q17         Q18\nQ01  0.35464628  0.33787966  0.24575263  0.49861806  0.37055051  0.34711804\nQ02 -0.14274026 -0.16469991 -0.16499581 -0.16755228 -0.08699527 -0.16389415\nQ03 -0.31791928 -0.37075510 -0.31239678 -0.41864780 -0.32737145 -0.37523290\nQ04  0.34429168  0.35080964  0.33423089  0.41586725  0.38273945  0.38200149\nQ05  0.30182159  0.31533810  0.26137190  0.39491795  0.31041722  0.32209148\nQ06  0.46640487  0.40224407  0.35989309  0.24433888  0.28226121  0.51332164\nQ07  0.44211926  0.44070276  0.39136675  0.38854534  0.39074283  0.50086685\nQ08  0.31424716  0.28058958  0.29968600  0.32149420  0.59014022  0.27974433\nQ09 -0.16743882 -0.12150197 -0.18657099 -0.18886556 -0.03681556 -0.14957782\nQ10  0.30196707  0.25468730  0.29523438  0.29058576  0.21832214  0.29250304\nQ11  0.42316548  0.32532025  0.36482687  0.36907763  0.58683495  0.37341373\nQ12  0.48871303  0.43270398  0.33179910  0.40805908  0.33269383  0.49296482\nQ13  1.00000000  0.44978632  0.34219704  0.35837775  0.40837657  0.53293713\nQ14  0.44978632  1.00000000  0.38011484  0.41841820  0.35374183  0.49830615\nQ15  0.34219704  0.38011484  1.00000000  0.45427861  0.37310235  0.34287045\nQ16  0.35837775  0.41841820  0.45427861  1.00000000  0.40976309  0.42197911\nQ17  0.40837657  0.35374183  0.37310235  0.40976309  1.00000000  0.37560681\nQ18  0.53293713  0.49830615  0.34287045  0.42197911  0.37560681  1.00000000\nQ19 -0.22697105 -0.25405813 -0.20980230 -0.26704702 -0.16288096 -0.25663183\nQ20  0.20396327  0.22592173  0.20625622  0.26514025  0.20523013  0.23518040\nQ21  0.37443078  0.39938896  0.29971557  0.42054273  0.36349147  0.43010427\nQ22 -0.19535632 -0.16983754 -0.16790617 -0.15579385 -0.12629066 -0.15982631\nQ23 -0.05298304 -0.04847418 -0.06200665 -0.08152195 -0.09167243 -0.08041698\n           Q19         Q20         Q21         Q22          Q23\nQ01 -0.1890110  0.21389794  0.32915314 -0.10440866 -0.004480593\nQ02  0.2032975 -0.20159437 -0.20461730  0.23087487  0.099678285\nQ03  0.3415737 -0.32483385 -0.41718781  0.20365686  0.150206522\nQ04 -0.1859775  0.24291796  0.41029317 -0.09838349 -0.033818152\nQ05 -0.1653221  0.19966945  0.33461494 -0.13253593 -0.041656841\nQ06 -0.1667502  0.10092489  0.27233273 -0.16513541 -0.068687430\nQ07 -0.2691203  0.22095420  0.48300388 -0.16820488 -0.070290157\nQ08 -0.1594767  0.17515089  0.29571756 -0.07917265 -0.050238392\nQ09  0.2493117 -0.15864747 -0.13594310  0.25684622  0.170774410\nQ10 -0.1272349  0.08406520  0.19313633 -0.13090831 -0.061917956\nQ11 -0.1996520  0.25533736  0.34643407 -0.16198921 -0.086372565\nQ12 -0.2666595  0.29802585  0.44063832 -0.16728557 -0.046425059\nQ13 -0.2269710  0.20396327  0.37443078 -0.19535632 -0.052983042\nQ14 -0.2540581  0.22592173  0.39938896 -0.16983754 -0.048474181\nQ15 -0.2098023  0.20625622  0.29971557 -0.16790617 -0.062006650\nQ16 -0.2670470  0.26514025  0.42054273 -0.15579385 -0.081521950\nQ17 -0.1628810  0.20523013  0.36349147 -0.12629066 -0.091672426\nQ18 -0.2566318  0.23518040  0.43010427 -0.15982631 -0.080416984\nQ19  1.0000000 -0.24859386 -0.27489793  0.23392259  0.122434401\nQ20 -0.2485939  1.00000000  0.46770448 -0.09970186 -0.034665293\nQ21 -0.2748979  0.46770448  1.00000000 -0.12902148 -0.067664367\nQ22  0.2339226 -0.09970186 -0.12902148  1.00000000  0.230369402\nQ23  0.1224344 -0.03466529 -0.06766437  0.23036940  1.000000000"
  },
  {
    "objectID": "factorAnalysis.html#step-2-lets-check-for-inter-correlation",
    "href": "factorAnalysis.html#step-2-lets-check-for-inter-correlation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.8 Step 2: Let’s check for Inter-correlation \n",
    "text": "9.8 Step 2: Let’s check for Inter-correlation \n\n\nlibrary(corrplot)\ncorrplot(raq.matrix, method = \"number\")\n\n\n\n\n\n\n\n\nWe can use bartlett’s test from the psych package\n\n\nlibrary(psych)\n\ncortest.bartlett(raq.matrix, n=2571)\n\n$chisq\n[1] 19334.49\n\n$p.value\n[1] 0\n\n$df\n[1] 253"
  },
  {
    "objectID": "factorAnalysis.html#step-3-check-sampling-adequacy",
    "href": "factorAnalysis.html#step-3-check-sampling-adequacy",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.9 Step 3: Check sampling adequacy",
    "text": "9.9 Step 3: Check sampling adequacy\n\nOverall should be > 0.5\n\n\nKMO(raq)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = raq)\nOverall MSA =  0.93\nMSA for each item = \n Q01  Q02  Q03  Q04  Q05  Q06  Q07  Q08  Q09  Q10  Q11  Q12  Q13  Q14  Q15  Q16 \n0.93 0.87 0.95 0.96 0.96 0.89 0.94 0.87 0.83 0.95 0.91 0.95 0.95 0.97 0.94 0.93 \n Q17  Q18  Q19  Q20  Q21  Q22  Q23 \n0.93 0.95 0.94 0.89 0.93 0.88 0.77"
  },
  {
    "objectID": "factorAnalysis.html#step-4-identify-number-of-factors",
    "href": "factorAnalysis.html#step-4-identify-number-of-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.10 Step 4: Identify number of factors",
    "text": "9.10 Step 4: Identify number of factors\n\nBased on Eigenvalues:\n\nKaiser (1960) – retain factors with eigen values > 1.\nJoliffe (1972) – retain factors with eigen values > .70.\n\n\nUse a scree plot: Cattell (1966): use ‘point of inflexion’.\n\n\n9.10.1 Which rule?\n\nUse Kaiser’s extraction when\n\nLess than 30 variables, communalities after extraction > 0.7\nSample size > 250 and mean communality > 0.6\n\n\nScree plot is good if sample size is > 200\n\n9.10.2 Scree plot\n\nscree(raq)\n\n\n\n\n\n\n\n\nWe are looking for the point of inflection\nWhere there is a drop-off\n\n\nOne approach: See how many factors we can draw a line through\n\n\n9.10.3 Parallel analysis\n\nHow many dimensions of stats anxiety are captured in the questionnaire?\n\n\nWe can run a parallel analysis to get an indication of the number of factors contained within the data\nParallel Analysis:\n\nSimulates data within the same range of values as our data set\nSuggests that we retain, at maximum, the factors with eigenvalues larger than those extracted from simulated data.\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(psych)\n\n parallel_analysis <- fa.parallel(raq)\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  7  and the number of components =  4 \n\n\n\nparallel_analysis\n\nCall: fa.parallel(x = raq)\nParallel analysis suggests that the number of factors =  7  and the number of components =  4 \n\n Eigen Values of \n  Original factors Resampled data Simulated data Original components\n1             6.64           0.21           0.20                7.29\n2             0.91           0.16           0.16                1.74\n3             0.63           0.13           0.13                1.32\n4             0.48           0.12           0.12                1.23\n5             0.29           0.10           0.10                0.99\n6             0.13           0.09           0.08                0.90\n7             0.08           0.07           0.07                0.81\n  Resampled components Simulated components\n1                 1.17                 1.17\n2                 1.15                 1.15\n3                 1.13                 1.13\n4                 1.11                 1.11\n5                 1.09                 1.09\n6                 1.08                 1.08\n7                 1.06                 1.06"
  },
  {
    "objectID": "factorAnalysis.html#step-5-perform-factor-analysis-with-initial-recommended-factors",
    "href": "factorAnalysis.html#step-5-perform-factor-analysis-with-initial-recommended-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.11 Step 5: Perform factor analysis (with initial recommended # factors)",
    "text": "9.11 Step 5: Perform factor analysis (with initial recommended # factors)\n\npaf <- fa(raq,\nnfactors = 6,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"none\")\n\n\npaf\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 6, rotate = \"none\", max.iter = 100, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2   PA3   PA4   PA5   PA6   h2   u2 com\nQ01  0.57  0.13 -0.12  0.23 -0.28 -0.19 0.52 0.48 2.3\nQ02 -0.28  0.37  0.17  0.12 -0.03  0.01 0.26 0.74 2.6\nQ03 -0.60  0.25  0.20 -0.02 -0.01  0.03 0.46 0.54 1.6\nQ04  0.61  0.08 -0.06  0.18 -0.09 -0.03 0.42 0.58 1.3\nQ05  0.52  0.04 -0.02  0.15 -0.17 -0.08 0.33 0.67 1.5\nQ06  0.55  0.02  0.49 -0.17  0.07 -0.01 0.57 0.43 2.2\nQ07  0.66 -0.03  0.22  0.03  0.11  0.06 0.50 0.50 1.3\nQ08  0.55  0.49 -0.27 -0.21  0.10 -0.02 0.66 0.34 2.9\nQ09 -0.27  0.46  0.12  0.21  0.10  0.03 0.35 0.65 2.4\nQ10  0.40 -0.01  0.17 -0.09 -0.15  0.02 0.22 0.78 1.8\nQ11  0.64  0.31 -0.20 -0.27  0.08 -0.04 0.63 0.37 2.1\nQ12  0.64 -0.10  0.06  0.15  0.05 -0.07 0.45 0.55 1.2\nQ13  0.65  0.02  0.22 -0.06  0.06 -0.13 0.50 0.50 1.4\nQ14  0.63 -0.04  0.16  0.06  0.01  0.01 0.42 0.58 1.2\nQ15  0.58 -0.01  0.07 -0.15 -0.19  0.44 0.59 0.41 2.3\nQ16  0.66 -0.02 -0.11  0.14 -0.28  0.09 0.56 0.44 1.6\nQ17  0.63  0.36 -0.15 -0.15  0.04  0.01 0.57 0.43 1.9\nQ18  0.68 -0.04  0.28  0.04  0.09 -0.10 0.57 0.43 1.4\nQ19 -0.40  0.27  0.11  0.06 -0.05  0.02 0.25 0.75 2.0\nQ20  0.41 -0.17 -0.25  0.19  0.24  0.11 0.37 0.63 3.5\nQ21  0.64 -0.10 -0.11  0.27  0.28  0.10 0.60 0.40 2.0\nQ22 -0.28  0.29  0.05  0.28  0.05  0.11 0.26 0.74 3.4\nQ23 -0.13  0.18  0.08  0.23  0.01  0.08 0.12 0.88 3.1\n\n                       PA1  PA2  PA3  PA4  PA5  PA6\nSS loadings           6.79 1.14 0.83 0.67 0.45 0.32\nProportion Var        0.30 0.05 0.04 0.03 0.02 0.01\nCumulative Var        0.30 0.34 0.38 0.41 0.43 0.44\nProportion Explained  0.67 0.11 0.08 0.07 0.04 0.03\nCumulative Proportion 0.67 0.78 0.86 0.92 0.97 1.00\n\nMean item complexity =  2\nTest of the hypothesis that 6 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 130  and the objective function was  0.23 \n\nThe root mean square of the residuals (RMSR) is  0.02 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  2571 with the empirical chi square  364.66  with prob <  3.9e-24 \nThe total n.obs was  2571  with Likelihood Chi Square =  578.65  with prob <  7.6e-58 \n\nTucker Lewis Index of factoring reliability =  0.954\nRMSEA index =  0.037  and the 90 % confidence intervals are  0.034 0.04\nBIC =  -442.12\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   PA1  PA2  PA3  PA4   PA5\nCorrelation of (regression) scores with factors   0.97 0.83 0.80 0.75  0.70\nMultiple R square of scores with factors          0.93 0.68 0.64 0.56  0.48\nMinimum correlation of possible factor scores     0.87 0.37 0.27 0.12 -0.03\n                                                    PA6\nCorrelation of (regression) scores with factors    0.65\nMultiple R square of scores with factors           0.42\nMinimum correlation of possible factor scores     -0.17\n\n\n\n9.11.1 Check the factor matrix\n\nWe are looking high levels of variance explained with SS loadings > 1\n\n\nprint(paf$loadings, cutoff=0, digits=3)\n\n\nLoadings:\n    PA1    PA2    PA3    PA4    PA5    PA6   \nQ01  0.567  0.129 -0.120  0.229 -0.275 -0.188\nQ02 -0.280  0.369  0.172  0.115 -0.029  0.009\nQ03 -0.603  0.245  0.199 -0.022 -0.006  0.030\nQ04  0.606  0.082 -0.056  0.184 -0.090 -0.033\nQ05  0.523  0.043 -0.020  0.154 -0.167 -0.083\nQ06  0.548  0.024  0.488 -0.166  0.073 -0.006\nQ07  0.662 -0.026  0.223  0.030  0.107  0.057\nQ08  0.545  0.488 -0.272 -0.214  0.096 -0.020\nQ09 -0.266  0.462  0.124  0.210  0.097  0.032\nQ10  0.405 -0.005  0.172 -0.090 -0.148  0.024\nQ11  0.644  0.312 -0.199 -0.270  0.085 -0.037\nQ12  0.641 -0.099  0.063  0.154  0.047 -0.067\nQ13  0.650  0.024  0.223 -0.058  0.061 -0.134\nQ14  0.626 -0.036  0.161  0.056  0.011  0.013\nQ15  0.580 -0.007  0.072 -0.152 -0.188  0.436\nQ16  0.661 -0.016 -0.109  0.138 -0.283  0.094\nQ17  0.629  0.355 -0.155 -0.150  0.038  0.006\nQ18  0.683 -0.039  0.277  0.041  0.092 -0.099\nQ19 -0.395  0.267  0.110  0.060 -0.052  0.022\nQ20  0.412 -0.171 -0.250  0.190  0.241  0.114\nQ21  0.644 -0.099 -0.110  0.270  0.283  0.099\nQ22 -0.279  0.291  0.050  0.284  0.047  0.114\nQ23 -0.130  0.182  0.081  0.235  0.011  0.077\n\n                 PA1   PA2   PA3   PA4   PA5   PA6\nSS loadings    6.786 1.140 0.827 0.667 0.452 0.324\nProportion Var 0.295 0.050 0.036 0.029 0.020 0.014\nCumulative Var 0.295 0.345 0.381 0.410 0.429 0.443\n\n\n\n9.11.2 Check the structure matrix\n\nprint(paf$Structure, cutoff=0, digits=3)\n\n\nLoadings:\n    PA1    PA2    PA3    PA4    PA5    PA6   \nQ01  0.567  0.129 -0.120  0.229 -0.275 -0.188\nQ02 -0.280  0.369  0.172  0.115 -0.029  0.009\nQ03 -0.603  0.245  0.199 -0.022 -0.006  0.030\nQ04  0.606  0.082 -0.056  0.184 -0.090 -0.033\nQ05  0.523  0.043 -0.020  0.154 -0.167 -0.083\nQ06  0.548  0.024  0.488 -0.166  0.073 -0.006\nQ07  0.662 -0.026  0.223  0.030  0.107  0.057\nQ08  0.545  0.488 -0.272 -0.214  0.096 -0.020\nQ09 -0.266  0.462  0.124  0.210  0.097  0.032\nQ10  0.405 -0.005  0.172 -0.090 -0.148  0.024\nQ11  0.644  0.312 -0.199 -0.270  0.085 -0.037\nQ12  0.641 -0.099  0.063  0.154  0.047 -0.067\nQ13  0.650  0.024  0.223 -0.058  0.061 -0.134\nQ14  0.626 -0.036  0.161  0.056  0.011  0.013\nQ15  0.580 -0.007  0.072 -0.152 -0.188  0.436\nQ16  0.661 -0.016 -0.109  0.138 -0.283  0.094\nQ17  0.629  0.355 -0.155 -0.150  0.038  0.006\nQ18  0.683 -0.039  0.277  0.041  0.092 -0.099\nQ19 -0.395  0.267  0.110  0.060 -0.052  0.022\nQ20  0.412 -0.171 -0.250  0.190  0.241  0.114\nQ21  0.644 -0.099 -0.110  0.270  0.283  0.099\nQ22 -0.279  0.291  0.050  0.284  0.047  0.114\nQ23 -0.130  0.182  0.081  0.235  0.011  0.077\n\n                 PA1   PA2   PA3   PA4   PA5   PA6\nSS loadings    6.786 1.140 0.827 0.667 0.452 0.324\nProportion Var 0.295 0.050 0.036 0.029 0.020 0.014\nCumulative Var 0.295 0.345 0.381 0.410 0.429 0.443\n\n\n\n9.11.3 Check eigenvalues\n\npaf$e.values[1:6]\n\n[1] 7.2900471 1.7388287 1.3167515 1.2271982 0.9878779 0.8953304\n\n\n\n9.11.4 Check communalities\n\nCommunality for each variable: the percentage of variance that can be explained by the retained factors.\nRetained factors should explain more of the variance in each variable.\n\n\npaf$communality\n\n      Q01       Q02       Q03       Q04       Q05       Q06       Q07       Q08 \n0.5170176 0.2585136 0.4643374 0.4196524 0.3341637 0.5720655 0.5042725 0.6649413 \n      Q09       Q10       Q11       Q12       Q13       Q14       Q15       Q16 \n0.3542281 0.2240464 0.6328967 0.4544862 0.4973541 0.4223263 0.5902303 0.5571656 \n      Q17       Q18       Q19       Q20       Q21       Q22       Q23 \n0.5700891 0.5655104 0.2467731 0.3686202 0.5991875 0.2606533 0.1178839"
  },
  {
    "objectID": "factorAnalysis.html#step-6-perform-factor-analysis-with-reduced-number-of-factors",
    "href": "factorAnalysis.html#step-6-perform-factor-analysis-with-reduced-number-of-factors",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.12 Step 6: Perform factor analysis (with reduced number of factors)",
    "text": "9.12 Step 6: Perform factor analysis (with reduced number of factors)\n\npaf1 <- fa(raq,\nnfactors = 2,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"none\")\n\npaf1\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 2, rotate = \"none\", max.iter = 100, fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2    h2   u2 com\nQ01  0.56  0.12 0.324 0.68 1.1\nQ02 -0.28  0.39 0.228 0.77 1.8\nQ03 -0.61  0.25 0.430 0.57 1.3\nQ04  0.61  0.09 0.377 0.62 1.0\nQ05  0.52  0.05 0.276 0.72 1.0\nQ06  0.53  0.04 0.282 0.72 1.0\nQ07  0.66 -0.01 0.437 0.56 1.0\nQ08  0.53  0.40 0.445 0.56 1.9\nQ09 -0.27  0.46 0.287 0.71 1.6\nQ10  0.40  0.00 0.163 0.84 1.0\nQ11  0.63  0.27 0.472 0.53 1.3\nQ12  0.64 -0.08 0.421 0.58 1.0\nQ13  0.65  0.04 0.421 0.58 1.0\nQ14  0.63 -0.02 0.396 0.60 1.0\nQ15  0.56  0.00 0.315 0.68 1.0\nQ16  0.65 -0.01 0.428 0.57 1.0\nQ17  0.63  0.34 0.511 0.49 1.5\nQ18  0.68 -0.02 0.461 0.54 1.0\nQ19 -0.40  0.28 0.238 0.76 1.8\nQ20  0.40 -0.15 0.187 0.81 1.3\nQ21  0.63 -0.07 0.403 0.60 1.0\nQ22 -0.28  0.29 0.161 0.84 2.0\nQ23 -0.13  0.19 0.053 0.95 1.8\n\n                       PA1  PA2\nSS loadings           6.67 1.04\nProportion Var        0.29 0.05\nCumulative Var        0.29 0.34\nProportion Explained  0.86 0.14\nCumulative Proportion 0.86 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 208  and the objective function was  1.23 \n\nThe root mean square of the residuals (RMSR) is  0.05 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2571 with the empirical chi square  3114.53  with prob <  0 \nThe total n.obs was  2571  with Likelihood Chi Square =  3155.34  with prob <  0 \n\nTucker Lewis Index of factoring reliability =  0.812\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.072 0.077\nBIC =  1522.12\nFit based upon off diagonal values = 0.97\nMeasures of factor score adequacy             \n                                                   PA1  PA2\nCorrelation of (regression) scores with factors   0.96 0.78\nMultiple R square of scores with factors          0.92 0.61\nMinimum correlation of possible factor scores     0.83 0.23\n\n\n\nplot(paf1)"
  },
  {
    "objectID": "factorAnalysis.html#factor-analysis-rotation",
    "href": "factorAnalysis.html#factor-analysis-rotation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.13 Factor analysis rotation",
    "text": "9.13 Factor analysis rotation\nWhat is rotation?\n\nIt is possible that variables load “highly” onto one factor and “medium” onto another\nBy rotating the factor axes, the variables are aligned with the factors that they load onto most\nThis helps us discriminate between factors\n\nThere are different methods of rotation\n\n\nOrthogonal rotation: Assumes that factors are unrelated and keeps them that way\n\nOblique rotation: Assumes that factors might be related and allows them to be correlated after rotation\n\n\nAre factors related? -Theoretical: Do we have logical reason for thinking they could be connected? -Based on data: Does the factor plot suggest independence or relatedness?"
  },
  {
    "objectID": "factorAnalysis.html#step-7-rotation",
    "href": "factorAnalysis.html#step-7-rotation",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.14 Step 7: Rotation",
    "text": "9.14 Step 7: Rotation\n\nPerform factor analysis (with rotation)\n\n\npaf2 <- fa(raq,\nnfactors = 2,\nfm=\"pa\",\nmax.iter = 100,\nrotate = \"oblimin\")\n\npaf2\n\nFactor Analysis using method =  pa\nCall: fa(r = raq, nfactors = 2, rotate = \"oblimin\", max.iter = 100, \n    fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n      PA1   PA2    h2   u2 com\nQ01  0.56  0.12 0.324 0.68 1.1\nQ02 -0.28  0.39 0.228 0.77 1.8\nQ03 -0.61  0.25 0.430 0.57 1.3\nQ04  0.61  0.09 0.377 0.62 1.0\nQ05  0.52  0.05 0.276 0.72 1.0\nQ06  0.53  0.04 0.282 0.72 1.0\nQ07  0.66 -0.01 0.437 0.56 1.0\nQ08  0.53  0.40 0.445 0.56 1.9\nQ09 -0.27  0.46 0.287 0.71 1.6\nQ10  0.40  0.00 0.163 0.84 1.0\nQ11  0.63  0.27 0.472 0.53 1.3\nQ12  0.64 -0.08 0.421 0.58 1.0\nQ13  0.65  0.04 0.421 0.58 1.0\nQ14  0.63 -0.02 0.396 0.60 1.0\nQ15  0.56  0.00 0.315 0.68 1.0\nQ16  0.65 -0.01 0.428 0.57 1.0\nQ17  0.63  0.34 0.511 0.49 1.5\nQ18  0.68 -0.02 0.461 0.54 1.0\nQ19 -0.40  0.28 0.238 0.76 1.8\nQ20  0.40 -0.15 0.187 0.81 1.3\nQ21  0.63 -0.07 0.403 0.60 1.0\nQ22 -0.28  0.29 0.161 0.84 2.0\nQ23 -0.13  0.19 0.053 0.95 1.8\n\n                       PA1  PA2\nSS loadings           6.67 1.04\nProportion Var        0.29 0.05\nCumulative Var        0.29 0.34\nProportion Explained  0.86 0.14\nCumulative Proportion 0.86 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  253  with the objective function =  7.55 with Chi Square =  19334.49\ndf of  the model are 208  and the objective function was  1.23 \n\nThe root mean square of the residuals (RMSR) is  0.05 \nThe df corrected root mean square of the residuals is  0.05 \n\nThe harmonic n.obs is  2571 with the empirical chi square  3114.53  with prob <  0 \nThe total n.obs was  2571  with Likelihood Chi Square =  3155.34  with prob <  0 \n\nTucker Lewis Index of factoring reliability =  0.812\nRMSEA index =  0.074  and the 90 % confidence intervals are  0.072 0.077\nBIC =  1522.12\nFit based upon off diagonal values = 0.97\nMeasures of factor score adequacy             \n                                                   PA1  PA2\nCorrelation of (regression) scores with factors   0.96 0.78\nMultiple R square of scores with factors          0.92 0.61\nMinimum correlation of possible factor scores     0.83 0.23\n\n\n\nplot(paf1)\n\n\n\n\n\n\n\n\nplot(paf2)"
  },
  {
    "objectID": "factorAnalysis.html#reliability-internal-consistency",
    "href": "factorAnalysis.html#reliability-internal-consistency",
    "title": "\n9  Factor Analysis\n",
    "section": "\n9.15 Reliability / internal consistency",
    "text": "9.15 Reliability / internal consistency\n\n9.15.1 Cronbach’s Alpha\n\nAn expansion of the split-half reliability concept\nAlpha takes all possible combination of items and assesses their relationship to each other\nHigh values above 0.7 suggest internal consistency among items\n\n9.15.2 Chronbach’s Alpha in R\n\nWe can use the alpha() function in the psych package\n\n\nlibrary(psych)\n\nalpha(raq)\n\nSome items ( Q02 Q03 Q09 Q19 Q22 Q23 ) were negatively correlated with the total scale and \nprobably should be reversed.  \nTo do this, run the function again with the 'check.keys=TRUE' option\nReliability analysis   \nCall: alpha(x = raq)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.75      0.77    0.83      0.13 3.4 0.0065  3.3 0.39     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.74  0.75  0.77\nDuhachek  0.74  0.75  0.77\n\n Reliability if an item is dropped:\n    raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nQ01      0.73      0.76    0.82      0.12 3.1   0.0071 0.071  0.23\nQ02      0.77      0.79    0.84      0.15 3.8   0.0061 0.071  0.25\nQ03      0.79      0.81    0.85      0.16 4.2   0.0055 0.059  0.25\nQ04      0.73      0.75    0.82      0.12 3.0   0.0072 0.070  0.22\nQ05      0.74      0.76    0.82      0.12 3.1   0.0071 0.072  0.22\nQ06      0.73      0.76    0.82      0.12 3.1   0.0072 0.072  0.23\nQ07      0.73      0.75    0.82      0.12 3.0   0.0074 0.069  0.22\nQ08      0.73      0.76    0.82      0.12 3.1   0.0071 0.072  0.23\nQ09      0.78      0.79    0.84      0.15 3.8   0.0058 0.071  0.25\nQ10      0.74      0.76    0.83      0.13 3.3   0.0068 0.074  0.23\nQ11      0.73      0.75    0.81      0.12 3.0   0.0072 0.069  0.22\nQ12      0.73      0.75    0.82      0.12 3.1   0.0072 0.069  0.22\nQ13      0.73      0.75    0.82      0.12 3.0   0.0073 0.069  0.22\nQ14      0.73      0.75    0.82      0.12 3.1   0.0072 0.070  0.22\nQ15      0.73      0.76    0.82      0.12 3.1   0.0071 0.071  0.22\nQ16      0.73      0.75    0.82      0.12 3.0   0.0072 0.069  0.22\nQ17      0.73      0.75    0.81      0.12 3.0   0.0072 0.070  0.22\nQ18      0.72      0.75    0.81      0.12 3.0   0.0074 0.068  0.22\nQ19      0.78      0.80    0.85      0.15 4.0   0.0057 0.067  0.26\nQ20      0.75      0.77    0.83      0.13 3.3   0.0067 0.073  0.25\nQ21      0.73      0.75    0.82      0.12 3.1   0.0072 0.069  0.22\nQ22      0.77      0.79    0.84      0.15 3.8   0.0059 0.071  0.26\nQ23      0.77      0.79    0.84      0.14 3.7   0.0061 0.074  0.26\n\n Item statistics \n       n   raw.r  std.r  r.cor r.drop mean   sd\nQ01 2571  0.5598  0.581  0.564  0.492  3.6 0.83\nQ02 2571 -0.0116 -0.018 -0.114 -0.105  4.4 0.85\nQ03 2571 -0.3356 -0.361 -0.465 -0.435  3.4 1.08\nQ04 2571  0.6064  0.618  0.606  0.533  3.2 0.95\nQ05 2571  0.5365  0.546  0.516  0.454  3.3 0.96\nQ06 2571  0.5709  0.560  0.547  0.478  3.8 1.12\nQ07 2571  0.6409  0.636  0.635  0.560  3.1 1.10\nQ08 2571  0.5646  0.582  0.578  0.493  3.8 0.87\nQ09 2571  0.0587  0.020 -0.068 -0.081  3.2 1.26\nQ10 2571  0.4300  0.442  0.391  0.346  3.7 0.88\nQ11 2571  0.6078  0.628  0.633  0.540  3.7 0.88\nQ12 2571  0.5909  0.602  0.593  0.519  2.8 0.92\nQ13 2571  0.6288  0.637  0.634  0.559  3.6 0.95\nQ14 2571  0.6056  0.609  0.596  0.528  3.1 1.00\nQ15 2571  0.5433  0.550  0.526  0.457  3.2 1.01\nQ16 2571  0.5965  0.615  0.612  0.525  3.1 0.92\nQ17 2571  0.6329  0.650  0.653  0.568  3.5 0.88\nQ18 2571  0.6534  0.653  0.656  0.578  3.4 1.05\nQ19 2571 -0.1316 -0.157 -0.264 -0.248  3.7 1.10\nQ20 2571  0.3705  0.375  0.326  0.265  2.4 1.04\nQ21 2571  0.5922  0.598  0.591  0.514  2.8 0.98\nQ22 2571 -0.0063 -0.027 -0.127 -0.121  3.1 1.04\nQ23 2571  0.1030  0.084 -0.014 -0.013  2.6 1.04\n\nNon missing response frequency for each item\n       1    2    3    4    5 miss\nQ01 0.02 0.07 0.29 0.52 0.11    0\nQ02 0.01 0.04 0.08 0.31 0.56    0\nQ03 0.03 0.17 0.34 0.26 0.19    0\nQ04 0.05 0.17 0.36 0.37 0.05    0\nQ05 0.04 0.18 0.29 0.43 0.06    0\nQ06 0.06 0.10 0.13 0.44 0.27    0\nQ07 0.09 0.24 0.26 0.34 0.07    0\nQ08 0.03 0.06 0.19 0.58 0.15    0\nQ09 0.08 0.28 0.23 0.20 0.20    0\nQ10 0.02 0.10 0.18 0.57 0.14    0\nQ11 0.02 0.06 0.22 0.53 0.16    0\nQ12 0.09 0.23 0.46 0.20 0.02    0\nQ13 0.03 0.12 0.25 0.48 0.12    0\nQ14 0.07 0.18 0.38 0.31 0.06    0\nQ15 0.06 0.18 0.30 0.39 0.07    0\nQ16 0.06 0.16 0.42 0.33 0.04    0\nQ17 0.03 0.10 0.27 0.52 0.08    0\nQ18 0.06 0.12 0.31 0.37 0.14    0\nQ19 0.02 0.15 0.22 0.33 0.29    0\nQ20 0.22 0.37 0.25 0.15 0.02    0\nQ21 0.09 0.29 0.34 0.26 0.02    0\nQ22 0.05 0.26 0.34 0.26 0.10    0\nQ23 0.12 0.42 0.27 0.12 0.06    0\n\n\n\nHere we get a warning that some of the items are negatively correlated and we should probably reverse them.\nThe decision to do so should be based on the logic of the questions themselves - check first\nHowever, since cronbach’s alpha is designed to check internal consistency related to a single construct, we would expect that negative correlations would only result from:\n\nItems that are designed to be reverse-scored\nQuestions that are related to another factor or construct\n\n\nLet’s check the questionnaire\n\n(Q02, Q03, Q09, Q19, Q22, Q23):\n\n\n\n\n\n\n\n\n\n\n\n\nIt is possible to run the analysis with automatic reversal of negatively-correlated items\n\n\nalpha(raq, check.keys=TRUE)\n\n\nReliability analysis   \nCall: alpha(x = raq, check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.89      0.89    0.91      0.27 8.3 0.0031  3.1 0.54     0.27\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.88  0.89   0.9\nDuhachek  0.88  0.89   0.9\n\n Reliability if an item is dropped:\n     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nQ01       0.88      0.89    0.90      0.26 7.9   0.0032 0.016  0.27\nQ02-      0.89      0.89    0.91      0.28 8.4   0.0031 0.016  0.28\nQ03-      0.88      0.89    0.90      0.26 7.8   0.0033 0.017  0.26\nQ04       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ05       0.89      0.89    0.90      0.27 8.0   0.0032 0.017  0.27\nQ06       0.88      0.89    0.90      0.27 8.0   0.0032 0.016  0.27\nQ07       0.88      0.89    0.90      0.26 7.7   0.0034 0.016  0.26\nQ08       0.89      0.89    0.90      0.27 8.0   0.0032 0.016  0.27\nQ09-      0.89      0.89    0.91      0.28 8.4   0.0030 0.016  0.28\nQ10       0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.28\nQ11       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ12       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ13       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ14       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ15       0.88      0.89    0.90      0.26 7.9   0.0033 0.017  0.27\nQ16       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ17       0.88      0.89    0.90      0.26 7.8   0.0033 0.016  0.26\nQ18       0.88      0.88    0.90      0.26 7.7   0.0034 0.016  0.26\nQ19-      0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.29\nQ20       0.89      0.89    0.90      0.27 8.2   0.0032 0.017  0.28\nQ21       0.88      0.89    0.90      0.26 7.7   0.0033 0.016  0.26\nQ22-      0.89      0.89    0.91      0.28 8.4   0.0031 0.016  0.29\nQ23-      0.89      0.90    0.91      0.28 8.7   0.0030 0.014  0.29\n\n Item statistics \n        n raw.r std.r r.cor r.drop mean   sd\nQ01  2571  0.55  0.57  0.54   0.50  3.6 0.83\nQ02- 2571  0.36  0.36  0.31   0.30  1.6 0.85\nQ03- 2571  0.65  0.64  0.62   0.59  2.6 1.08\nQ04  2571  0.61  0.61  0.59   0.55  3.2 0.95\nQ05  2571  0.54  0.55  0.52   0.48  3.3 0.96\nQ06  2571  0.56  0.55  0.53   0.49  3.8 1.12\nQ07  2571  0.67  0.67  0.65   0.62  3.1 1.10\nQ08  2571  0.51  0.53  0.51   0.46  3.8 0.87\nQ09- 2571  0.37  0.35  0.30   0.28  2.8 1.26\nQ10  2571  0.44  0.45  0.40   0.38  3.7 0.88\nQ11  2571  0.63  0.64  0.63   0.58  3.7 0.88\nQ12  2571  0.65  0.65  0.64   0.60  2.8 0.92\nQ13  2571  0.65  0.65  0.64   0.60  3.6 0.95\nQ14  2571  0.64  0.64  0.62   0.59  3.1 1.00\nQ15  2571  0.59  0.59  0.56   0.53  3.2 1.01\nQ16  2571  0.66  0.67  0.65   0.61  3.1 0.92\nQ17  2571  0.61  0.62  0.61   0.56  3.5 0.88\nQ18  2571  0.68  0.68  0.67   0.63  3.4 1.05\nQ19- 2571  0.47  0.46  0.42   0.40  2.3 1.10\nQ20  2571  0.45  0.45  0.41   0.38  2.4 1.04\nQ21  2571  0.64  0.64  0.63   0.59  2.8 0.98\nQ22- 2571  0.37  0.36  0.31   0.30  2.9 1.04\nQ23- 2571  0.23  0.22  0.15   0.15  3.4 1.04\n\nNon missing response frequency for each item\n       1    2    3    4    5 miss\nQ01 0.02 0.07 0.29 0.52 0.11    0\nQ02 0.01 0.04 0.08 0.31 0.56    0\nQ03 0.03 0.17 0.34 0.26 0.19    0\nQ04 0.05 0.17 0.36 0.37 0.05    0\nQ05 0.04 0.18 0.29 0.43 0.06    0\nQ06 0.06 0.10 0.13 0.44 0.27    0\nQ07 0.09 0.24 0.26 0.34 0.07    0\nQ08 0.03 0.06 0.19 0.58 0.15    0\nQ09 0.08 0.28 0.23 0.20 0.20    0\nQ10 0.02 0.10 0.18 0.57 0.14    0\nQ11 0.02 0.06 0.22 0.53 0.16    0\nQ12 0.09 0.23 0.46 0.20 0.02    0\nQ13 0.03 0.12 0.25 0.48 0.12    0\nQ14 0.07 0.18 0.38 0.31 0.06    0\nQ15 0.06 0.18 0.30 0.39 0.07    0\nQ16 0.06 0.16 0.42 0.33 0.04    0\nQ17 0.03 0.10 0.27 0.52 0.08    0\nQ18 0.06 0.12 0.31 0.37 0.14    0\nQ19 0.02 0.15 0.22 0.33 0.29    0\nQ20 0.22 0.37 0.25 0.15 0.02    0\nQ21 0.09 0.29 0.34 0.26 0.02    0\nQ22 0.05 0.26 0.34 0.26 0.10    0\nQ23 0.12 0.42 0.27 0.12 0.06    0"
  }
]